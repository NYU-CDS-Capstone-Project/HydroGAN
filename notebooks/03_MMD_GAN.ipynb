{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/OctoberChang/MMD-GAN - accompanying the paper MMD-GAN: Towards Deeper Understanding of Moment Matching Network.\n",
    "\n",
    "To check GPU usage, open new terminal inside Jupyter and nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import h5py\n",
    "import timeit\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import colors\n",
    "import h5py\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "batch_size = 16       # BATCH_SIZE: batch size for training\n",
    "workers = 1           # WORKERS: number of threads to load data\n",
    "experiment = \"./mmd_gan\"       # EXPERIMENT: output directory of sampled images\n",
    "gpu_device = 0        # GPU_DEVICE: gpu id (default 0)\n",
    "nc = 1                # NC: number of channels in images\n",
    "nz = 500                # NZ: hidden dimension in z and codespace\n",
    "image_size = 128       # IMAGE_SIZE: image size of dataset - \n",
    "                        # for our dataset more like one edge of the subcube\n",
    "lr = 5e-5               # LR: learning rate (default 5e-5)\n",
    "max_iter = 150         # MAX_ITER: max iteration for training\n",
    "\n",
    "# args.manual_seed = 1126\n",
    "manual_seed = 1126\n",
    "n_samples = 256*8      # on prince, number of samples to get from the training cube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert n_samples / batch_size > 100, \"The gen_iterations wont work properly!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset           # DATASET: type of dataset (mnist/cifar10/celeba/lsun)\n",
    "#dataroot          # DATAROOT: path to dataset\n",
    "#netG              # NETG: path to generator model\n",
    "#netD              # NETD: path to discriminator model\n",
    "#Diters            # DITERS: number of updates for discriminator per one generator update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    \"\"\"Function for dividing/truncating cmaps\"\"\"\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "\n",
    "def visualize_cube(cube=None,      ## array name\n",
    "             edge_dim=None,        ## edge dimension (128 for 128 x 128 x 128 cube)\n",
    "             start_cube_index_x=0,\n",
    "             start_cube_index_y=0,\n",
    "             start_cube_index_z=0,\n",
    "             fig_size=None,\n",
    "             stdev_to_white=1,\n",
    "             norm_multiply=600,\n",
    "             color_map=\"Blues\",\n",
    "             lognormal=False):\n",
    "    \n",
    "    cube_size = edge_dim\n",
    "    edge = np.array([*range(cube_size)])\n",
    "    \n",
    "    end_x = start_cube_index_x + cube_size\n",
    "    end_y = start_cube_index_y + cube_size\n",
    "    end_z = start_cube_index_z + cube_size\n",
    "    \n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    data_value = cube[start_cube_index_x:end_x,\n",
    "                      start_cube_index_y:end_y,\n",
    "                      start_cube_index_z:end_z]\n",
    "    \n",
    "    x,y,z = edge,edge,edge\n",
    "    product = [*itertools.product(x,y,z)]\n",
    "    \n",
    "    X = np.array([product[k][0] for k in [*range(len(product))]])\n",
    "    Y = np.array([product[k][1] for k in [*range(len(product))]])\n",
    "    Z = np.array([product[k][2] for k in [*range(len(product))]])\n",
    "    \n",
    "    ## map data to 1d array that corresponds to the axis values in the product array\n",
    "    data_1dim = np.array([data_value[X[i]][Y[i]][Z[i]] for i in [*range(len(product))]])\n",
    "    \n",
    "    \n",
    "    initial_mean = np.mean(data_1dim) - stdev_to_white*np.std(data_1dim)\n",
    "    mask = data_1dim > initial_mean\n",
    "    mask = mask.astype(np.int)\n",
    "    \n",
    "    data_1dim = np.multiply(mask,data_1dim)\n",
    "    ## mask X,Y,Z to match the dimensions of the data\n",
    "    X, Y, Z, data_1dim = [axis[np.where(data_1dim>0)] for axis in [X,Y,Z,data_1dim]]\n",
    "\n",
    "    if lognormal == False:\n",
    "        s = norm_multiply*data_1dim/np.linalg.norm(data_1dim)\n",
    "    else:\n",
    "        s = np.log(norm_multiply*data_1dim/np.linalg.norm(data_1dim))\n",
    "    \n",
    "    cmap=plt.get_cmap(color_map)\n",
    "    new_cmap = truncate_colormap(cmap, 0.99, 1,n=10)\n",
    "    \n",
    "    ## IGNORE BELOW 3D PLOT FORMATTING \n",
    "    \n",
    "    ## plot cube\n",
    "    \n",
    "    cube_definition = [(start_cube_index_x, start_cube_index_x, start_cube_index_x),\n",
    "                      (start_cube_index_x, start_cube_index_x+edge_dim, start_cube_index_x),\n",
    "                      (start_cube_index_x+edge_dim, start_cube_index_x, start_cube_index_x),\n",
    "                      (start_cube_index_x, start_cube_index_x, start_cube_index_x+edge_dim)]\n",
    "    \n",
    "    cube_definition_array = [\n",
    "        np.array(list(item))\n",
    "        for item in cube_definition\n",
    "    ]\n",
    "    \n",
    "    points = []\n",
    "    points += cube_definition_array\n",
    "    vectors = [\n",
    "        cube_definition_array[1] - cube_definition_array[0],\n",
    "        cube_definition_array[2] - cube_definition_array[0],\n",
    "        cube_definition_array[3] - cube_definition_array[0]\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    points += [cube_definition_array[0] + vectors[0] + vectors[1]]\n",
    "    points += [cube_definition_array[0] + vectors[0] + vectors[2]]\n",
    "    points += [cube_definition_array[0] + vectors[1] + vectors[2]]\n",
    "    points += [cube_definition_array[0] + vectors[0] + vectors[1] + vectors[2]]\n",
    "\n",
    "    points = np.array(points)\n",
    "\n",
    "    edges = [\n",
    "        [points[0], points[3], points[5], points[1]],\n",
    "        [points[1], points[5], points[7], points[4]],\n",
    "        [points[4], points[2], points[6], points[7]],\n",
    "        [points[2], points[6], points[3], points[0]],\n",
    "        [points[0], points[2], points[4], points[1]],\n",
    "        [points[3], points[6], points[7], points[5]]\n",
    "    ]\n",
    "    \n",
    "#     ax.fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    faces = Poly3DCollection(edges, linewidths=1, edgecolors='k',)\n",
    "    faces.set_facecolor((0,0,1,0)) ## set transparent facecolor to the cube\n",
    "    \n",
    "    ax.add_collection3d(faces)\n",
    "    \n",
    "    ax.scatter(points[:,0], points[:,1], points[:,2], s=0)\n",
    "\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    ax.xaxis.pane.fill = False\n",
    "    ax.yaxis.pane.fill = False\n",
    "    ax.zaxis.pane.fill = False\n",
    "    \n",
    "    ax.xaxis.pane.set_edgecolor('w')\n",
    "    ax.yaxis.pane.set_edgecolor('w')\n",
    "    ax.zaxis.pane.set_edgecolor('w')\n",
    "    \n",
    "    ax.xaxis.set_major_locator(MultipleLocator(edge_dim))\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(edge_dim))\n",
    "    ax.zaxis.set_major_locator(MultipleLocator(edge_dim))\n",
    "    \n",
    "    ax.grid(False)\n",
    "    \n",
    "    ax.set_xlim3d(0,edge_dim)\n",
    "    ax.set_ylim3d(0,edge_dim)\n",
    "    ax.set_zlim3d(0,edge_dim)\n",
    "#     ax.get_frame_on()\n",
    "    \n",
    "    ax.xaxis._axinfo['tick']['inward_factor'] = 0\n",
    "    ax.xaxis._axinfo['tick']['outward_factor'] = 0\n",
    "    ax.yaxis._axinfo['tick']['inward_factor'] = 0\n",
    "    ax.yaxis._axinfo['tick']['outward_factor'] = 0\n",
    "    ax.zaxis._axinfo['tick']['inward_factor'] = 0\n",
    "    ax.zaxis._axinfo['tick']['outward_factor'] = 0\n",
    "    \n",
    "    ax.scatter(X, Y, Z,       ## axis vals\n",
    "               c=data_1dim,   ## data, mapped to 1-dim\n",
    "               cmap=new_cmap,\n",
    "               s=s,           ## sizes - dims multiplied by each data point's magnitude\n",
    "               alpha=0.7,\n",
    "               edgecolors=\"face\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_var_est = 1e-8\n",
    "\n",
    "# Consider linear time MMD with a linear kernel:\n",
    "# K(f(x), f(y)) = f(x)^Tf(y)\n",
    "# h(z_i, z_j) = k(x_i, x_j) + k(y_i, y_j) - k(x_i, y_j) - k(x_j, y_i)\n",
    "#             = [f(x_i) - f(y_i)]^T[f(x_j) - f(y_j)]\n",
    "#\n",
    "# f_of_X: batch_size * k\n",
    "# f_of_Y: batch_size * k\n",
    "def linear_mmd2(f_of_X, f_of_Y):\n",
    "    loss = 0.0\n",
    "    delta = f_of_X - f_of_Y\n",
    "    loss = torch.mean((delta[:-1] * delta[1:]).sum(1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider linear time MMD with a polynomial kernel:\n",
    "# K(f(x), f(y)) = (alpha*f(x)^Tf(y) + c)^d\n",
    "# f_of_X: batch_size * k\n",
    "# f_of_Y: batch_size * k\n",
    "def poly_mmd2(f_of_X, f_of_Y, d=2, alpha=1.0, c=2.0):\n",
    "    K_XX = (alpha * (f_of_X[:-1] * f_of_X[1:]).sum(1) + c)\n",
    "    K_XX_mean = torch.mean(K_XX.pow(d))\n",
    "\n",
    "    K_YY = (alpha * (f_of_Y[:-1] * f_of_Y[1:]).sum(1) + c)\n",
    "    K_YY_mean = torch.mean(K_YY.pow(d))\n",
    "\n",
    "    K_XY = (alpha * (f_of_X[:-1] * f_of_Y[1:]).sum(1) + c)\n",
    "    K_XY_mean = torch.mean(K_XY.pow(d))\n",
    "\n",
    "    K_YX = (alpha * (f_of_Y[:-1] * f_of_X[1:]).sum(1) + c)\n",
    "    K_YX_mean = torch.mean(K_YX.pow(d))\n",
    "\n",
    "    return K_XX_mean + K_YY_mean - K_XY_mean - K_YX_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mix_rbf_kernel(X, Y, sigma_list):\n",
    "    assert(X.size(0) == Y.size(0))\n",
    "    m = X.size(0)\n",
    "\n",
    "    Z = torch.cat((X, Y), 0)\n",
    "    ZZT = torch.mm(Z, Z.t())\n",
    "    diag_ZZT = torch.diag(ZZT).unsqueeze(1)\n",
    "    Z_norm_sqr = diag_ZZT.expand_as(ZZT)\n",
    "    exponent = Z_norm_sqr - 2 * ZZT + Z_norm_sqr.t()\n",
    "\n",
    "    K = 0.0\n",
    "    for sigma in sigma_list:\n",
    "        gamma = 1.0 / (2 * sigma**2)\n",
    "        K += torch.exp(-gamma * exponent)\n",
    "\n",
    "    return K[:m, :m], K[:m, m:], K[m:, m:], len(sigma_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_rbf_mmd2(X, Y, sigma_list, biased=True):\n",
    "    K_XX, K_XY, K_YY, d = _mix_rbf_kernel(X, Y, sigma_list)\n",
    "    # return _mmd2(K_XX, K_XY, K_YY, const_diagonal=d, biased=biased)\n",
    "    return _mmd2(K_XX, K_XY, K_YY, const_diagonal=False, biased=biased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_rbf_mmd2_and_ratio(X, Y, sigma_list, biased=True):\n",
    "    K_XX, K_XY, K_YY, d = _mix_rbf_kernel(X, Y, sigma_list)\n",
    "    # return _mmd2_and_ratio(K_XX, K_XY, K_YY, const_diagonal=d, biased=biased)\n",
    "    return _mmd2_and_ratio(K_XX, K_XY, K_YY, const_diagonal=False, biased=biased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Helper functions to compute variances based on kernel matrices\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def _mmd2(K_XX, K_XY, K_YY, const_diagonal=False, biased=False):\n",
    "    m = K_XX.size(0)    # assume X, Y are same shape\n",
    "\n",
    "    # Get the various sums of kernels that we'll use\n",
    "    # Kts drop the diagonal, but we don't need to compute them explicitly\n",
    "    if const_diagonal is not False:\n",
    "        diag_X = diag_Y = const_diagonal\n",
    "        sum_diag_X = sum_diag_Y = m * const_diagonal\n",
    "    else:\n",
    "        diag_X = torch.diag(K_XX)                       # (m,)\n",
    "        diag_Y = torch.diag(K_YY)                       # (m,)\n",
    "        sum_diag_X = torch.sum(diag_X)\n",
    "        sum_diag_Y = torch.sum(diag_Y)\n",
    "\n",
    "    Kt_XX_sums = K_XX.sum(dim=1) - diag_X             # \\tilde{K}_XX * e = K_XX * e - diag_X\n",
    "    Kt_YY_sums = K_YY.sum(dim=1) - diag_Y             # \\tilde{K}_YY * e = K_YY * e - diag_Y\n",
    "    K_XY_sums_0 = K_XY.sum(dim=0)                     # K_{XY}^T * e\n",
    "\n",
    "    Kt_XX_sum = Kt_XX_sums.sum()                       # e^T * \\tilde{K}_XX * e\n",
    "    Kt_YY_sum = Kt_YY_sums.sum()                       # e^T * \\tilde{K}_YY * e\n",
    "    K_XY_sum = K_XY_sums_0.sum()                       # e^T * K_{XY} * e\n",
    "\n",
    "    if biased:\n",
    "        mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)\n",
    "            + (Kt_YY_sum + sum_diag_Y) / (m * m)\n",
    "            - 2.0 * K_XY_sum / (m * m))\n",
    "    else:\n",
    "        mmd2 = (Kt_XX_sum / (m * (m - 1))\n",
    "            + Kt_YY_sum / (m * (m - 1))\n",
    "            - 2.0 * K_XY_sum / (m * m))\n",
    "\n",
    "    return mmd2\n",
    "\n",
    "\n",
    "def _mmd2_and_ratio(K_XX, K_XY, K_YY, const_diagonal=False, biased=False):\n",
    "    mmd2, var_est = _mmd2_and_variance(K_XX, K_XY, K_YY, const_diagonal=const_diagonal, biased=biased)\n",
    "    loss = mmd2 / torch.sqrt(torch.clamp(var_est, min=min_var_est))\n",
    "    return loss, mmd2, var_est\n",
    "\n",
    "\n",
    "def _mmd2_and_variance(K_XX, K_XY, K_YY, const_diagonal=False, biased=False):\n",
    "    m = K_XX.size(0)    # assume X, Y are same shape\n",
    "\n",
    "    # Get the various sums of kernels that we'll use\n",
    "    # Kts drop the diagonal, but we don't need to compute them explicitly\n",
    "    if const_diagonal is not False:\n",
    "        diag_X = diag_Y = const_diagonal\n",
    "        sum_diag_X = sum_diag_Y = m * const_diagonal\n",
    "        sum_diag2_X = sum_diag2_Y = m * const_diagonal**2\n",
    "    else:\n",
    "        diag_X = torch.diag(K_XX)                       # (m,)\n",
    "        diag_Y = torch.diag(K_YY)                       # (m,)\n",
    "        sum_diag_X = torch.sum(diag_X)\n",
    "        sum_diag_Y = torch.sum(diag_Y)\n",
    "        sum_diag2_X = diag_X.dot(diag_X)\n",
    "        sum_diag2_Y = diag_Y.dot(diag_Y)\n",
    "\n",
    "    Kt_XX_sums = K_XX.sum(dim=1) - diag_X             # \\tilde{K}_XX * e = K_XX * e - diag_X\n",
    "    Kt_YY_sums = K_YY.sum(dim=1) - diag_Y             # \\tilde{K}_YY * e = K_YY * e - diag_Y\n",
    "    K_XY_sums_0 = K_XY.sum(dim=0)                     # K_{XY}^T * e\n",
    "    K_XY_sums_1 = K_XY.sum(dim=1)                     # K_{XY} * e\n",
    "\n",
    "    Kt_XX_sum = Kt_XX_sums.sum()                       # e^T * \\tilde{K}_XX * e\n",
    "    Kt_YY_sum = Kt_YY_sums.sum()                       # e^T * \\tilde{K}_YY * e\n",
    "    K_XY_sum = K_XY_sums_0.sum()                       # e^T * K_{XY} * e\n",
    "\n",
    "    Kt_XX_2_sum = (K_XX ** 2).sum() - sum_diag2_X      # \\| \\tilde{K}_XX \\|_F^2\n",
    "    Kt_YY_2_sum = (K_YY ** 2).sum() - sum_diag2_Y      # \\| \\tilde{K}_YY \\|_F^2\n",
    "    K_XY_2_sum  = (K_XY ** 2).sum()                    # \\| K_{XY} \\|_F^2\n",
    "\n",
    "    if biased:\n",
    "        mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)\n",
    "            + (Kt_YY_sum + sum_diag_Y) / (m * m)\n",
    "            - 2.0 * K_XY_sum / (m * m))\n",
    "    else:\n",
    "        mmd2 = (Kt_XX_sum / (m * (m - 1))\n",
    "            + Kt_YY_sum / (m * (m - 1))\n",
    "            - 2.0 * K_XY_sum / (m * m))\n",
    "\n",
    "    var_est = (\n",
    "        2.0 / (m**2 * (m - 1.0)**2) * (2 * Kt_XX_sums.dot(Kt_XX_sums) - Kt_XX_2_sum + 2 * Kt_YY_sums.dot(Kt_YY_sums) - Kt_YY_2_sum)\n",
    "        - (4.0*m - 6.0) / (m**3 * (m - 1.0)**3) * (Kt_XX_sum**2 + Kt_YY_sum**2)\n",
    "        + 4.0*(m - 2.0) / (m**3 * (m - 1.0)**2) * (K_XY_sums_1.dot(K_XY_sums_1) + K_XY_sums_0.dot(K_XY_sums_0))\n",
    "        - 4.0*(m - 3.0) / (m**3 * (m - 1.0)**2) * (K_XY_2_sum) - (8 * m - 12) / (m**5 * (m - 1)) * K_XY_sum**2\n",
    "        + 8.0 / (m**3 * (m - 1.0)) * (\n",
    "            1.0 / m * (Kt_XX_sum + Kt_YY_sum) * K_XY_sum\n",
    "            - Kt_XX_sums.dot(K_XY_sums_1)\n",
    "            - Kt_YY_sums.dot(K_XY_sums_0))\n",
    "        )\n",
    "    return mmd2, var_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, dim=1):\n",
    "    return x.div(x.norm(2, dim=dim).expand_as(x))\n",
    "\n",
    "\n",
    "def match(x, y, dist):\n",
    "    '''\n",
    "    Computes distance between corresponding points points in `x` and `y`\n",
    "    using distance `dist`.\n",
    "    \n",
    "    # compute L2-loss of AE\n",
    "    L2_AE_X_D = match(x.view(batch_size, -1), f_dec_X_D, 'L2')\n",
    "    L2_AE_Y_D = match(y.view(batch_size, -1), f_dec_Y_D, 'L2')\n",
    "    \n",
    "    '''\n",
    "    if dist == 'L2':\n",
    "        return (x - y).pow(2).mean()\n",
    "    elif dist == 'L1':\n",
    "        return (x - y).abs().mean()\n",
    "    elif dist == 'cos':\n",
    "        x_n = normalize(x)\n",
    "        y_n = normalize(y)\n",
    "        return 2 - (x_n).mul(y_n).mean()\n",
    "    else:\n",
    "        assert dist == 'none', 'wtf ?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_norm(m, norm_type=2):\n",
    "    total_norm = 0.0\n",
    "    for p in m.parameters():\n",
    "        param_norm = p.grad.data.norm(norm_type)\n",
    "        total_norm += param_norm ** norm_type\n",
    "    total_norm = total_norm ** (1. / norm_type)\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.1)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: batch_size * k * 1 * 1\n",
    "# output: batch_size * nc * image_size * image_size\n",
    "\n",
    "\"\"\"\n",
    "# construct encoder/decoder modules\n",
    "hidden_dim = nz\n",
    "G_decoder = Decoder(image_size, \n",
    "                    nc, \n",
    "                    k=nz, \n",
    "                    ngf=16)\n",
    "D_encoder = Encoder(image_size, \n",
    "                    nc, \n",
    "                    k=hidden_dim, \n",
    "                    ndf=16)\n",
    "D_decoder = Decoder(image_size, \n",
    "                    nc, \n",
    "                    k=hidden_dim, \n",
    "                    ngf=16)\n",
    "                    \n",
    "What is ngf?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, isize, nc, k=100, ngf=64):\n",
    "        super(Decoder, self).__init__()\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, tisize = ngf // 2, 4\n",
    "        print(\"cngf = \" + str(cngf))\n",
    "        print(\"tisize = \" + str(tisize))\n",
    "        \n",
    "        while tisize != isize:\n",
    "            cngf = cngf * 2\n",
    "            tisize = tisize * 2\n",
    "            \n",
    "        print(\"cngf = \" + str(cngf))\n",
    "        print(\"tisize = \" + str(tisize))\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        main.add_module('initial_{0}-{1}_convt'.format(k, cngf), \n",
    "                        nn.ConvTranspose3d(in_channels = k,\n",
    "                                           out_channels = cngf,\n",
    "                                           kernel_size = 4, \n",
    "                                           stride = 1,\n",
    "                                           padding = 0, \n",
    "                                           bias=False))\n",
    "        main.add_module('initial_{0}_batchnorm'.format(cngf), \n",
    "                        nn.BatchNorm3d(num_features = cngf))\n",
    "        main.add_module('initial_{0}_relu'.format(cngf), \n",
    "                        nn.ReLU(True))\n",
    "\n",
    "        csize = 4\n",
    "        while csize < isize // 2:\n",
    "            main.add_module('pyramid_{0}-{1}_convt'.format(cngf, cngf // 2),\n",
    "                            nn.ConvTranspose3d(in_channels = cngf,\n",
    "                                               out_channels = cngf // 2,\n",
    "                                               kernel_size = 4,\n",
    "                                               stride = 2,\n",
    "                                               padding = 1,\n",
    "                                               bias=False))\n",
    "            main.add_module('pyramid_{0}_batchnorm'.format(cngf // 2),\n",
    "                            nn.BatchNorm3d(num_features = cngf // 2))\n",
    "            main.add_module('pyramid_{0}_relu'.format(cngf // 2),\n",
    "                            nn.ReLU(True))\n",
    "            cngf = cngf // 2\n",
    "            csize = csize * 2\n",
    "\n",
    "        main.add_module('final_{0}-{1}_convt'.format(cngf, nc), \n",
    "                        nn.ConvTranspose3d(in_channels = cngf,\n",
    "                                           out_channels = nc,\n",
    "                                           kernel_size = 4,\n",
    "                                           stride = 2,\n",
    "                                           padding = 1,\n",
    "                                           bias=False))\n",
    "#         main.add_module('final_{0}_tanh'.format(nc),\n",
    "#                         nn.Tanh())\n",
    "#         main.add_module('final_{0}_sigmoid'.format(nc),\n",
    "#                 nn.Sigmoid())\n",
    "#         main.add_module('final_{0}_leakyrelu'.format(nc),\n",
    "#                 nn.LeakyReLU(negative_slope=0.01,\n",
    "#                              inplace = True))\n",
    "        main.add_module('final_{0}_relu'.format(nc),\n",
    "                        nn.ReLU(True))\n",
    "#         main.add_module('final_{0}_linear'.format(nc),\n",
    "#                         nn.Linear(in_features = isize, \n",
    "#                                   out_features = isize))\n",
    "    \n",
    "        self.main = main\n",
    "        \n",
    "        # to print out the resulting structure\n",
    "#         print(main)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NetG is a decoder\n",
    "# input: batch_size * nz * 1 * 1\n",
    "# output: batch_size * nc * image_size * image_size\n",
    "class NetG(nn.Module):\n",
    "    def __init__(self, decoder):\n",
    "        super(NetG, self).__init__()\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.decoder(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# input: batch_size * nc * 64 * 64\n",
    "# output: batch_size * k * 1 * 1\n",
    "\n",
    "\"\"\"\n",
    "# construct encoder/decoder modules\n",
    "hidden_dim = nz\n",
    "G_decoder = Decoder(image_size, \n",
    "                    nc, \n",
    "                    k=nz, \n",
    "                    ngf=16)\n",
    "D_encoder = Encoder(image_size, \n",
    "                    nc, \n",
    "                    k=hidden_dim, \n",
    "                    ndf=16)\n",
    "D_decoder = Decoder(image_size, \n",
    "                    nc, \n",
    "                    k=hidden_dim, \n",
    "                    ngf=16)\n",
    "\"\"\"\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, isize, nc, k=100, ndf=64):\n",
    "        \"\"\"\n",
    "        isize = image_size\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        # input is nc x isize x isize\n",
    "        main = nn.Sequential()\n",
    "        main.add_module('initial_conv_{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv3d(in_channels = nc,\n",
    "                                  out_channels = ndf,\n",
    "                                  kernel_size = 4,\n",
    "                                  stride = 2,\n",
    "                                  padding = 1,\n",
    "                                  bias=False))\n",
    "        main.add_module('initial_relu_{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        \n",
    "        \n",
    "        csize, cndf = isize / 2, ndf\n",
    "        print(\"csize = \" + str(csize))\n",
    "        print(\"cndf = \" + str(cndf))\n",
    "\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid_{0}-{1}_conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv3d(in_channels = in_feat,\n",
    "                                      out_channels = out_feat,\n",
    "                                      kernel_size = 4,\n",
    "                                      stride = 2,\n",
    "                                      padding = 1,\n",
    "                                      bias=False))\n",
    "            main.add_module('pyramid_{0}_batchnorm'.format(out_feat),\n",
    "                            nn.BatchNorm3d(num_features = out_feat))\n",
    "            main.add_module('pyramid_{0}_relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "            \n",
    "        main.add_module('final_{0}-{1}_conv'.format(cndf, 1),\n",
    "                        nn.Conv3d(in_channels = cndf,\n",
    "                                  out_channels = k,\n",
    "                                  kernel_size = 4,\n",
    "                                  stride = 1,\n",
    "                                  padding = 0,\n",
    "                                  bias=False))\n",
    "\n",
    "        self.main = main\n",
    "        \n",
    "        # to print out the resulting structure\n",
    "#         print(main)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NetD is an encoder + decoder\n",
    "# input: batch_size * nc * image_size * image_size\n",
    "# f_enc_X: batch_size * k * 1 * 1\n",
    "# f_dec_X: batch_size * nc * image_size * image_size\n",
    "class NetD(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(NetD, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input):\n",
    "        f_enc_X = self.encoder(input)\n",
    "        f_dec_X = self.decoder(f_enc_X)\n",
    "\n",
    "        f_enc_X = f_enc_X.view(input.size(0), -1)\n",
    "        f_dec_X = f_dec_X.view(input.size(0), -1)\n",
    "        return f_enc_X, f_dec_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ONE_SIDED(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ONE_SIDED, self).__init__()\n",
    "\n",
    "        main = nn.ReLU()\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(-input)\n",
    "        output = -output.mean()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if args.experiment is None:\n",
    "#     args.experiment = 'samples'\n",
    "# os.system('mkdir {0}'.format(args.experiment))\n",
    "\n",
    "if experiment is None:\n",
    "    experiment = 'samples'\n",
    "os.system('mkdir {0}'.format(experiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "#     args.cuda = True\n",
    "    cuda = True\n",
    "#     torch.cuda.set_device(args.gpu_device)\n",
    "    torch.cuda.set_device(gpu_device)\n",
    "    print(\"Using GPU device\", torch.cuda.current_device())\n",
    "else:\n",
    "    raise EnvironmentError(\"GPU device not available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(seed=args.manual_seed)\n",
    "# random.seed(args.manual_seed)\n",
    "# torch.manual_seed(args.manual_seed)\n",
    "# torch.cuda.manual_seed(args.manual_seed)\n",
    "# cudnn.benchmark = True\n",
    "\n",
    "np.random.seed(seed=manual_seed)\n",
    "random.seed(manual_seed)\n",
    "torch.manual_seed(manual_seed)\n",
    "torch.cuda.manual_seed(manual_seed)\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_test(s_test, s_train):\n",
    "    #2048/16=128\n",
    "    m=8\n",
    "    x=random.randint(0,m)*s_train\n",
    "    y=random.randint(0,m)*s_train\n",
    "    z=random.randint(0,m)*s_train\n",
    "    #print(x,y,z)\n",
    "    return {'x':[x,x+s_test], 'y':[y,y+s_test], 'z':[z,z+s_test]}\n",
    "\n",
    "def check_coords(test_coords, train_coords):\n",
    "    valid=True\n",
    "    for i in ['x','y','z']:\n",
    "        r=(max(test_coords[i][0], \n",
    "               train_coords[i][0]), \n",
    "           min(test_coords[i][1],\n",
    "               train_coords[i][1]))\n",
    "        if r[0]<=r[1]:\n",
    "            valid=False\n",
    "    return valid\n",
    "\n",
    "def get_samples(s_sample, nsamples, redshift, test_coords):\n",
    "    #n is size of minibatch, get valid samples (not intersecting with test_coords)\n",
    "    sample_list=[]\n",
    "    m=2048-128\n",
    "    for n in range(nsamples):\n",
    "        #print(\"Sample No = \" + str(n + 1) + \" / \" + str(nsamples))\n",
    "        sample_valid=False\n",
    "        while sample_valid==False:\n",
    "            x = random.randint(0,m)\n",
    "            y = random.randint(0,m)\n",
    "            z = random.randint(0,m)\n",
    "            sample_coords = {'x':[x,x+s_sample], \n",
    "                             'y':[y,y+s_sample], \n",
    "                             'z':[z,z+s_sample]}\n",
    "            \n",
    "            sample_valid = check_coords(test_coords, \n",
    "                                        sample_coords)\n",
    "        \n",
    "        sample_list.append(sample_coords)\n",
    "    \n",
    "    print(\"Sampling finished.\")\n",
    "        \n",
    "    #Load cube and get samples and convert them to np.arrays\n",
    "    sample_array=[]\n",
    "    datapath=''\n",
    "    f = h5py.File(datapath+'fields_z='+redshift+'.hdf5', 'r')\n",
    "    f=f['delta_HI']\n",
    "    \n",
    "    # getting the max of the whole cube\n",
    "    #print(f.shape)\n",
    "    max_list = []\n",
    "    for i in range(f.shape[0]):\n",
    "        #print(np.max(f[i:i+1,:,:]))\n",
    "        max_list.append(np.max(f[i:i+1,:,:]))\n",
    "    max_cube = max(max_list)\n",
    "    #f.close()\n",
    "    \n",
    "    # getting the min of the whole cube\n",
    "    #print(f.shape)\n",
    "    \n",
    "#     min_list = []\n",
    "#     for i in range(f.shape[0]):\n",
    "#         #print(np.max(f[i:i+1,:,:]))\n",
    "#         min_list.append(np.min(f[i:i+1,:,:]))\n",
    "#     min_cube = max(min_list)\n",
    "    \n",
    "    print(\"Getting max & min value finished.\")\n",
    "    \n",
    "    counter = 0\n",
    "    for c in sample_list:\n",
    "        print(\"Counter = \" + str(counter + 1) + \" / \" + str(len(sample_list)))\n",
    "        a = f[c['x'][0]:c['x'][1],\n",
    "              c['y'][0]:c['y'][1],\n",
    "              c['z'][0]:c['z'][1]]\n",
    "        \n",
    "        a = np.array(a)\n",
    "\n",
    "        # standardizing into the [0,1] interval using the max of the whole cube\n",
    "        a = a / max_cube\n",
    "        \n",
    "        # taking the log because the data is log-normally distributed\n",
    "#         a = np.add(a,1e-10)\n",
    "#         a = np.add(a, np.divide(min_cube, 100.0))\n",
    "\n",
    "        # log(a + 1) transformation to keep 0s 0\n",
    "#         a = a + 1\n",
    "#         a = np.log(a)\n",
    "\n",
    "        # normalizing\n",
    "#         mean_a = np.mean(a)\n",
    "#         std_a = np.std(a)\n",
    "        \n",
    "#         a = (a - mean_a) / std_a\n",
    "         \n",
    "#         a = max_cube - a\n",
    "#         a = np.log(a)\n",
    "#         a = np.log(a)        \n",
    "#         a = np.log(a)\n",
    "#         a = np.log(a)\n",
    "        \n",
    "        sample_array.append(a)\n",
    "    \n",
    "        counter = counter + 1\n",
    "        \n",
    "    f=0\n",
    "    return sample_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcd = define_test(s_test = 1024,\n",
    "            s_train = 128)\n",
    "testcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trial_sample = get_samples(s_sample = 128, \n",
    "            nsamples = 1, \n",
    "            redshift = \"1.0\", \n",
    "            test_coords = testcd)\n",
    "trial_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_sample[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_sample[0].reshape(-1,).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_plot = trial_sample[0].reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_plot.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_plot.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_plot.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,8))\n",
    "plt.title(\"Trial Sample\")\n",
    "plt.xlim((trial_plot.min(),\n",
    "         trial_plot.max()))\n",
    "bins = np.linspace(trial_plot.min(),\n",
    "                   trial_plot.max(), \n",
    "                   100)\n",
    "plt.hist(trial_plot, bins = bins, \n",
    "         color = \"blue\" ,\n",
    "         alpha = 0.3, \n",
    "         label = \"Trial\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cube(cube=trial_sample[0],      ## array name\n",
    "             edge_dim=trial_sample[0].shape[0],        ## edge dimension (128 for 128 x 128 x 128 cube)\n",
    "             start_cube_index_x=0,\n",
    "             start_cube_index_y=0,\n",
    "             start_cube_index_z=0,\n",
    "             fig_size=(10,10),\n",
    "             stdev_to_white=-2,\n",
    "             norm_multiply=1000,\n",
    "             color_map=\"Blues\",\n",
    "             lognormal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HydrogenDataset2(Dataset):\n",
    "    \"\"\"Hydrogen Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, h5_file, root_dir, s_test, s_train,\n",
    "                 s_sample, nsamples, redshift):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h5_file (string): name of the h5 file with 32 sampled cubes.\n",
    "            root_dir (string): Directory with the .h5 file.\n",
    "        \"\"\"\n",
    "        file_size = os.path.getsize(root_dir + h5_file) / 1e6 # in MBs\n",
    "        print(\"The whole file size is \" + str(int(file_size)) + \" MBs\")\n",
    "        \n",
    "        # self.subcubes = h5py.File('../data/sample_32.h5', 'r')\n",
    "        self.subcubes = h5py.File(root_dir + h5_file, 'r')[\"delta_HI\"]\n",
    "        self.h5_file = h5_file\n",
    "        self.root_dir = root_dir\n",
    "        self.s_test = s_test\n",
    "        self.s_train = s_train\n",
    "        self.t_coords = define_test(self.s_test,\n",
    "                                    self.s_train)\n",
    "        self.s_sample = s_sample\n",
    "        self.nsamples = nsamples\n",
    "        self.redshift = redshift\n",
    "        \n",
    "        self.samples = get_samples(s_sample = self.s_sample,\n",
    "                             nsamples = self.nsamples,\n",
    "                             redshift = self.redshift,\n",
    "                             test_coords = self.t_coords)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Function called when len(self) is executed\n",
    "        \n",
    "        #print(len(self.subcubes))\n",
    "#         return len(self.nsamples)\n",
    "        return self.nsamples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        This can be implemented in such a way that the whole h5 file read \n",
    "        using h5py.File() and get_sample() function is called to return\n",
    "        a random subcube. This won't increase memory usage because the\n",
    "        subcubes will be read in the same way and only the batch will\n",
    "        be read into memory.\n",
    "        \n",
    "        Here we have implemented it so that it can be used with data\n",
    "        generated by get_sample() function.\n",
    "        \n",
    "        The output of this function is one subcube with the dimensions\n",
    "        specified by get_sample() implementation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # default version -> error in training because of dimensions\n",
    "#         sample = self.subcubes[idx]\n",
    "        \n",
    "        # reshaped version to add another dimension\n",
    "#         sample = self.subcubes[idx].reshape((1,128,128,128))\n",
    "\n",
    "        # On prince using get_samples()\n",
    "#         print(\"nsamples = \" + str(self.nsamples))\n",
    "#         sample = get_samples(s_sample = self.s_sample,\n",
    "#                              nsamples = self.nsamples,\n",
    "#                              redshift = self.redshift,\n",
    "#                              test_coords = self.t_coords)\n",
    "    \n",
    "        sample = self.samples[idx].reshape((1,128,128,128))\n",
    "        \n",
    "        # added division by 1e6 for exploding variance\n",
    "        # and resulting in inf during reparametrization trick part\n",
    "#         sample = sample/1e6\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HydrogenDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, h5_file, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h5_file (string): name of the h5 file with 32 sampled cubes.\n",
    "            root_dir (string): Directory with the .h5 file.\n",
    "        \"\"\"\n",
    "        file_size = os.path.getsize(root_dir + h5_file) / 1e6 # in MBs\n",
    "        print(\"The file size is \" + str(int(file_size)) + \" MBs\")\n",
    "        \n",
    "        # self.subcubes = h5py.File('../data/sample_32.h5', 'r')\n",
    "        self.subcubes = h5py.File(root_dir + h5_file, 'r')['sample32']\n",
    "        self.h5_file = h5_file\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        # Function called when len(self) is executed\n",
    "        \n",
    "        #print(len(self.subcubes))\n",
    "        return len(self.subcubes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        This can be implemented in such a way that the whole h5 file read \n",
    "        using h5py.File() and get_sample() function is called to return\n",
    "        a random subcube. This won't increase memory usage because the\n",
    "        subcubes will be read in the same way and only the batch will\n",
    "        be read into memory.\n",
    "        \n",
    "        Here we have implemented it so that it can be used with data\n",
    "        generated by get_sample() function.\n",
    "        \n",
    "        The output of this function is one subcube with the dimensions\n",
    "        specified by get_sample() implementation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # default version -> error in training because of dimensions\n",
    "        #sample = self.subcubes[idx]\n",
    "        \n",
    "        # reshaped version to add another dimension\n",
    "        sample = self.subcubes[idx].reshape((1,128,128,128))\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# on prince\n",
    "sampled_subcubes = HydrogenDataset2(h5_file=\"fields_z=1.0.hdf5\",\n",
    "                                    root_dir = \"\",\n",
    "                                    s_test = 1024, \n",
    "                                    s_train = 128,\n",
    "                                    s_sample = 128, \n",
    "                                    nsamples = n_samples, \n",
    "                                    redshift = \"1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on local\n",
    "# sampled_subcubes = HydrogenDataset(h5_file=\"sample_32.h5\",\n",
    "#                                     root_dir = \"../data/\")\n",
    "# dataset = sampled_subcubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "trn_loader = torch.utils.data.DataLoader(sampled_subcubes, \n",
    "                                         batch_size = batch_size,\n",
    "                                         shuffle=True, \n",
    "                                         num_workers=int(workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct encoder/decoder modules\n",
    "hidden_dim = nz\n",
    "G_decoder = Decoder(image_size, \n",
    "                    nc, \n",
    "                    k=nz, \n",
    "                    ngf=16)\n",
    "D_encoder = Encoder(image_size, \n",
    "                    nc, \n",
    "                    k=hidden_dim, \n",
    "                    ndf=16)\n",
    "D_decoder = Decoder(image_size, \n",
    "                    nc, \n",
    "                    k=hidden_dim, \n",
    "                    ngf=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "netG = NetG(G_decoder)\n",
    "netD = NetD(D_encoder, D_decoder)\n",
    "one_sided = ONE_SIDED()\n",
    "print(\"netG:\", netG)\n",
    "print(\"\\n \\n netD:\", netD)\n",
    "print(\"\\n \\n oneSide:\", one_sided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "one_sided.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigma for MMD\n",
    "\"\"\"\n",
    "Change this too\n",
    "\"\"\"\n",
    "base = 1.0\n",
    "sigma_list = [1, 2, 4, 8, 16]\n",
    "sigma_list = [sigma / base for sigma in sigma_list]\n",
    "sigma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put variable into cuda device\n",
    "fixed_noise = torch.cuda.FloatTensor(64, nz, 1, 1).normal_(0, 1)\n",
    "one = torch.tensor(1.0).cuda()\n",
    "#one = torch.cuda.FloatTensor([1])\n",
    "mone = one * -1\n",
    "if cuda:\n",
    "    netG.cuda()\n",
    "    netD.cuda()\n",
    "    one_sided.cuda()\n",
    "fixed_noise = Variable(fixed_noise, \n",
    "                       requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimizer\n",
    "optimizerG = torch.optim.RMSprop(netG.parameters(), \n",
    "                                 lr=lr)\n",
    "optimizerD = torch.optim.RMSprop(netD.parameters(), \n",
    "                                 lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What are these?\n",
    "\"\"\"\n",
    "\n",
    "lambda_MMD = 1.0\n",
    "lambda_AE_X = 8.0\n",
    "lambda_AE_Y = 8.0\n",
    "lambda_rg = 16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time = timeit.default_timer()\n",
    "print(\"time = \" + str(time))\n",
    "\n",
    "gen_iterations = 0  # the code default is = 0\n",
    "\n",
    "# lists for tracking - Discriminator side\n",
    "mmd2_D_before_ReLU_list = []\n",
    "mmd2_D_after_ReLU_list = []\n",
    "one_side_errD_list = []\n",
    "L2_AE_X_D_list = []\n",
    "L2_AE_Y_D_list = []\n",
    "errD_list = []\n",
    "\n",
    "# lists for tracking - Generator side\n",
    "mmd2_G_before_ReLU_list = []\n",
    "mmd2_G_after_ReLU_list = []\n",
    "one_side_errG_list = []\n",
    "errG_list = []\n",
    "# errG = torch.Tensor(np.array(0.0))\n",
    "# print(errG.item())\n",
    "\n",
    "# lists for tracking count of nonzero voxels\n",
    "log_nonzero_recon_over_real_list = []\n",
    "\n",
    "# list for tracking gradient norms for generator and discriminator\n",
    "grad_norm_D = []\n",
    "grad_norm_G = []\n",
    "\n",
    "\n",
    "for t in range(max_iter):\n",
    "    print(\"\\n -----------------------------------------------\")\n",
    "    print(\"Epoch = \" + str(t+1) + \" / \" + str(max_iter))\n",
    "    print(\"----------------------------------------------- \\n\")\n",
    "    \n",
    "    data_iter = iter(trn_loader)\n",
    "    print(\"len(trn_loader) = \" + str(len(trn_loader)))\n",
    "    i = 0\n",
    "    plotted = 0\n",
    "    plotted_2 = 0\n",
    "    plotted_3 = 0\n",
    "    plotted_4 = 0   # grad norm plotting controller\n",
    "    \n",
    "    while (i < len(trn_loader)):\n",
    "        \n",
    "        # ---------------------------\n",
    "        #        Optimize over NetD\n",
    "        # ---------------------------\n",
    "        print(\"Optimize over NetD\")\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "            \n",
    "        \"\"\"\n",
    "        What does the below if-else do?\n",
    "        \"\"\"\n",
    "        print(\"gen_iterations = \" + str(gen_iterations))\n",
    "        if gen_iterations < 25 or gen_iterations % 500 == 0:\n",
    "#         if gen_iterations < 3 or gen_iterations % 500 == 0:\n",
    "            Diters = 100\n",
    "            Giters = 1\n",
    "        else:\n",
    "            Diters = 5\n",
    "            Giters = 1\n",
    "\n",
    "        for j in range(Diters):\n",
    "            if i == len(trn_loader):\n",
    "                break\n",
    "\n",
    "            print(\"\\n j / Diter = \" + str(j+1) + \" / \" + str(Diters))\n",
    "            # clamp parameters of NetD encoder to a cube\n",
    "            # do not clamp parameters of NetD decoder!!!\n",
    "            # exactly like numpy.clip()\n",
    "            \"\"\"\n",
    "            Given an interval, values outside the interval are clipped to the interval edges. \n",
    "            For example, if an interval of [0, 1] is specified, values smaller than 0 become 0, \n",
    "            and values larger than 1 become 1.\n",
    "            \"\"\"\n",
    "            for p in netD.encoder.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "            data = data_iter.next()\n",
    "#             print(\"data shape = \" + str(data.shape))\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "            netD.zero_grad()\n",
    "\n",
    "#             x_cpu, _ = data\n",
    "            x_cpu = data\n",
    "            x = Variable(x_cpu.cuda())\n",
    "            batch_size = x.size(0)\n",
    "            print(\"batch_size = \" + str(batch_size))\n",
    "\n",
    "            # output of the discriminator with real data input\n",
    "            \"\"\"\n",
    "            2097152^(1/3) = 128 (= one side of our cube so the\n",
    "            reconstructed cube is the same size as the original one)\n",
    "            This one just acts like an autoencoder\n",
    "            \"\"\"\n",
    "            f_enc_X_D, f_dec_X_D = netD(x)\n",
    "            print(\"netD(x) outputs:\")\n",
    "            print(\"f_enc_X_D size = \" + str(f_enc_X_D.size()))\n",
    "            print(\"f_dec_X_D size = \" + str(f_dec_X_D.size()))\n",
    "            \n",
    "            \n",
    "\n",
    "            noise = torch.cuda.FloatTensor(batch_size, \n",
    "                                            nz, \n",
    "                                            1, \n",
    "                                            1,\n",
    "                                            1).normal_(0, 1)\n",
    "            with torch.no_grad():\n",
    "                #noise = Variable(noise, volatile=True)  # total freeze netG\n",
    "                noise = Variable(noise)\n",
    "#             print(\"noise shape = \" + str(noise.shape))\n",
    "\n",
    "            # output of the generator with noise input\n",
    "            y = Variable(netG(noise).data)\n",
    "#             print(\"y shape = \" + str(y.shape))\n",
    "#             print(\"y[0] shape = \" + str(y[0].shape))\n",
    "#             print(\"y[0][0] shape = \" + str(y[0][0].shape))\n",
    "#             sample_cube_viz = y[0][0].cpu().detach().numpy()\n",
    "#             print(\"sample_cube_viz shape = \" + str(sample_cube_viz.shape))\n",
    "        \n",
    "\n",
    "\n",
    "            # output of the discriminator with noise input\n",
    "            # this tests discriminator \n",
    "            f_enc_Y_D, f_dec_Y_D = netD(y)\n",
    "\n",
    "            # compute biased MMD2 and use ReLU to prevent negative value\n",
    "            mmd2_D = mix_rbf_mmd2(f_enc_X_D, \n",
    "                                  f_enc_Y_D, \n",
    "                                  sigma_list)\n",
    "#             mmd2_D =poly_mmd2(f_enc_X_D, f_enc_Y_D)\n",
    "#             mmd2_D = linear_mmd2(f_enc_X_D, f_enc_Y_D)\n",
    "            \n",
    "            print(\"mmd2_D before ReLU = \" + str(mmd2_D.item()))\n",
    "            mmd2_D_before_ReLU_list.append(mmd2_D.item())\n",
    "            mmd2_D = F.relu(mmd2_D)\n",
    "            print(\"mmd2_D after ReLU = \" + str(mmd2_D.item()))\n",
    "            mmd2_D_after_ReLU_list.append(mmd2_D.item())\n",
    "\n",
    "            # compute rank hinge loss\n",
    "            #print('f_enc_X_D:', f_enc_X_D.size())\n",
    "            #print('f_enc_Y_D:', f_enc_Y_D.size())\n",
    "            one_side_errD = one_sided(f_enc_X_D.mean(0) - f_enc_Y_D.mean(0))\n",
    "            print(\"one_side_errD = \" + str(one_side_errD.item()))\n",
    "            one_side_errD_list.append(one_side_errD.item())\n",
    "            \n",
    "            # compute L2-loss of AE\n",
    "            \"\"\"\n",
    "            These L2 losses are decreasing like a standard optimization\n",
    "            which means that the autoencoder is learning how to encode\n",
    "            and decode using 3D convolutions.\n",
    "            x = batch data\n",
    "            y = cubes generated by the Generator with noise input\n",
    "            f_dec_X_D = \n",
    "            f_dec_Y_D = \n",
    "            \"\"\"\n",
    "            L2_AE_X_D = match(x.view(batch_size, -1), f_dec_X_D, 'L2')\n",
    "            L2_AE_Y_D = match(y.view(batch_size, -1), f_dec_Y_D, 'L2')\n",
    "#             L2_AE_X_D = match(x.view(batch_size, -1), f_dec_X_D, 'L1')\n",
    "#             L2_AE_Y_D = match(y.view(batch_size, -1), f_dec_Y_D, 'L1')\n",
    "            \n",
    "            print(\"L2-loss of AE, L2_AE_X_D = \" + str(L2_AE_X_D.item()))\n",
    "            print(\"L2-loss of AE, L2_AE_Y_D = \" + str(L2_AE_Y_D.item()))\n",
    "            L2_AE_X_D_list.append(L2_AE_X_D.item())\n",
    "            L2_AE_Y_D_list.append(L2_AE_Y_D.item())\n",
    "            \n",
    "\n",
    "\n",
    "#             print(\"lambda_rg = \" + str(lambda_rg))\n",
    "            errD = torch.sqrt(mmd2_D) + lambda_rg * one_side_errD - lambda_AE_X * L2_AE_X_D - lambda_AE_Y * L2_AE_Y_D\n",
    "#             print(\"errD shape = \" + str(errD.shape))\n",
    "            print(\"errD = \" + str(errD.item()))\n",
    "            errD_list.append(errD.item())\n",
    "            errD.backward(mone)\n",
    "            optimizerD.step()\n",
    "            \n",
    "            \n",
    "            # plotting Discriminator plots\n",
    "            if j % 5 == 0 and plotted < 1:\n",
    "                plt.figure(1, figsize = (8,4))\n",
    "                plt.title(\"mmd2_D_before_ReLU_list\")\n",
    "                plt.plot(mmd2_D_before_ReLU_list)\n",
    "                plt.show() \n",
    "                plt.figure(2, figsize = (8,4))\n",
    "                plt.title(\"mmd2_D_after_ReLU_list\")\n",
    "                plt.plot(mmd2_D_after_ReLU_list)\n",
    "                plt.show() \n",
    "                plt.figure(3, figsize = (8,4))\n",
    "                plt.title(\"one_side_errD_list\")\n",
    "                plt.plot(one_side_errD_list)\n",
    "                plt.show() \n",
    "                plt.figure(4, figsize = (8,4))\n",
    "                plt.title(\"L2_AE_X_D_list\")\n",
    "                plt.plot(L2_AE_X_D_list)\n",
    "                plt.show() \n",
    "                plt.figure(5, figsize = (8,4))\n",
    "                plt.title(\"L2_AE_Y_D_list\")\n",
    "                plt.plot(L2_AE_Y_D_list)\n",
    "                plt.show() \n",
    "                plt.figure(6, figsize = (8,4))\n",
    "                plt.title(\"errD_list\")\n",
    "                plt.plot(errD_list)\n",
    "                plt.show() \n",
    "                \n",
    "                # plot output of the discriminator with real data input\n",
    "                # and output of the discriminator with noise input\n",
    "                # on the same histogram \n",
    "                recon_plot = y[0].cpu().view(-1,1).detach().numpy()\n",
    "                real_plot = x[0].cpu().view(-1,1).detach().numpy()\n",
    "                print(\"max(x[0]) = \" + str(max(real_plot)))\n",
    "                print(\"max(y[0]) = \" + str(max(recon_plot)))\n",
    "                print(\"min(x[0]) = \" + str(min(real_plot)))\n",
    "                print(\"min(y[0]) = \" + str(min(recon_plot)))\n",
    "                recon_plot = recon_plot[np.nonzero(recon_plot)]\n",
    "#                 recon_plot = recon_plot[np.greater(recon_plot, 0)]\n",
    "                real_plot = real_plot[np.nonzero(real_plot)]\n",
    "#                 print(\"max(x[0] - nonzero) = \" + str(max(real_plot)))\n",
    "#                 print(\"max(y[0] - nonzero) = \" + str(max(recon_plot)))\n",
    "#                 print(\"min(x[0] - nonzero) = \" + str(min(real_plot)))\n",
    "#                 print(\"min(y[0] - nonzero) = \" + str(min(recon_plot)))\n",
    "#                 recon_plot = recon_plot + 1\n",
    "#                 real_plot = real_plot + 1\n",
    "                \n",
    "                recon_plot = np.log(recon_plot)\n",
    "                real_plot = np.log(real_plot)\n",
    "                print(\"len(real_plot) - nonzero elements = \" + str(len(real_plot)))\n",
    "                print(\"len(recon_plot) - nonzero elements = \" + str(len(recon_plot)))\n",
    "#                 log_nonzero_real_list.append(len(real_plot))\n",
    "#                 log_nonzero_recon_list.append(len(recon_plot))\n",
    "                try:\n",
    "                    log_nonzero_recon_over_real_list.append(len(recon_plot) / len(real_plot))\n",
    "                    print(\"max(real_plot) = \" + str(max(real_plot)))\n",
    "                    print(\"max(recon_plot) = \" + str(max(recon_plot)))\n",
    "                    print(\"min(real_plot) = \" + str(min(real_plot)))\n",
    "                    print(\"min(recon_plot) = \" + str(min(recon_plot)))\n",
    "\n",
    "                    plt.figure(figsize = (16,8))\n",
    "                    plt.title(\"Generator(Noise)\")\n",
    "                    plt.xlim(min(recon_plot.min(),real_plot.min()),\n",
    "                            max(recon_plot.max(),real_plot.max()))\n",
    "                    bins = np.linspace(min(recon_plot.min(),real_plot.min()),\n",
    "                                       max(recon_plot.max(),real_plot.max()), \n",
    "                                       100)\n",
    "                    plt.hist(recon_plot, bins = bins, \n",
    "                             color = \"red\" ,alpha= 0.5, label = \"Reconstructed - Only Nonzero\")\n",
    "                    plt.hist(real_plot, bins = bins, \n",
    "                             color = \"blue\" ,alpha = 0.3, label = \"Real - Only Nonzero\")\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "\n",
    "                    plt.figure(figsize = (16,8))\n",
    "                    plt.title(\"Nonzero in Reconstructed Subcubes / Nonzero in Real Subcubes\")\n",
    "                    plt.ylim(-0.0001, 10)\n",
    "                    plt.plot(log_nonzero_recon_over_real_list, \n",
    "                             color = \"blue\", \n",
    "                             label = \"Nonzero in Reconstructed Subcubes / Nonzero in Real Subcubes\")\n",
    "                    plt.show()  \n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                plotted = plotted + 1\n",
    "                \n",
    "                \n",
    "                \n",
    "            if plotted_2 < 1:\n",
    "\n",
    "#                 print(\"\\n Reconstructed, AutoEncoder Generated Real Cube\")\n",
    "#                 recon_real_viz = visualize_cube(cube=f_dec_X_D[0].cpu().view(128,128,128).detach().numpy(),      ## array name\n",
    "#                                          edge_dim=f_dec_X_D[0].shape[0],        ## edge dimension (128 for 128 x 128 x 128 cube)\n",
    "#                                          start_cube_index_x=0,\n",
    "#                                          start_cube_index_y=0,\n",
    "#                                          start_cube_index_z=0,\n",
    "#                                          fig_size=(10,10),\n",
    "#                                          stdev_to_white=-2,\n",
    "#                                          norm_multiply=1000,\n",
    "#                                          color_map=\"Blues\",\n",
    "#                                          lognormal=False)\n",
    "#                 print(\"\\n Reconstructed, AutoEncoder Generated Noise-Input Cube\")\n",
    "#                 recon_fake_viz = visualize_cube(cube=f_dec_Y_D[0][0].cpu().view(128,128,128).detach().numpy(),      ## array name\n",
    "#                                          edge_dim=f_dec_Y_D[0][0].shape[0],        ## edge dimension (128 for 128 x 128 x 128 cube)\n",
    "#                                          start_cube_index_x=0,\n",
    "#                                          start_cube_index_y=0,\n",
    "#                                          start_cube_index_z=0,\n",
    "#                                          fig_size=(10,10),\n",
    "#                                          stdev_to_white=-2,\n",
    "#                                          norm_multiply=1000,\n",
    "#                                          color_map=\"Blues\",\n",
    "#                                          lognormal=False)\n",
    "                print(\"\\n Noise-Input Generated Cube\")\n",
    "                sample_viz = visualize_cube(cube=y[0][0].cpu().detach().numpy(),      ## array name\n",
    "                                         edge_dim=y[0][0].shape[0],        ## edge dimension (128 for 128 x 128 x 128 cube)\n",
    "                                         start_cube_index_x=0,\n",
    "                                         start_cube_index_y=0,\n",
    "                                         start_cube_index_z=0,\n",
    "                                         fig_size=(10,10),\n",
    "                                         stdev_to_white=-2,\n",
    "                                         norm_multiply=1000,\n",
    "                                         color_map=\"Blues\",\n",
    "                                         lognormal=False)\n",
    "                print(\"\\n Real Cube\")\n",
    "                real_viz = visualize_cube(cube=x[0][0].cpu().detach().numpy(),      ## array name\n",
    "                                         edge_dim=x[0][0].shape[0],        ## edge dimension (128 for 128 x 128 x 128 cube)\n",
    "                                         start_cube_index_x=0,\n",
    "                                         start_cube_index_y=0,\n",
    "                                         start_cube_index_z=0,\n",
    "                                         fig_size=(10,10),\n",
    "                                         stdev_to_white=-2,\n",
    "                                         norm_multiply=1000,\n",
    "                                         color_map=\"Blues\",\n",
    "                                         lognormal=False)\n",
    "#             sample_viz.show()\n",
    "\n",
    "                plotted_2 = plotted_2 + 1 # to limit one 3d plotting per epoch\n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "        print(\"\\n Finished optimizing over NetD \\n\")\n",
    "            \n",
    "\n",
    "        # ---------------------------\n",
    "        #        Optimize over NetG\n",
    "        # ---------------------------\n",
    "        \"\"\"\n",
    "        Because i is increased in each training loop for the\n",
    "        discriminitor, the below condition of if i == len(trn_loader)\n",
    "        is True in every epoch.\n",
    "        Should an i = 0 be added to the beginning of the netG optimization?\n",
    "        Look at paper to see how the training method is.\n",
    "        \"\"\"\n",
    "        print(\"Optimize over NetG\")\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        print(\"Giters = \" + str(Giters))\n",
    "        for j in range(Giters):\n",
    "            print(\"i = \" + str(i))\n",
    "            print(\"len(trn_loader) = \" + str(len(trn_loader)))\n",
    "            if i == len(trn_loader):\n",
    "                print(\"Breaking from the Generator training loop\")\n",
    "                break\n",
    "\n",
    "            print(\"\\n j / Giter = \" + str(j+1) + \" / \" + str(Giters))\n",
    "            data = data_iter.next()\n",
    "            i += 1\n",
    "            netG.zero_grad()\n",
    "\n",
    "#             x_cpu, _ = data\n",
    "            x_cpu = data\n",
    "            x = Variable(x_cpu.cuda())\n",
    "            batch_size = x.size(0)\n",
    "\n",
    "            # output of discriminator with real input\n",
    "            f_enc_X, f_dec_X = netD(x)\n",
    "\n",
    "            noise = torch.cuda.FloatTensor(batch_size, \n",
    "                                           nz, \n",
    "                                           1,\n",
    "                                           1,\n",
    "                                           1).normal_(0, 1)\n",
    "            noise = Variable(noise)\n",
    "            \n",
    "            # output of the generator with noise input\n",
    "            y = netG(noise)\n",
    "\n",
    "            # output of the discriminator with noise input\n",
    "            f_enc_Y, f_dec_Y = netD(y)\n",
    "\n",
    "            # compute biased MMD2 and use ReLU to prevent negative value\n",
    "            mmd2_G = mix_rbf_mmd2(f_enc_X, f_enc_Y, sigma_list)\n",
    "#             mmd2_G = poly_mmd2(f_enc_X, f_enc_Y)\n",
    "#             mmd2_G = linear_mmd2(f_enc_X, f_enc_Y)\n",
    "    \n",
    "            mmd2_G_before_ReLU_list.append(mmd2_G)\n",
    "            mmd2_G = F.relu(mmd2_G)\n",
    "            mmd2_G_after_ReLU_list.append(mmd2_G)\n",
    "\n",
    "            # compute rank hinge loss\n",
    "            one_side_errG = one_sided(f_enc_X.mean(0) - f_enc_Y.mean(0))\n",
    "            one_side_errG_list.append(one_side_errG)\n",
    "\n",
    "            errG = torch.sqrt(mmd2_G) + lambda_rg * one_side_errG\n",
    "            print(\"errG = \" + str(errG))\n",
    "#             print(\"one = \") + str(one)\n",
    "            errG_list.append(errG.item())\n",
    "            errG.backward(one)\n",
    "            optimizerG.step()\n",
    "\n",
    "            gen_iterations += 1\n",
    "            \n",
    "            if plotted_3 < 1:\n",
    "                # plotting Generator plots\n",
    "                plt.figure(1, figsize = (8,4))\n",
    "                plt.title(\"mmd2_G_before_ReLU_list\")\n",
    "                plt.plot(mmd2_G_before_ReLU_list)\n",
    "                plt.show() \n",
    "                plt.figure(2, figsize = (8,4))\n",
    "                plt.title(\"mmd2_G_after_ReLU_list\")\n",
    "                plt.plot(mmd2_G_after_ReLU_list)\n",
    "                plt.show() \n",
    "                plt.figure(3, figsize = (8,4))\n",
    "                plt.title(\"one_side_errG_list\")\n",
    "                plt.plot(one_side_errG_list)\n",
    "                plt.show() \n",
    "                plt.figure(4, figsize = (8,4))\n",
    "                plt.title(\"errG_list\")\n",
    "                plt.plot(errG_list)\n",
    "                plt.show()            \n",
    "            \n",
    "                plotted_3 = plotted_3 + 1\n",
    "\n",
    "        run_time = (timeit.default_timer() - time) / 60.0\n",
    "        print(\"run_time = \" + str(run_time))\n",
    "        try:\n",
    "            print('[%3d/%3d][%3d/%3d] [%5d] (%.2f m) MMD2_D %.6f hinge %.6f L2_AE_X %.6f L2_AE_Y %.6f loss_D %.6f Loss_G %.6f f_X %.6f f_Y %.6f |gD| %.4f |gG| %.4f'\n",
    "    #                   % (t, max_iter, i, len(trn_loader), gen_iterations, run_time,\n",
    "    #                      mmd2_D.data[0], one_side_errD.data[0],\n",
    "    #                      L2_AE_X_D.data[0], L2_AE_Y_D.data[0],\n",
    "    #                      errD.data[0], errG.data[0],\n",
    "    #                      f_enc_X_D.mean().data[0], f_enc_Y_D.mean().data[0],\n",
    "    #                      grad_norm(netD), grad_norm(netG)))\n",
    "                % (t, max_iter, i, len(trn_loader), gen_iterations, run_time,\n",
    "                     mmd2_D.item(), one_side_errD.item(),\n",
    "                     L2_AE_X_D.item(), L2_AE_Y_D.item(),\n",
    "                     errD.item(), errG.item(),\n",
    "                     f_enc_X_D.mean().item(), f_enc_Y_D.mean().item(),\n",
    "                     grad_norm(netD), grad_norm(netG)))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        \n",
    "#         if gen_iterations % 500 == 0:\n",
    "#             y_fixed = netG(fixed_noise)\n",
    "#             y_fixed.data = y_fixed.data.mul(0.5).add(0.5)\n",
    "#             f_dec_X_D = f_dec_X_D.view(f_dec_X_D.size(0), args.nc, args.image_size, args.image_size)\n",
    "#             f_dec_X_D.data = f_dec_X_D.data.mul(0.5).add(0.5)\n",
    "#             vutils.save_image(y_fixed.data, '{0}/fake_samples_{1}.png'.format(args.experiment, gen_iterations))\n",
    "#             vutils.save_image(f_dec_X_D.data, '{0}/decode_samples_{1}.png'.format(args.experiment, gen_iterations))\n",
    "\n",
    "        # plotting gradient norms for monitoring\n",
    "        grad_norm_D.append(grad_norm(netD))\n",
    "        grad_norm_G.append(grad_norm(netG))\n",
    "        \n",
    "        if plotted_4 < 1:\n",
    "            plt.figure(1, figsize = (8,4))\n",
    "            plt.title(\"grad_norm_D\")\n",
    "            plt.plot(grad_norm_D)\n",
    "            plt.show() \n",
    "            plt.figure(2, figsize = (8,4))\n",
    "            plt.title(\"grad_norm_G\")\n",
    "            plt.plot(grad_norm_G)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    if t % 5 == 0:\n",
    "        torch.save(netG.state_dict(), '{0}/netG_iter_{1}.pth'.format(experiment, t))\n",
    "        torch.save(netD.state_dict(), '{0}/netD_iter_{1}.pth'.format(experiment, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
