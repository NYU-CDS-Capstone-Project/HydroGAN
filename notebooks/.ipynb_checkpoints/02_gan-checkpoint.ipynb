{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/examples/blob/master/dcgan/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--dataset', required=True, help='cifar10 | lsun | imagenet | folder | lfw | fake')\n",
    "# parser.add_argument('--dataroot', required=True, help='path to dataset')\n",
    "# parser.add_argument('--workers', type=int, help='number of data loading workers', default=2)\n",
    "# parser.add_argument('--batchSize', type=int, default=64, help='input batch size')\n",
    "# parser.add_argument('--imageSize', type=int, default=64, help='the height / width of the input image to network')\n",
    "# parser.add_argument('--nz', type=int, default=100, help='size of the latent z vector')\n",
    "# parser.add_argument('--ngf', type=int, default=64)\n",
    "# parser.add_argument('--ndf', type=int, default=64)\n",
    "# parser.add_argument('--niter', type=int, default=25, help='number of epochs to train for')\n",
    "# parser.add_argument('--lr', type=float, default=0.0002, help='learning rate, default=0.0002')\n",
    "# parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "# parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
    "# parser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')\n",
    "# parser.add_argument('--netG', default='', help=\"path to netG (to continue training)\")\n",
    "# parser.add_argument('--netD', default='', help=\"path to netD (to continue training)\")\n",
    "# parser.add_argument('--outf', default='.', help='folder to output images and model checkpoints')\n",
    "# parser.add_argument('--manualSeed', type=int, help='manual seed')\n",
    "\n",
    "# opt = parser.parse_args()\n",
    "# print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \n",
    "# dataroot = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workers = 1\n",
    "batchSize = 1\n",
    "imageSize = 128\n",
    "nz = 10         # 'size of the latent z vector'\n",
    "ngf = 8         # dimension of the cube\n",
    "ndf = 8         # dimension of the cube\n",
    "niter = 25       # 'number of epochs to train for'\n",
    "lr = 0.0002      # 'learning rate, default=0.0002'\n",
    "beta1 = 0.5      # 'beta1 for adam. default=0.5'\n",
    "cuda = False\n",
    "ngpu = 0      \n",
    "netG_arg = \"\"        # \"path to netG (to continue training)\"\n",
    "netD_arg = \"\"        # \"path to netD (to continue training)\"\n",
    "outf = \".\"       # 'folder to output images and model checkpoints'\n",
    "manualSeed = 1   # 'manual seed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "#     os.makedirs(opt.outf)\n",
    "    os.makedirs(outf)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x110252150>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if opt.manualSeed is None:\n",
    "#     opt.manualSeed = random.randint(1, 10000)\n",
    "# print(\"Random Seed: \", opt.manualSeed)\n",
    "# random.seed(opt.manualSeed)\n",
    "# torch.manual_seed(opt.manualSeed)\n",
    "\n",
    "if manualSeed is None:\n",
    "    manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() and not opt.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_test(s_test, s_train):\n",
    "    #2048/16=128\n",
    "    m=8\n",
    "    x=random.randint(0,m)*s_train\n",
    "    y=random.randint(0,m)*s_train\n",
    "    z=random.randint(0,m)*s_train\n",
    "    #print(x,y,z)\n",
    "    return {'x':[x,x+s_test], 'y':[y,y+s_test], 'z':[z,z+s_test]}\n",
    "\n",
    "def check_coords(test_coords, train_coords):\n",
    "    valid=True\n",
    "    for i in ['x','y','z']:\n",
    "        r=(max(test_coords[i][0], \n",
    "               train_coords[i][0]), \n",
    "           min(test_coords[i][1],\n",
    "               train_coords[i][1]))\n",
    "        if r[0]<=r[1]:\n",
    "            valid=False\n",
    "    return valid\n",
    "\n",
    "def get_samples(s_sample, nsamples, redshift, test_coords):\n",
    "    #n is size of minibatch, get valid samples (not intersecting with test_coords)\n",
    "    sample_list=[]\n",
    "    m=2048-128\n",
    "    for n in range(nsamples):\n",
    "        #print(\"Sample No = \" + str(n + 1) + \" / \" + str(nsamples))\n",
    "        sample_valid=False\n",
    "        while sample_valid==False:\n",
    "            x = random.randint(0,m)\n",
    "            y = random.randint(0,m)\n",
    "            z = random.randint(0,m)\n",
    "            sample_coords = {'x':[x,x+s_sample], \n",
    "                             'y':[y,y+s_sample], \n",
    "                             'z':[z,z+s_sample]}\n",
    "            \n",
    "            sample_valid = check_coords(test_coords, \n",
    "                                        sample_coords)\n",
    "        \n",
    "        sample_list.append(sample_coords)\n",
    "    \n",
    "    print(\"Sampling finished.\")\n",
    "        \n",
    "    #Load cube and get samples and convert them to np.arrays\n",
    "    sample_array=[]\n",
    "    datapath=''\n",
    "    f = h5py.File(datapath+'fields_z='+redshift+'.hdf5', 'r')\n",
    "    f=f['delta_HI']\n",
    "    \n",
    "    # getting the max of the whole cube\n",
    "    #print(f.shape)\n",
    "    max_list = []\n",
    "    for i in range(f.shape[0]):\n",
    "        #print(np.max(f[i:i+1,:,:]))\n",
    "        max_list.append(np.max(f[i:i+1,:,:]))\n",
    "    max_cube = max(max_list)\n",
    "    #f.close()\n",
    "    \n",
    "    print(\"Getting max value finished.\")\n",
    "    \n",
    "    counter = 0\n",
    "    for c in sample_list:\n",
    "        print(\"Counter = \" + str(counter + 1) + \" / \" + str(len(sample_list)))\n",
    "        a = f[c['x'][0]:c['x'][1],\n",
    "              c['y'][0]:c['y'][1],\n",
    "              c['z'][0]:c['z'][1]]\n",
    "        \n",
    "        # a = np.array(a)\n",
    "        a = np.array(a) / max_cube\n",
    "        sample_array.append(a)\n",
    "    \n",
    "        counter = counter + 1\n",
    "        \n",
    "    f=0\n",
    "    return sample_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HydrogenDataset2(Dataset):\n",
    "    \"\"\"Hydrogen Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, h5_file, root_dir, s_test, s_train,\n",
    "                 s_sample, nsamples, redshift):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h5_file (string): name of the h5 file with 32 sampled cubes.\n",
    "            root_dir (string): Directory with the .h5 file.\n",
    "        \"\"\"\n",
    "        file_size = os.path.getsize(root_dir + h5_file) / 1e6 # in MBs\n",
    "        print(\"The whole file size is \" + str(int(file_size)) + \" MBs\")\n",
    "        \n",
    "        # self.subcubes = h5py.File('../data/sample_32.h5', 'r')\n",
    "        self.subcubes = h5py.File(root_dir + h5_file, 'r')[\"delta_HI\"]\n",
    "        self.h5_file = h5_file\n",
    "        self.root_dir = root_dir\n",
    "        self.s_test = s_test\n",
    "        self.s_train = s_train\n",
    "        self.t_coords = define_test(self.s_test,\n",
    "                                    self.s_train)\n",
    "        self.s_sample = s_sample\n",
    "        self.nsamples = nsamples\n",
    "        self.redshift = redshift\n",
    "        \n",
    "        self.samples = get_samples(s_sample = self.s_sample,\n",
    "                             nsamples = self.nsamples,\n",
    "                             redshift = self.redshift,\n",
    "                             test_coords = self.t_coords)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Function called when len(self) is executed\n",
    "        \n",
    "        #print(len(self.subcubes))\n",
    "#         return len(self.nsamples)\n",
    "        return self.nsamples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        This can be implemented in such a way that the whole h5 file read \n",
    "        using h5py.File() and get_sample() function is called to return\n",
    "        a random subcube. This won't increase memory usage because the\n",
    "        subcubes will be read in the same way and only the batch will\n",
    "        be read into memory.\n",
    "        \n",
    "        Here we have implemented it so that it can be used with data\n",
    "        generated by get_sample() function.\n",
    "        \n",
    "        The output of this function is one subcube with the dimensions\n",
    "        specified by get_sample() implementation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # default version -> error in training because of dimensions\n",
    "#         sample = self.subcubes[idx]\n",
    "        \n",
    "        # reshaped version to add another dimension\n",
    "#         sample = self.subcubes[idx].reshape((1,128,128,128))\n",
    "\n",
    "        # On prince using get_samples()\n",
    "#         print(\"nsamples = \" + str(self.nsamples))\n",
    "#         sample = get_samples(s_sample = self.s_sample,\n",
    "#                              nsamples = self.nsamples,\n",
    "#                              redshift = self.redshift,\n",
    "#                              test_coords = self.t_coords)\n",
    "    \n",
    "        sample = self.samples[idx].reshape((1,128,128,128))\n",
    "        \n",
    "        # added division by 1e6 for exploding variance\n",
    "        # and resulting in inf during reparametrization trick part\n",
    "        sample = sample/1e6\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HydrogenDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, h5_file, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h5_file (string): name of the h5 file with 32 sampled cubes.\n",
    "            root_dir (string): Directory with the .h5 file.\n",
    "        \"\"\"\n",
    "        file_size = os.path.getsize(root_dir + h5_file) / 1e6 # in MBs\n",
    "        print(\"The file size is \" + str(int(file_size)) + \" MBs\")\n",
    "        \n",
    "        # self.subcubes = h5py.File('../data/sample_32.h5', 'r')\n",
    "        self.subcubes = h5py.File(root_dir + h5_file, 'r')['sample32']\n",
    "        self.h5_file = h5_file\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        # Function called when len(self) is executed\n",
    "        \n",
    "        #print(len(self.subcubes))\n",
    "        return len(self.subcubes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        This can be implemented in such a way that the whole h5 file read \n",
    "        using h5py.File() and get_sample() function is called to return\n",
    "        a random subcube. This won't increase memory usage because the\n",
    "        subcubes will be read in the same way and only the batch will\n",
    "        be read into memory.\n",
    "        \n",
    "        Here we have implemented it so that it can be used with data\n",
    "        generated by get_sample() function.\n",
    "        \n",
    "        The output of this function is one subcube with the dimensions\n",
    "        specified by get_sample() implementation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # default version -> error in training because of dimensions\n",
    "        #sample = self.subcubes[idx]\n",
    "        \n",
    "        # reshaped version to add another dimension\n",
    "        sample = self.subcubes[idx].reshape((1,128,128,128))\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fields_z=1.0.hdf5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-3c85f3c774f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                     \u001b[0ms_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                     \u001b[0mnsamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                     redshift = \"1.0\")\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-2bdb40001a8a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, h5_file, root_dir, s_test, s_train, s_sample, nsamples, redshift)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mroot_dir\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDirectory\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mh5\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \"\"\"\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mfile_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh5_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1e6\u001b[0m \u001b[0;31m# in MBs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The whole file size is \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" MBs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/genericpath.py\u001b[0m in \u001b[0;36mgetsize\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;34m\"\"\"Return the size of a file, reported by os.stat().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fields_z=1.0.hdf5'"
     ]
    }
   ],
   "source": [
    "# on prince\n",
    "sampled_subcubes = HydrogenDataset2(h5_file=\"fields_z=1.0.hdf5\",\n",
    "                                    root_dir = \"\",\n",
    "                                    s_test = 1024, \n",
    "                                    s_train = 128,\n",
    "                                    s_sample = 128, \n",
    "                                    nsamples = 128, \n",
    "                                    redshift = \"1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file size is 268 MBs\n"
     ]
    }
   ],
   "source": [
    "# on local\n",
    "sampled_subcubes = HydrogenDataset(h5_file=\"sample_32.h5\",\n",
    "                                    root_dir = \"../data/\")\n",
    "dataset = sampled_subcubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                         #batch_size=opt.batchSize,\n",
    "                                         batch_size = batchSize,\n",
    "                                         shuffle=True, \n",
    "                                         #num_workers=int(opt.workers)\n",
    "                                         num_workers=int(workers)\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if opt.cuda else \"cpu\")\n",
    "# ngpu = int(opt.ngpu)\n",
    "# nz = int(opt.nz)\n",
    "# ngf = int(opt.ngf)\n",
    "# ndf = int(opt.ndf)\n",
    "# nc = 3\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "ngpu = int(ngpu)\n",
    "nz = int(nz)\n",
    "ngf = int(ngf)\n",
    "ndf = int(ndf)\n",
    "\n",
    "# number of channels\n",
    "nc = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose3d(in_channels=nz,\n",
    "                               out_channels=ngf * 16,\n",
    "                               kernel_size = 4,\n",
    "                               stride = 1,\n",
    "                               padding = 0, \n",
    "                               bias=False),\n",
    "            nn.BatchNorm3d(ngf * 16),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose3d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm3d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose3d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm3d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose3d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm3d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose3d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm3d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose3d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        print(\"Forward Propagation of Generator\")\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, \n",
    "                                               input, \n",
    "                                               range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose3d(10, 128, kernel_size=(4, 4, 4), stride=(1, 1, 1), bias=False)\n",
      "    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): ConvTranspose3d(128, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): ConvTranspose3d(64, 32, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "    (7): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): ConvTranspose3d(32, 16, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "    (10): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace)\n",
      "    (12): ConvTranspose3d(16, 8, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "    (13): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU(inplace)\n",
      "    (15): ConvTranspose3d(8, 1, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "    (16): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netG = Generator(ngpu).to(device)\n",
    "netG.apply(weights_init)\n",
    "# if opt.netG != '':\n",
    "#     netG.load_state_dict(torch.load(opt.netG))\n",
    "if netG_arg != '':\n",
    "    netG.load_state_dict(torch.load(netG_param))\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            # nc = number of channels\n",
    "            \n",
    "            nn.Conv3d(in_channels=1, \n",
    "                      out_channels=8, \n",
    "                      kernel_size=(4,4,4), # == 4\n",
    "                      stride = (2,2,2), # == 2\n",
    "                      padding=(1,1,1), # == 1\n",
    "                      bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv3d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm3d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv3d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm3d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv3d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm3d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv3d(ndf * 8, ndf * 16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm3d(ndf * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv3d(ndf * 16, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, \n",
    "                                               input, \n",
    "                                               range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv3d(1, 8, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (2): Conv3d(8, 16, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "    (3): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (5): Conv3d(16, 32, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "    (6): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (8): Conv3d(32, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "    (9): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (11): Conv3d(64, 128, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "    (12): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (14): Conv3d(128, 1, kernel_size=(4, 4, 4), stride=(1, 1, 1), bias=False)\n",
      "    (15): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netD = Discriminator(ngpu).to(device)\n",
    "netD.apply(weights_init)\n",
    "if netD_arg != '':\n",
    "    netD.load_state_dict(torch.load(netD_arg))\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fixed_noise = torch.randn(batchSize, \n",
    "#                           nz, \n",
    "#                           1, \n",
    "#                           1, \n",
    "#                           device=device)\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), \n",
    "                        lr=lr, \n",
    "                        betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), \n",
    "                        lr=lr, \n",
    "                        betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape = torch.Size([1, 1, 128, 128, 128])\n",
      "batch_size = 1\n",
      "Discriminator real label = tensor([1.])\n",
      "Discriminator output shape = torch.Size([1])\n",
      "Discriminator error (input = real) = tensor(0.5270, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_x = 0.5903812646865845\n",
      "batch_size = 1\n",
      "nz = 10\n",
      "Noise shape = torch.Size([1, 10, 1, 1, 1])\n",
      "Forward Propagation of Generator\n",
      "Generator output size = torch.Size([1, 1, 128, 128, 128])\n",
      "Discriminator error (input = generated by fake noise) = tensor(0.4572, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_G_z1 = 0.3669271171092987\n",
      "errD = errD_real + errD_fake = tensor(0.9842, grad_fn=<ThAddBackward>)\n",
      "[0/25][0/32] Loss_D: 0.9842 Loss_G: 12.8205 D(x): 0.5904 D(G(z)): 0.3669 / 0.0000\n",
      "Data Shape = torch.Size([1, 1, 128, 128, 128])\n",
      "batch_size = 1\n",
      "Discriminator real label = tensor([1.])\n",
      "Discriminator output shape = torch.Size([1])\n",
      "Discriminator error (input = real) = tensor(0.9658, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_x = 0.3806743621826172\n",
      "batch_size = 1\n",
      "nz = 10\n",
      "Noise shape = torch.Size([1, 10, 1, 1, 1])\n",
      "Forward Propagation of Generator\n",
      "Generator output size = torch.Size([1, 1, 128, 128, 128])\n",
      "Discriminator error (input = generated by fake noise) = tensor(0.0670, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_G_z1 = 0.06478135287761688\n",
      "errD = errD_real + errD_fake = tensor(1.0328, grad_fn=<ThAddBackward>)\n",
      "[0/25][1/32] Loss_D: 1.0328 Loss_G: 5.7110 D(x): 0.3807 D(G(z)): 0.0648 / 0.0033\n",
      "Data Shape = torch.Size([1, 1, 128, 128, 128])\n",
      "batch_size = 1\n",
      "Discriminator real label = tensor([1.])\n",
      "Discriminator output shape = torch.Size([1])\n",
      "Discriminator error (input = real) = tensor(0.1046, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_x = 0.9007107019424438\n",
      "batch_size = 1\n",
      "nz = 10\n",
      "Noise shape = torch.Size([1, 10, 1, 1, 1])\n",
      "Forward Propagation of Generator\n",
      "Generator output size = torch.Size([1, 1, 128, 128, 128])\n",
      "Discriminator error (input = generated by fake noise) = tensor(0.2465, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_G_z1 = 0.21846939623355865\n",
      "errD = errD_real + errD_fake = tensor(0.3511, grad_fn=<ThAddBackward>)\n",
      "[0/25][2/32] Loss_D: 0.3511 Loss_G: 8.3033 D(x): 0.9007 D(G(z)): 0.2185 / 0.0002\n",
      "Data Shape = torch.Size([1, 1, 128, 128, 128])\n",
      "batch_size = 1\n",
      "Discriminator real label = tensor([1.])\n",
      "Discriminator output shape = torch.Size([1])\n",
      "Discriminator error (input = real) = tensor(0.1531, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_x = 0.8580207228660583\n",
      "batch_size = 1\n",
      "nz = 10\n",
      "Noise shape = torch.Size([1, 10, 1, 1, 1])\n",
      "Forward Propagation of Generator\n",
      "Generator output size = torch.Size([1, 1, 128, 128, 128])\n",
      "Discriminator error (input = generated by fake noise) = tensor(0.0372, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_G_z1 = 0.036526795476675034\n",
      "errD = errD_real + errD_fake = tensor(0.1903, grad_fn=<ThAddBackward>)\n",
      "[0/25][3/32] Loss_D: 0.1903 Loss_G: 5.3076 D(x): 0.8580 D(G(z)): 0.0365 / 0.0050\n",
      "Data Shape = torch.Size([1, 1, 128, 128, 128])\n",
      "batch_size = 1\n",
      "Discriminator real label = tensor([1.])\n",
      "Discriminator output shape = torch.Size([1])\n",
      "Discriminator error (input = real) = tensor(0.7831, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_x = 0.45700886845588684\n",
      "batch_size = 1\n",
      "nz = 10\n",
      "Noise shape = torch.Size([1, 10, 1, 1, 1])\n",
      "Forward Propagation of Generator\n",
      "Generator output size = torch.Size([1, 1, 128, 128, 128])\n",
      "Discriminator error (input = generated by fake noise) = tensor(0.0238, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_G_z1 = 0.023529132828116417\n",
      "errD = errD_real + errD_fake = tensor(0.8069, grad_fn=<ThAddBackward>)\n",
      "[0/25][4/32] Loss_D: 0.8069 Loss_G: 4.6984 D(x): 0.4570 D(G(z)): 0.0235 / 0.0091\n",
      "Data Shape = torch.Size([1, 1, 128, 128, 128])\n",
      "batch_size = 1\n",
      "Discriminator real label = tensor([1.])\n",
      "Discriminator output shape = torch.Size([1])\n",
      "Discriminator error (input = real) = tensor(0.3241, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_x = 0.7232063412666321\n",
      "batch_size = 1\n",
      "nz = 10\n",
      "Noise shape = torch.Size([1, 10, 1, 1, 1])\n",
      "Forward Propagation of Generator\n",
      "Generator output size = torch.Size([1, 1, 128, 128, 128])\n",
      "Discriminator error (input = generated by fake noise) = tensor(0.1237, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_G_z1 = 0.11635886877775192\n",
      "errD = errD_real + errD_fake = tensor(0.4478, grad_fn=<ThAddBackward>)\n",
      "[0/25][5/32] Loss_D: 0.4478 Loss_G: 5.7429 D(x): 0.7232 D(G(z)): 0.1164 / 0.0032\n",
      "Data Shape = torch.Size([1, 1, 128, 128, 128])\n",
      "batch_size = 1\n",
      "Discriminator real label = tensor([1.])\n",
      "Discriminator output shape = torch.Size([1])\n",
      "Discriminator error (input = real) = tensor(1.9417, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_x = 0.14345906674861908\n",
      "batch_size = 1\n",
      "nz = 10\n",
      "Noise shape = torch.Size([1, 10, 1, 1, 1])\n",
      "Forward Propagation of Generator\n",
      "Generator output size = torch.Size([1, 1, 128, 128, 128])\n",
      "Discriminator error (input = generated by fake noise) = tensor(0.1659, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_G_z1 = 0.15286624431610107\n",
      "errD = errD_real + errD_fake = tensor(2.1076, grad_fn=<ThAddBackward>)\n",
      "[0/25][6/32] Loss_D: 2.1076 Loss_G: 5.7452 D(x): 0.1435 D(G(z)): 0.1529 / 0.0032\n",
      "Data Shape = torch.Size([1, 1, 128, 128, 128])\n",
      "batch_size = 1\n",
      "Discriminator real label = tensor([1.])\n",
      "Discriminator output shape = torch.Size([1])\n",
      "Discriminator error (input = real) = tensor(0.4834, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_x = 0.6166820526123047\n",
      "batch_size = 1\n",
      "nz = 10\n",
      "Noise shape = torch.Size([1, 10, 1, 1, 1])\n",
      "Forward Propagation of Generator\n",
      "Generator output size = torch.Size([1, 1, 128, 128, 128])\n",
      "Discriminator error (input = generated by fake noise) = tensor(0.0427, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_G_z1 = 0.04183401167392731\n",
      "errD = errD_real + errD_fake = tensor(0.5261, grad_fn=<ThAddBackward>)\n",
      "[0/25][7/32] Loss_D: 0.5261 Loss_G: 4.6019 D(x): 0.6167 D(G(z)): 0.0418 / 0.0100\n",
      "Data Shape = torch.Size([1, 1, 128, 128, 128])\n",
      "batch_size = 1\n",
      "Discriminator real label = tensor([1.])\n",
      "Discriminator output shape = torch.Size([1])\n",
      "Discriminator error (input = real) = tensor(0.7974, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_x = 0.45050671696662903\n",
      "batch_size = 1\n",
      "nz = 10\n",
      "Noise shape = torch.Size([1, 10, 1, 1, 1])\n",
      "Forward Propagation of Generator\n",
      "Generator output size = torch.Size([1, 1, 128, 128, 128])\n",
      "Discriminator error (input = generated by fake noise) = tensor(0.1146, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_G_z1 = 0.10831088572740555\n",
      "errD = errD_real + errD_fake = tensor(0.9120, grad_fn=<ThAddBackward>)\n",
      "[0/25][8/32] Loss_D: 0.9120 Loss_G: 4.5975 D(x): 0.4505 D(G(z)): 0.1083 / 0.0101\n",
      "Data Shape = torch.Size([1, 1, 128, 128, 128])\n",
      "batch_size = 1\n",
      "Discriminator real label = tensor([1.])\n",
      "Discriminator output shape = torch.Size([1])\n",
      "Discriminator error (input = real) = tensor(0.2555, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_x = 0.7745633721351624\n",
      "batch_size = 1\n",
      "nz = 10\n",
      "Noise shape = torch.Size([1, 10, 1, 1, 1])\n",
      "Forward Propagation of Generator\n",
      "Generator output size = torch.Size([1, 1, 128, 128, 128])\n",
      "Discriminator error (input = generated by fake noise) = tensor(0.1030, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_G_z1 = 0.0978650450706482\n",
      "errD = errD_real + errD_fake = tensor(0.3584, grad_fn=<ThAddBackward>)\n",
      "[0/25][9/32] Loss_D: 0.3584 Loss_G: 5.1300 D(x): 0.7746 D(G(z)): 0.0979 / 0.0059\n",
      "Data Shape = torch.Size([1, 1, 128, 128, 128])\n",
      "batch_size = 1\n",
      "Discriminator real label = tensor([1.])\n",
      "Discriminator output shape = torch.Size([1])\n",
      "Discriminator error (input = real) = tensor(1.4003, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_x = 0.24651981890201569\n",
      "batch_size = 1\n",
      "nz = 10\n",
      "Noise shape = torch.Size([1, 10, 1, 1, 1])\n",
      "Forward Propagation of Generator\n",
      "Generator output size = torch.Size([1, 1, 128, 128, 128])\n",
      "Discriminator error (input = generated by fake noise) = tensor(0.2580, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_G_z1 = 0.22742605209350586\n",
      "errD = errD_real + errD_fake = tensor(1.6583, grad_fn=<ThAddBackward>)\n",
      "[0/25][10/32] Loss_D: 1.6583 Loss_G: 6.3475 D(x): 0.2465 D(G(z)): 0.2274 / 0.0018\n",
      "Data Shape = torch.Size([1, 1, 128, 128, 128])\n",
      "batch_size = 1\n",
      "Discriminator real label = tensor([1.])\n",
      "Discriminator output shape = torch.Size([1])\n",
      "Discriminator error (input = real) = tensor(1.2441, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_x = 0.28821131587028503\n",
      "batch_size = 1\n",
      "nz = 10\n",
      "Noise shape = torch.Size([1, 10, 1, 1, 1])\n",
      "Forward Propagation of Generator\n",
      "Generator output size = torch.Size([1, 1, 128, 128, 128])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator error (input = generated by fake noise) = tensor(2.4976, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_G_z1 = 0.91771399974823\n",
      "errD = errD_real + errD_fake = tensor(3.7416, grad_fn=<ThAddBackward>)\n",
      "[0/25][11/32] Loss_D: 3.7416 Loss_G: 11.6538 D(x): 0.2882 D(G(z)): 0.9177 / 0.0000\n",
      "Data Shape = torch.Size([1, 1, 128, 128, 128])\n",
      "batch_size = 1\n",
      "Discriminator real label = tensor([1.])\n",
      "Discriminator output shape = torch.Size([1])\n",
      "Discriminator error (input = real) = tensor(0.3697, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_x = 0.6909088492393494\n",
      "batch_size = 1\n",
      "nz = 10\n",
      "Noise shape = torch.Size([1, 10, 1, 1, 1])\n",
      "Forward Propagation of Generator\n",
      "Generator output size = torch.Size([1, 1, 128, 128, 128])\n",
      "Discriminator error (input = generated by fake noise) = tensor(0.2572, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_G_z1 = 0.22682443261146545\n",
      "errD = errD_real + errD_fake = tensor(0.6270, grad_fn=<ThAddBackward>)\n",
      "[0/25][12/32] Loss_D: 0.6270 Loss_G: 6.9945 D(x): 0.6909 D(G(z)): 0.2268 / 0.0009\n",
      "Data Shape = torch.Size([1, 1, 128, 128, 128])\n",
      "batch_size = 1\n",
      "Discriminator real label = tensor([1.])\n",
      "Discriminator output shape = torch.Size([1])\n",
      "Discriminator error (input = real) = tensor(1.3561, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_x = 0.257673442363739\n",
      "batch_size = 1\n",
      "nz = 10\n",
      "Noise shape = torch.Size([1, 10, 1, 1, 1])\n",
      "Forward Propagation of Generator\n",
      "Generator output size = torch.Size([1, 1, 128, 128, 128])\n",
      "Discriminator error (input = generated by fake noise) = tensor(0.0484, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_G_z1 = 0.04723810404539108\n",
      "errD = errD_real + errD_fake = tensor(1.4045, grad_fn=<ThAddBackward>)\n",
      "[0/25][13/32] Loss_D: 1.4045 Loss_G: 4.7105 D(x): 0.2577 D(G(z)): 0.0472 / 0.0090\n",
      "Data Shape = torch.Size([1, 1, 128, 128, 128])\n",
      "batch_size = 1\n",
      "Discriminator real label = tensor([1.])\n",
      "Discriminator output shape = torch.Size([1])\n",
      "Discriminator error (input = real) = tensor(0.1156, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_x = 0.8908140659332275\n",
      "batch_size = 1\n",
      "nz = 10\n",
      "Noise shape = torch.Size([1, 10, 1, 1, 1])\n",
      "Forward Propagation of Generator\n",
      "Generator output size = torch.Size([1, 1, 128, 128, 128])\n",
      "Discriminator error (input = generated by fake noise) = tensor(0.0752, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_G_z1 = 0.07246764749288559\n",
      "errD = errD_real + errD_fake = tensor(0.1908, grad_fn=<ThAddBackward>)\n",
      "[0/25][14/32] Loss_D: 0.1908 Loss_G: 4.6375 D(x): 0.8908 D(G(z)): 0.0725 / 0.0097\n",
      "Data Shape = torch.Size([1, 1, 128, 128, 128])\n",
      "batch_size = 1\n",
      "Discriminator real label = tensor([1.])\n",
      "Discriminator output shape = torch.Size([1])\n",
      "Discriminator error (input = real) = tensor(1.6471, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_x = 0.19260291755199432\n",
      "batch_size = 1\n",
      "nz = 10\n",
      "Noise shape = torch.Size([1, 10, 1, 1, 1])\n",
      "Forward Propagation of Generator\n",
      "Generator output size = torch.Size([1, 1, 128, 128, 128])\n",
      "Discriminator error (input = generated by fake noise) = tensor(0.0020, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "D_G_z1 = 0.002038845093920827\n",
      "errD = errD_real + errD_fake = tensor(1.6492, grad_fn=<ThAddBackward>)\n",
      "[0/25][15/32] Loss_D: 1.6492 Loss_G: 6.2404 D(x): 0.1926 D(G(z)): 0.0020 / 0.0019\n",
      "Data Shape = torch.Size([1, 1, 128, 128, 128])\n",
      "batch_size = 1\n",
      "Discriminator real label = tensor([1.])\n",
      "Discriminator output shape = torch.Size([1])\n",
      "Discriminator error (input = real) = tensor(0.2673, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/atakanokan/anaconda/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/atakanokan/anaconda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/atakanokan/anaconda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/atakanokan/anaconda/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/Users/atakanokan/anaconda/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/Users/atakanokan/anaconda/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/Users/atakanokan/anaconda/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Users/atakanokan/anaconda/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-d50cbb7c0456>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Discriminator error (input = real) = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrD_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0merrD_real\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mD_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for epoch in range(opt.niter):\n",
    "for epoch in range(niter):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        print(\"\")\n",
    "        \n",
    "        print(\"Data Shape = \" + str(data.shape))\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        #real_cpu = data[0].to(device)\n",
    "        real_cpu = data.to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        print(\"batch_size = \" + str(batch_size))\n",
    "        \n",
    "        # torch.full -> \n",
    "        # Returns a tensor of size size filled with fill_value\n",
    "        label = torch.full(size = (batch_size,), \n",
    "                           fill_value = real_label, \n",
    "                           device = device)\n",
    "        print(\"Discriminator real label = \" + str(label))\n",
    "\n",
    "        output = netD(real_cpu)\n",
    "        print(\"Discriminator output shape = \" + str(output.shape))\n",
    "        \n",
    "        \n",
    "        errD_real = criterion(output, label)\n",
    "        print(\"Discriminator error (input = real) = \" + str(errD_real))\n",
    "        \n",
    "        errD_real.backward()\n",
    "        \n",
    "        D_x = output.mean().item()\n",
    "        print(\"D_x = \" + str(D_x))\n",
    "\n",
    "        # train with fake\n",
    "        print(\"batch_size = \" + str(batch_size))\n",
    "        print(\"nz = \" + str(nz))\n",
    "        noise = torch.randn(batch_size, \n",
    "                            nz, \n",
    "                            1, \n",
    "                            1,\n",
    "                            1,\n",
    "                            device=device)\n",
    "        print(\"Noise shape = \" + str(noise.shape))\n",
    "\n",
    "        \n",
    "        fake = netG(noise)\n",
    "        print(\"Generator output size = \" + str(fake.shape))\n",
    "    \n",
    "        # change label from real label (=1) to fake_label (=0)\n",
    "        label.fill_(fake_label)\n",
    "        \n",
    "        output = netD(fake.detach())\n",
    "        \n",
    "        # calculate discriminator's error for fake generated input\n",
    "        errD_fake = criterion(output, label)\n",
    "        print(\"Discriminator error (input = generated by fake noise) = \" + \\\n",
    "              str(errD_fake))\n",
    "        \n",
    "        \n",
    "        errD_fake.backward()\n",
    "        \n",
    "        \n",
    "        D_G_z1 = output.mean().item()\n",
    "        print(\"D_G_z1 = \" + str(D_G_z1))\n",
    "        \n",
    "        errD = errD_real + errD_fake\n",
    "        print(\"errD = errD_real + errD_fake = \" + str(errD))\n",
    "        \n",
    "        optimizerD.step()\n",
    "\n",
    "        \n",
    "        \n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, niter, i, len(dataloader),\n",
    "                 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "#         if i % 100 == 0:\n",
    "#             vutils.save_image(real_cpu,\n",
    "#                     '%s/real_samples.png' % opt.outf,\n",
    "#                     normalize=True)\n",
    "#             fake = netG(fixed_noise)\n",
    "#             vutils.save_image(fake.detach(),\n",
    "#                     '%s/fake_samples_epoch_%03d.png' % (opt.outf, epoch),\n",
    "#                     normalize=True)\n",
    "\n",
    "    # do checkpointing\n",
    "#     torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (opt.outf, epoch))\n",
    "#     torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (opt.outf, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
