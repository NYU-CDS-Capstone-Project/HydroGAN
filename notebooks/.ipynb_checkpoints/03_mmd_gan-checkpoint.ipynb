{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/OctoberChang/MMD-GAN - accompanying the paper MMD-GAN: Towards Deeper Understanding of Moment Matching Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import h5py\n",
    "import timeit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Arguments\n",
    "batch_size = 16       # BATCH_SIZE: batch size for training\n",
    "workers = 1           # WORKERS: number of threads to load data\n",
    "experiment = \"./mmd_gan\"       # EXPERIMENT: output directory of sampled images\n",
    "gpu_device = 0        # GPU_DEVICE: gpu id (default 0)\n",
    "nc = 1                # NC: number of channels in images\n",
    "nz = 100                # NZ: hidden dimension in z and codespace\n",
    "image_size = 128       # IMAGE_SIZE: image size of dataset - for our dataset more like one edge of the subcube\n",
    "\n",
    "# args.manual_seed = 1126\n",
    "manual_seed = 1126\n",
    "n_samples = 256      # on prince, number of samples to get from the training cube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataroot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-490ffcaaf0f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m           \u001b[0;31m# DATASET: type of dataset (mnist/cifar10/celeba/lsun)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataroot\u001b[0m          \u001b[0;31m# DATAROOT: path to dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mworkers\u001b[0m           \u001b[0;31m# WORKERS: number of threads to load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m       \u001b[0;31m# BATCH_SIZE: batch size for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataroot' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset           # DATASET: type of dataset (mnist/cifar10/celeba/lsun)\n",
    "dataroot          # DATAROOT: path to dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "max_iter          # MAX_ITER: max iteration for training\n",
    "lr                # LR: learning rate (default 5e-5)\n",
    "\n",
    "netG              # NETG: path to generator model\n",
    "netD              # NETD: path to discriminator model\n",
    "Diters            # DITERS: number of updates for discriminator per one generator update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_var_est = 1e-8\n",
    "\n",
    "# Consider linear time MMD with a linear kernel:\n",
    "# K(f(x), f(y)) = f(x)^Tf(y)\n",
    "# h(z_i, z_j) = k(x_i, x_j) + k(y_i, y_j) - k(x_i, y_j) - k(x_j, y_i)\n",
    "#             = [f(x_i) - f(y_i)]^T[f(x_j) - f(y_j)]\n",
    "#\n",
    "# f_of_X: batch_size * k\n",
    "# f_of_Y: batch_size * k\n",
    "def linear_mmd2(f_of_X, f_of_Y):\n",
    "    loss = 0.0\n",
    "    delta = f_of_X - f_of_Y\n",
    "    loss = torch.mean((delta[:-1] * delta[1:]).sum(1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Consider linear time MMD with a polynomial kernel:\n",
    "# K(f(x), f(y)) = (alpha*f(x)^Tf(y) + c)^d\n",
    "# f_of_X: batch_size * k\n",
    "# f_of_Y: batch_size * k\n",
    "def poly_mmd2(f_of_X, f_of_Y, d=2, alpha=1.0, c=2.0):\n",
    "    K_XX = (alpha * (f_of_X[:-1] * f_of_X[1:]).sum(1) + c)\n",
    "    K_XX_mean = torch.mean(K_XX.pow(d))\n",
    "\n",
    "    K_YY = (alpha * (f_of_Y[:-1] * f_of_Y[1:]).sum(1) + c)\n",
    "    K_YY_mean = torch.mean(K_YY.pow(d))\n",
    "\n",
    "    K_XY = (alpha * (f_of_X[:-1] * f_of_Y[1:]).sum(1) + c)\n",
    "    K_XY_mean = torch.mean(K_XY.pow(d))\n",
    "\n",
    "    K_YX = (alpha * (f_of_Y[:-1] * f_of_X[1:]).sum(1) + c)\n",
    "    K_YX_mean = torch.mean(K_YX.pow(d))\n",
    "\n",
    "    return K_XX_mean + K_YY_mean - K_XY_mean - K_YX_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _mix_rbf_kernel(X, Y, sigma_list):\n",
    "    assert(X.size(0) == Y.size(0))\n",
    "    m = X.size(0)\n",
    "\n",
    "    Z = torch.cat((X, Y), 0)\n",
    "    ZZT = torch.mm(Z, Z.t())\n",
    "    diag_ZZT = torch.diag(ZZT).unsqueeze(1)\n",
    "    Z_norm_sqr = diag_ZZT.expand_as(ZZT)\n",
    "    exponent = Z_norm_sqr - 2 * ZZT + Z_norm_sqr.t()\n",
    "\n",
    "    K = 0.0\n",
    "    for sigma in sigma_list:\n",
    "        gamma = 1.0 / (2 * sigma**2)\n",
    "        K += torch.exp(-gamma * exponent)\n",
    "\n",
    "    return K[:m, :m], K[:m, m:], K[m:, m:], len(sigma_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mix_rbf_mmd2(X, Y, sigma_list, biased=True):\n",
    "    K_XX, K_XY, K_YY, d = _mix_rbf_kernel(X, Y, sigma_list)\n",
    "    # return _mmd2(K_XX, K_XY, K_YY, const_diagonal=d, biased=biased)\n",
    "    return _mmd2(K_XX, K_XY, K_YY, const_diagonal=False, biased=biased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mix_rbf_mmd2_and_ratio(X, Y, sigma_list, biased=True):\n",
    "    K_XX, K_XY, K_YY, d = _mix_rbf_kernel(X, Y, sigma_list)\n",
    "    # return _mmd2_and_ratio(K_XX, K_XY, K_YY, const_diagonal=d, biased=biased)\n",
    "    return _mmd2_and_ratio(K_XX, K_XY, K_YY, const_diagonal=False, biased=biased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Helper functions to compute variances based on kernel matrices\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def _mmd2(K_XX, K_XY, K_YY, const_diagonal=False, biased=False):\n",
    "    m = K_XX.size(0)    # assume X, Y are same shape\n",
    "\n",
    "    # Get the various sums of kernels that we'll use\n",
    "    # Kts drop the diagonal, but we don't need to compute them explicitly\n",
    "    if const_diagonal is not False:\n",
    "        diag_X = diag_Y = const_diagonal\n",
    "        sum_diag_X = sum_diag_Y = m * const_diagonal\n",
    "    else:\n",
    "        diag_X = torch.diag(K_XX)                       # (m,)\n",
    "        diag_Y = torch.diag(K_YY)                       # (m,)\n",
    "        sum_diag_X = torch.sum(diag_X)\n",
    "        sum_diag_Y = torch.sum(diag_Y)\n",
    "\n",
    "    Kt_XX_sums = K_XX.sum(dim=1) - diag_X             # \\tilde{K}_XX * e = K_XX * e - diag_X\n",
    "    Kt_YY_sums = K_YY.sum(dim=1) - diag_Y             # \\tilde{K}_YY * e = K_YY * e - diag_Y\n",
    "    K_XY_sums_0 = K_XY.sum(dim=0)                     # K_{XY}^T * e\n",
    "\n",
    "    Kt_XX_sum = Kt_XX_sums.sum()                       # e^T * \\tilde{K}_XX * e\n",
    "    Kt_YY_sum = Kt_YY_sums.sum()                       # e^T * \\tilde{K}_YY * e\n",
    "    K_XY_sum = K_XY_sums_0.sum()                       # e^T * K_{XY} * e\n",
    "\n",
    "    if biased:\n",
    "        mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)\n",
    "            + (Kt_YY_sum + sum_diag_Y) / (m * m)\n",
    "            - 2.0 * K_XY_sum / (m * m))\n",
    "    else:\n",
    "        mmd2 = (Kt_XX_sum / (m * (m - 1))\n",
    "            + Kt_YY_sum / (m * (m - 1))\n",
    "            - 2.0 * K_XY_sum / (m * m))\n",
    "\n",
    "    return mmd2\n",
    "\n",
    "\n",
    "def _mmd2_and_ratio(K_XX, K_XY, K_YY, const_diagonal=False, biased=False):\n",
    "    mmd2, var_est = _mmd2_and_variance(K_XX, K_XY, K_YY, const_diagonal=const_diagonal, biased=biased)\n",
    "    loss = mmd2 / torch.sqrt(torch.clamp(var_est, min=min_var_est))\n",
    "    return loss, mmd2, var_est\n",
    "\n",
    "\n",
    "def _mmd2_and_variance(K_XX, K_XY, K_YY, const_diagonal=False, biased=False):\n",
    "    m = K_XX.size(0)    # assume X, Y are same shape\n",
    "\n",
    "    # Get the various sums of kernels that we'll use\n",
    "    # Kts drop the diagonal, but we don't need to compute them explicitly\n",
    "    if const_diagonal is not False:\n",
    "        diag_X = diag_Y = const_diagonal\n",
    "        sum_diag_X = sum_diag_Y = m * const_diagonal\n",
    "        sum_diag2_X = sum_diag2_Y = m * const_diagonal**2\n",
    "    else:\n",
    "        diag_X = torch.diag(K_XX)                       # (m,)\n",
    "        diag_Y = torch.diag(K_YY)                       # (m,)\n",
    "        sum_diag_X = torch.sum(diag_X)\n",
    "        sum_diag_Y = torch.sum(diag_Y)\n",
    "        sum_diag2_X = diag_X.dot(diag_X)\n",
    "        sum_diag2_Y = diag_Y.dot(diag_Y)\n",
    "\n",
    "    Kt_XX_sums = K_XX.sum(dim=1) - diag_X             # \\tilde{K}_XX * e = K_XX * e - diag_X\n",
    "    Kt_YY_sums = K_YY.sum(dim=1) - diag_Y             # \\tilde{K}_YY * e = K_YY * e - diag_Y\n",
    "    K_XY_sums_0 = K_XY.sum(dim=0)                     # K_{XY}^T * e\n",
    "    K_XY_sums_1 = K_XY.sum(dim=1)                     # K_{XY} * e\n",
    "\n",
    "    Kt_XX_sum = Kt_XX_sums.sum()                       # e^T * \\tilde{K}_XX * e\n",
    "    Kt_YY_sum = Kt_YY_sums.sum()                       # e^T * \\tilde{K}_YY * e\n",
    "    K_XY_sum = K_XY_sums_0.sum()                       # e^T * K_{XY} * e\n",
    "\n",
    "    Kt_XX_2_sum = (K_XX ** 2).sum() - sum_diag2_X      # \\| \\tilde{K}_XX \\|_F^2\n",
    "    Kt_YY_2_sum = (K_YY ** 2).sum() - sum_diag2_Y      # \\| \\tilde{K}_YY \\|_F^2\n",
    "    K_XY_2_sum  = (K_XY ** 2).sum()                    # \\| K_{XY} \\|_F^2\n",
    "\n",
    "    if biased:\n",
    "        mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)\n",
    "            + (Kt_YY_sum + sum_diag_Y) / (m * m)\n",
    "            - 2.0 * K_XY_sum / (m * m))\n",
    "    else:\n",
    "        mmd2 = (Kt_XX_sum / (m * (m - 1))\n",
    "            + Kt_YY_sum / (m * (m - 1))\n",
    "            - 2.0 * K_XY_sum / (m * m))\n",
    "\n",
    "    var_est = (\n",
    "        2.0 / (m**2 * (m - 1.0)**2) * (2 * Kt_XX_sums.dot(Kt_XX_sums) - Kt_XX_2_sum + 2 * Kt_YY_sums.dot(Kt_YY_sums) - Kt_YY_2_sum)\n",
    "        - (4.0*m - 6.0) / (m**3 * (m - 1.0)**3) * (Kt_XX_sum**2 + Kt_YY_sum**2)\n",
    "        + 4.0*(m - 2.0) / (m**3 * (m - 1.0)**2) * (K_XY_sums_1.dot(K_XY_sums_1) + K_XY_sums_0.dot(K_XY_sums_0))\n",
    "        - 4.0*(m - 3.0) / (m**3 * (m - 1.0)**2) * (K_XY_2_sum) - (8 * m - 12) / (m**5 * (m - 1)) * K_XY_sum**2\n",
    "        + 8.0 / (m**3 * (m - 1.0)) * (\n",
    "            1.0 / m * (Kt_XX_sum + Kt_YY_sum) * K_XY_sum\n",
    "            - Kt_XX_sums.dot(K_XY_sums_1)\n",
    "            - Kt_YY_sums.dot(K_XY_sums_0))\n",
    "        )\n",
    "    return mmd2, var_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_norm(m, norm_type=2):\n",
    "    total_norm = 0.0\n",
    "    for p in m.parameters():\n",
    "        param_norm = p.grad.data.norm(norm_type)\n",
    "        total_norm += param_norm ** norm_type\n",
    "    total_norm = total_norm ** (1. / norm_type)\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.1)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: batch_size * k * 1 * 1\n",
    "# output: batch_size * nc * image_size * image_size\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, isize, nc, k=100, ngf=64):\n",
    "        super(Decoder, self).__init__()\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, tisize = ngf // 2, 4\n",
    "        while tisize != isize:\n",
    "            cngf = cngf * 2\n",
    "            tisize = tisize * 2\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        main.add_module('initial_{0}-{1}_convt'.format(k, cngf), \n",
    "                        nn.ConvTranspose3d(k, cngf, 4, 1, 0, bias=False))\n",
    "        main.add_module('initial_{0}_batchnorm'.format(cngf), \n",
    "                        nn.BatchNorm3d(cngf))\n",
    "        main.add_module('initial_{0}_relu'.format(cngf), \n",
    "                        nn.ReLU(True))\n",
    "\n",
    "        csize = 4\n",
    "        while csize < isize // 2:\n",
    "            main.add_module('pyramid_{0}-{1}_convt'.format(cngf, cngf // 2),\n",
    "                            nn.ConvTranspose3d(cngf, cngf // 2, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid_{0}_batchnorm'.format(cngf // 2),\n",
    "                            nn.BatchNorm3d(cngf // 2))\n",
    "            main.add_module('pyramid_{0}_relu'.format(cngf // 2),\n",
    "                            nn.ReLU(True))\n",
    "            cngf = cngf // 2\n",
    "            csize = csize * 2\n",
    "\n",
    "        main.add_module('final_{0}-{1}_convt'.format(cngf, nc), \n",
    "                        nn.ConvTranspose3d(cngf, nc, 4, 2, 1, bias=False))\n",
    "        main.add_module('final_{0}_tanh'.format(nc),\n",
    "                        nn.Tanh())\n",
    "\n",
    "        self.main = main\n",
    "        \n",
    "        # to print out the resulting structure\n",
    "#         print(main)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NetG is a decoder\n",
    "# input: batch_size * nz * 1 * 1\n",
    "# output: batch_size * nc * image_size * image_size\n",
    "class NetG(nn.Module):\n",
    "    def __init__(self, decoder):\n",
    "        super(NetG, self).__init__()\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.decoder(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# input: batch_size * nc * 64 * 64\n",
    "# output: batch_size * k * 1 * 1\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, isize, nc, k=100, ndf=64):\n",
    "        \"\"\"\n",
    "        isize = image_size\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        # input is nc x isize x isize\n",
    "        main = nn.Sequential()\n",
    "        main.add_module('initial_conv_{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv3d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial_relu_{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid_{0}-{1}_conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv3d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid_{0}_batchnorm'.format(out_feat),\n",
    "                            nn.BatchNorm3d(out_feat))\n",
    "            main.add_module('pyramid_{0}_relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        main.add_module('final_{0}-{1}_conv'.format(cndf, 1),\n",
    "                        nn.Conv3d(cndf, k, 4, 1, 0, bias=False))\n",
    "\n",
    "        self.main = main\n",
    "        \n",
    "        # to print out the resulting structure\n",
    "#         print(main)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NetD is an encoder + decoder\n",
    "# input: batch_size * nc * image_size * image_size\n",
    "# f_enc_X: batch_size * k * 1 * 1\n",
    "# f_dec_X: batch_size * nc * image_size * image_size\n",
    "class NetD(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(NetD, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input):\n",
    "        f_enc_X = self.encoder(input)\n",
    "        f_dec_X = self.decoder(f_enc_X)\n",
    "\n",
    "        f_enc_X = f_enc_X.view(input.size(0), -1)\n",
    "        f_dec_X = f_dec_X.view(input.size(0), -1)\n",
    "        return f_enc_X, f_dec_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ONE_SIDED(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ONE_SIDED, self).__init__()\n",
    "\n",
    "        main = nn.ReLU()\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(-input)\n",
    "        output = -output.mean()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if args.experiment is None:\n",
    "#     args.experiment = 'samples'\n",
    "# os.system('mkdir {0}'.format(args.experiment))\n",
    "\n",
    "if experiment is None:\n",
    "    experiment = 'samples'\n",
    "os.system('mkdir {0}'.format(experiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "GPU device not available!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-aed3c59b2fb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Using GPU device\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GPU device not available!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: GPU device not available!"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "#     args.cuda = True\n",
    "    cuda = True\n",
    "#     torch.cuda.set_device(args.gpu_device)\n",
    "    torch.cuda.set_device(gpu_device)\n",
    "    print(\"Using GPU device\", torch.cuda.current_device())\n",
    "else:\n",
    "    raise EnvironmentError(\"GPU device not available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.random.seed(seed=args.manual_seed)\n",
    "# random.seed(args.manual_seed)\n",
    "# torch.manual_seed(args.manual_seed)\n",
    "# torch.cuda.manual_seed(args.manual_seed)\n",
    "# cudnn.benchmark = True\n",
    "\n",
    "np.random.seed(seed=manual_seed)\n",
    "random.seed(manual_seed)\n",
    "torch.manual_seed(manual_seed)\n",
    "torch.cuda.manual_seed(manual_seed)\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_test(s_test, s_train):\n",
    "    #2048/16=128\n",
    "    m=8\n",
    "    x=random.randint(0,m)*s_train\n",
    "    y=random.randint(0,m)*s_train\n",
    "    z=random.randint(0,m)*s_train\n",
    "    #print(x,y,z)\n",
    "    return {'x':[x,x+s_test], 'y':[y,y+s_test], 'z':[z,z+s_test]}\n",
    "\n",
    "def check_coords(test_coords, train_coords):\n",
    "    valid=True\n",
    "    for i in ['x','y','z']:\n",
    "        r=(max(test_coords[i][0], \n",
    "               train_coords[i][0]), \n",
    "           min(test_coords[i][1],\n",
    "               train_coords[i][1]))\n",
    "        if r[0]<=r[1]:\n",
    "            valid=False\n",
    "    return valid\n",
    "\n",
    "def get_samples(s_sample, nsamples, redshift, test_coords):\n",
    "    #n is size of minibatch, get valid samples (not intersecting with test_coords)\n",
    "    sample_list=[]\n",
    "    m=2048-128\n",
    "    for n in range(nsamples):\n",
    "        #print(\"Sample No = \" + str(n + 1) + \" / \" + str(nsamples))\n",
    "        sample_valid=False\n",
    "        while sample_valid==False:\n",
    "            x = random.randint(0,m)\n",
    "            y = random.randint(0,m)\n",
    "            z = random.randint(0,m)\n",
    "            sample_coords = {'x':[x,x+s_sample], \n",
    "                             'y':[y,y+s_sample], \n",
    "                             'z':[z,z+s_sample]}\n",
    "            \n",
    "            sample_valid = check_coords(test_coords, \n",
    "                                        sample_coords)\n",
    "        \n",
    "        sample_list.append(sample_coords)\n",
    "    \n",
    "    print(\"Sampling finished.\")\n",
    "        \n",
    "    #Load cube and get samples and convert them to np.arrays\n",
    "    sample_array=[]\n",
    "    datapath=''\n",
    "    f = h5py.File(datapath+'fields_z='+redshift+'.hdf5', 'r')\n",
    "    f=f['delta_HI']\n",
    "    \n",
    "    # getting the max of the whole cube\n",
    "    #print(f.shape)\n",
    "    max_list = []\n",
    "    for i in range(f.shape[0]):\n",
    "        #print(np.max(f[i:i+1,:,:]))\n",
    "        max_list.append(np.max(f[i:i+1,:,:]))\n",
    "    max_cube = max(max_list)\n",
    "    #f.close()\n",
    "    \n",
    "    print(\"Getting max value finished.\")\n",
    "    \n",
    "    counter = 0\n",
    "    for c in sample_list:\n",
    "        print(\"Counter = \" + str(counter + 1) + \" / \" + str(len(sample_list)))\n",
    "        a = f[c['x'][0]:c['x'][1],\n",
    "              c['y'][0]:c['y'][1],\n",
    "              c['z'][0]:c['z'][1]]\n",
    "        \n",
    "        # a = np.array(a)\n",
    "        a = np.array(a) / max_cube\n",
    "        sample_array.append(a)\n",
    "    \n",
    "        counter = counter + 1\n",
    "        \n",
    "    f=0\n",
    "    return sample_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HydrogenDataset2(Dataset):\n",
    "    \"\"\"Hydrogen Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, h5_file, root_dir, s_test, s_train,\n",
    "                 s_sample, nsamples, redshift):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h5_file (string): name of the h5 file with 32 sampled cubes.\n",
    "            root_dir (string): Directory with the .h5 file.\n",
    "        \"\"\"\n",
    "        file_size = os.path.getsize(root_dir + h5_file) / 1e6 # in MBs\n",
    "        print(\"The whole file size is \" + str(int(file_size)) + \" MBs\")\n",
    "        \n",
    "        # self.subcubes = h5py.File('../data/sample_32.h5', 'r')\n",
    "        self.subcubes = h5py.File(root_dir + h5_file, 'r')[\"delta_HI\"]\n",
    "        self.h5_file = h5_file\n",
    "        self.root_dir = root_dir\n",
    "        self.s_test = s_test\n",
    "        self.s_train = s_train\n",
    "        self.t_coords = define_test(self.s_test,\n",
    "                                    self.s_train)\n",
    "        self.s_sample = s_sample\n",
    "        self.nsamples = nsamples\n",
    "        self.redshift = redshift\n",
    "        \n",
    "        self.samples = get_samples(s_sample = self.s_sample,\n",
    "                             nsamples = self.nsamples,\n",
    "                             redshift = self.redshift,\n",
    "                             test_coords = self.t_coords)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Function called when len(self) is executed\n",
    "        \n",
    "        #print(len(self.subcubes))\n",
    "#         return len(self.nsamples)\n",
    "        return self.nsamples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        This can be implemented in such a way that the whole h5 file read \n",
    "        using h5py.File() and get_sample() function is called to return\n",
    "        a random subcube. This won't increase memory usage because the\n",
    "        subcubes will be read in the same way and only the batch will\n",
    "        be read into memory.\n",
    "        \n",
    "        Here we have implemented it so that it can be used with data\n",
    "        generated by get_sample() function.\n",
    "        \n",
    "        The output of this function is one subcube with the dimensions\n",
    "        specified by get_sample() implementation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # default version -> error in training because of dimensions\n",
    "#         sample = self.subcubes[idx]\n",
    "        \n",
    "        # reshaped version to add another dimension\n",
    "#         sample = self.subcubes[idx].reshape((1,128,128,128))\n",
    "\n",
    "        # On prince using get_samples()\n",
    "#         print(\"nsamples = \" + str(self.nsamples))\n",
    "#         sample = get_samples(s_sample = self.s_sample,\n",
    "#                              nsamples = self.nsamples,\n",
    "#                              redshift = self.redshift,\n",
    "#                              test_coords = self.t_coords)\n",
    "    \n",
    "        sample = self.samples[idx].reshape((1,128,128,128))\n",
    "        \n",
    "        # added division by 1e6 for exploding variance\n",
    "        # and resulting in inf during reparametrization trick part\n",
    "        sample = sample/1e6\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HydrogenDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, h5_file, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h5_file (string): name of the h5 file with 32 sampled cubes.\n",
    "            root_dir (string): Directory with the .h5 file.\n",
    "        \"\"\"\n",
    "        file_size = os.path.getsize(root_dir + h5_file) / 1e6 # in MBs\n",
    "        print(\"The file size is \" + str(int(file_size)) + \" MBs\")\n",
    "        \n",
    "        # self.subcubes = h5py.File('../data/sample_32.h5', 'r')\n",
    "        self.subcubes = h5py.File(root_dir + h5_file, 'r')['sample32']\n",
    "        self.h5_file = h5_file\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        # Function called when len(self) is executed\n",
    "        \n",
    "        #print(len(self.subcubes))\n",
    "        return len(self.subcubes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        This can be implemented in such a way that the whole h5 file read \n",
    "        using h5py.File() and get_sample() function is called to return\n",
    "        a random subcube. This won't increase memory usage because the\n",
    "        subcubes will be read in the same way and only the batch will\n",
    "        be read into memory.\n",
    "        \n",
    "        Here we have implemented it so that it can be used with data\n",
    "        generated by get_sample() function.\n",
    "        \n",
    "        The output of this function is one subcube with the dimensions\n",
    "        specified by get_sample() implementation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # default version -> error in training because of dimensions\n",
    "        #sample = self.subcubes[idx]\n",
    "        \n",
    "        # reshaped version to add another dimension\n",
    "        sample = self.subcubes[idx].reshape((1,128,128,128))\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fields_z=1.0.hdf5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-22924d03f402>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                     \u001b[0ms_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                     \u001b[0mnsamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                     redshift = \"1.0\")\n\u001b[0m",
      "\u001b[0;32m<ipython-input-123-2bdb40001a8a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, h5_file, root_dir, s_test, s_train, s_sample, nsamples, redshift)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mroot_dir\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDirectory\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mh5\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \"\"\"\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mfile_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh5_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1e6\u001b[0m \u001b[0;31m# in MBs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The whole file size is \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" MBs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/genericpath.py\u001b[0m in \u001b[0;36mgetsize\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;34m\"\"\"Return the size of a file, reported by os.stat().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fields_z=1.0.hdf5'"
     ]
    }
   ],
   "source": [
    "# on prince\n",
    "sampled_subcubes = HydrogenDataset2(h5_file=\"fields_z=1.0.hdf5\",\n",
    "                                    root_dir = \"\",\n",
    "                                    s_test = 1024, \n",
    "                                    s_train = 128,\n",
    "                                    s_sample = 128, \n",
    "                                    nsamples = n_samples, \n",
    "                                    redshift = \"1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file size is 268 MBs\n"
     ]
    }
   ],
   "source": [
    "# on local\n",
    "sampled_subcubes = HydrogenDataset(h5_file=\"sample_32.h5\",\n",
    "                                    root_dir = \"../data/\")\n",
    "dataset = sampled_subcubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "# trn_dataset = util.get_data(args, train_flag=True)\n",
    "# trn_loader = torch.utils.data.DataLoader(trn_dataset,\n",
    "#                                          batch_size=args.batch_size,\n",
    "#                                          shuffle=True,\n",
    "#                                          num_workers=int(args.workers))\n",
    "\n",
    "trn_loader = torch.utils.data.DataLoader(dataset, \n",
    "                                         batch_size = batch_size,\n",
    "                                         shuffle=True, \n",
    "                                         num_workers=int(workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # construct encoder/decoder modules\n",
    "# hidden_dim = args.nz\n",
    "# G_decoder = base_module.Decoder(args.image_size, \n",
    "#                                 args.nc, \n",
    "#                                 k=args.nz, \n",
    "#                                 ngf=64)\n",
    "# D_encoder = base_module.Encoder(args.image_size, \n",
    "#                                 args.nc, \n",
    "#                                 k=hidden_dim, \n",
    "#                                 ndf=64)\n",
    "# D_decoder = base_module.Decoder(args.image_size, \n",
    "#                                 args.nc, \n",
    "#                                 k=hidden_dim, \n",
    "#                                 ngf=64)\n",
    "\n",
    "# construct encoder/decoder modules\n",
    "hidden_dim = nz\n",
    "G_decoder = Decoder(image_size, \n",
    "                    nc, \n",
    "                    k=nz, \n",
    "                    ngf=64)\n",
    "D_encoder = Encoder(image_size, \n",
    "                    nc, \n",
    "                    k=hidden_dim, \n",
    "                    ndf=64)\n",
    "D_decoder = Decoder(image_size, \n",
    "                    nc, \n",
    "                    k=hidden_dim, \n",
    "                    ngf=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "netG: NetG(\n",
      "  (decoder): Decoder(\n",
      "    (main): Sequential(\n",
      "      (initial_100-1024_convt): ConvTranspose3d(100, 1024, kernel_size=(4, 4, 4), stride=(1, 1, 1), bias=False)\n",
      "      (initial_1024_batchnorm): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (initial_1024_relu): ReLU(inplace)\n",
      "      (pyramid_1024-512_convt): ConvTranspose3d(1024, 512, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (pyramid_512_batchnorm): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (pyramid_512_relu): ReLU(inplace)\n",
      "      (pyramid_512-256_convt): ConvTranspose3d(512, 256, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (pyramid_256_batchnorm): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (pyramid_256_relu): ReLU(inplace)\n",
      "      (pyramid_256-128_convt): ConvTranspose3d(256, 128, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (pyramid_128_batchnorm): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (pyramid_128_relu): ReLU(inplace)\n",
      "      (pyramid_128-64_convt): ConvTranspose3d(128, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (pyramid_64_batchnorm): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (pyramid_64_relu): ReLU(inplace)\n",
      "      (final_64-1_convt): ConvTranspose3d(64, 1, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (final_1_tanh): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "netD: NetD(\n",
      "  (encoder): Encoder(\n",
      "    (main): Sequential(\n",
      "      (initial_conv_1-64): Conv3d(1, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (initial_relu_64): LeakyReLU(negative_slope=0.2, inplace)\n",
      "      (pyramid_64-128_conv): Conv3d(64, 128, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (pyramid_128_batchnorm): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (pyramid_128_relu): LeakyReLU(negative_slope=0.2, inplace)\n",
      "      (pyramid_128-256_conv): Conv3d(128, 256, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (pyramid_256_batchnorm): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (pyramid_256_relu): LeakyReLU(negative_slope=0.2, inplace)\n",
      "      (pyramid_256-512_conv): Conv3d(256, 512, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (pyramid_512_batchnorm): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (pyramid_512_relu): LeakyReLU(negative_slope=0.2, inplace)\n",
      "      (pyramid_512-1024_conv): Conv3d(512, 1024, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (pyramid_1024_batchnorm): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (pyramid_1024_relu): LeakyReLU(negative_slope=0.2, inplace)\n",
      "      (final_1024-1_conv): Conv3d(1024, 100, kernel_size=(4, 4, 4), stride=(1, 1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (main): Sequential(\n",
      "      (initial_100-1024_convt): ConvTranspose3d(100, 1024, kernel_size=(4, 4, 4), stride=(1, 1, 1), bias=False)\n",
      "      (initial_1024_batchnorm): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (initial_1024_relu): ReLU(inplace)\n",
      "      (pyramid_1024-512_convt): ConvTranspose3d(1024, 512, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (pyramid_512_batchnorm): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (pyramid_512_relu): ReLU(inplace)\n",
      "      (pyramid_512-256_convt): ConvTranspose3d(512, 256, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (pyramid_256_batchnorm): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (pyramid_256_relu): ReLU(inplace)\n",
      "      (pyramid_256-128_convt): ConvTranspose3d(256, 128, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (pyramid_128_batchnorm): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (pyramid_128_relu): ReLU(inplace)\n",
      "      (pyramid_128-64_convt): ConvTranspose3d(128, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (pyramid_64_batchnorm): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (pyramid_64_relu): ReLU(inplace)\n",
      "      (final_64-1_convt): ConvTranspose3d(64, 1, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (final_1_tanh): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "oneSide: ONE_SIDED(\n",
      "  (main): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netG = NetG(G_decoder)\n",
    "netD = NetD(D_encoder, D_decoder)\n",
    "one_sided = ONE_SIDED()\n",
    "print(\"netG:\", netG)\n",
    "print(\"netD:\", netD)\n",
    "print(\"oneSide:\", one_sided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ONE_SIDED(\n",
       "  (main): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "one_sided.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sigma for MMD\n",
    "base = 1.0\n",
    "sigma_list = [1, 2, 4, 8, 16]\n",
    "sigma_list = [sigma / base for sigma in sigma_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put variable into cuda device\n",
    "fixed_noise = torch.cuda.FloatTensor(64, args.nz, 1, 1).normal_(0, 1)\n",
    "one = torch.cuda.FloatTensor([1])\n",
    "mone = one * -1\n",
    "if args.cuda:\n",
    "    netG.cuda()\n",
    "    netD.cuda()\n",
    "    one_sided.cuda()\n",
    "fixed_noise = Variable(fixed_noise, \n",
    "                       requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup optimizer\n",
    "optimizerG = torch.optim.RMSprop(netG.parameters(), \n",
    "                                 lr=args.lr)\n",
    "optimizerD = torch.optim.RMSprop(netD.parameters(), \n",
    "                                 lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambda_MMD = 1.0\n",
    "lambda_AE_X = 8.0\n",
    "lambda_AE_Y = 8.0\n",
    "lambda_rg = 16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time = timeit.default_timer()\n",
    "gen_iterations = 0\n",
    "for t in range(args.max_iter):\n",
    "    data_iter = iter(trn_loader)\n",
    "    i = 0\n",
    "    while (i < len(trn_loader)):\n",
    "        # ---------------------------\n",
    "        #        Optimize over NetD\n",
    "        # ---------------------------\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        if gen_iterations < 25 or gen_iterations % 500 == 0:\n",
    "            Diters = 100\n",
    "            Giters = 1\n",
    "        else:\n",
    "            Diters = 5\n",
    "            Giters = 1\n",
    "\n",
    "        for j in range(Diters):\n",
    "            if i == len(trn_loader):\n",
    "                break\n",
    "\n",
    "            # clamp parameters of NetD encoder to a cube\n",
    "            # do not clamp paramters of NetD decoder!!!\n",
    "            for p in netD.encoder.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "            data = data_iter.next()\n",
    "            i += 1\n",
    "            netD.zero_grad()\n",
    "\n",
    "            x_cpu, _ = data\n",
    "            x = Variable(x_cpu.cuda())\n",
    "            batch_size = x.size(0)\n",
    "\n",
    "            f_enc_X_D, f_dec_X_D = netD(x)\n",
    "\n",
    "            noise = torch.cuda.FloatTensor(batch_size, args.nz, 1, 1).normal_(0, 1)\n",
    "            noise = Variable(noise, volatile=True)  # total freeze netG\n",
    "            y = Variable(netG(noise).data)\n",
    "\n",
    "            f_enc_Y_D, f_dec_Y_D = netD(y)\n",
    "\n",
    "            # compute biased MMD2 and use ReLU to prevent negative value\n",
    "            mmd2_D = mix_rbf_mmd2(f_enc_X_D, f_enc_Y_D, sigma_list)\n",
    "            mmd2_D = F.relu(mmd2_D)\n",
    "\n",
    "            # compute rank hinge loss\n",
    "            #print('f_enc_X_D:', f_enc_X_D.size())\n",
    "            #print('f_enc_Y_D:', f_enc_Y_D.size())\n",
    "            one_side_errD = one_sided(f_enc_X_D.mean(0) - f_enc_Y_D.mean(0))\n",
    "\n",
    "            # compute L2-loss of AE\n",
    "            L2_AE_X_D = util.match(x.view(batch_size, -1), f_dec_X_D, 'L2')\n",
    "            L2_AE_Y_D = util.match(y.view(batch_size, -1), f_dec_Y_D, 'L2')\n",
    "\n",
    "            errD = torch.sqrt(mmd2_D) + lambda_rg * one_side_errD - lambda_AE_X * L2_AE_X_D - lambda_AE_Y * L2_AE_Y_D\n",
    "            errD.backward(mone)\n",
    "            optimizerD.step()\n",
    "\n",
    "        # ---------------------------\n",
    "        #        Optimize over NetG\n",
    "        # ---------------------------\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        for j in range(Giters):\n",
    "            if i == len(trn_loader):\n",
    "                break\n",
    "\n",
    "            data = data_iter.next()\n",
    "            i += 1\n",
    "            netG.zero_grad()\n",
    "\n",
    "            x_cpu, _ = data\n",
    "            x = Variable(x_cpu.cuda())\n",
    "            batch_size = x.size(0)\n",
    "\n",
    "            f_enc_X, f_dec_X = netD(x)\n",
    "\n",
    "            noise = torch.cuda.FloatTensor(batch_size, args.nz, 1, 1).normal_(0, 1)\n",
    "            noise = Variable(noise)\n",
    "            y = netG(noise)\n",
    "\n",
    "            f_enc_Y, f_dec_Y = netD(y)\n",
    "\n",
    "            # compute biased MMD2 and use ReLU to prevent negative value\n",
    "            mmd2_G = mix_rbf_mmd2(f_enc_X, f_enc_Y, sigma_list)\n",
    "            mmd2_G = F.relu(mmd2_G)\n",
    "\n",
    "            # compute rank hinge loss\n",
    "            one_side_errG = one_sided(f_enc_X.mean(0) - f_enc_Y.mean(0))\n",
    "\n",
    "            errG = torch.sqrt(mmd2_G) + lambda_rg * one_side_errG\n",
    "            errG.backward(one)\n",
    "            optimizerG.step()\n",
    "\n",
    "            gen_iterations += 1\n",
    "\n",
    "        run_time = (timeit.default_timer() - time) / 60.0\n",
    "        print('[%3d/%3d][%3d/%3d] [%5d] (%.2f m) MMD2_D %.6f hinge %.6f L2_AE_X %.6f L2_AE_Y %.6f loss_D %.6f Loss_G %.6f f_X %.6f f_Y %.6f |gD| %.4f |gG| %.4f'\n",
    "              % (t, args.max_iter, i, len(trn_loader), gen_iterations, run_time,\n",
    "                 mmd2_D.data[0], one_side_errD.data[0],\n",
    "                 L2_AE_X_D.data[0], L2_AE_Y_D.data[0],\n",
    "                 errD.data[0], errG.data[0],\n",
    "                 f_enc_X_D.mean().data[0], f_enc_Y_D.mean().data[0],\n",
    "                 base_module.grad_norm(netD), base_module.grad_norm(netG)))\n",
    "\n",
    "        if gen_iterations % 500 == 0:\n",
    "            y_fixed = netG(fixed_noise)\n",
    "            y_fixed.data = y_fixed.data.mul(0.5).add(0.5)\n",
    "            f_dec_X_D = f_dec_X_D.view(f_dec_X_D.size(0), args.nc, args.image_size, args.image_size)\n",
    "            f_dec_X_D.data = f_dec_X_D.data.mul(0.5).add(0.5)\n",
    "            vutils.save_image(y_fixed.data, '{0}/fake_samples_{1}.png'.format(args.experiment, gen_iterations))\n",
    "            vutils.save_image(f_dec_X_D.data, '{0}/decode_samples_{1}.png'.format(args.experiment, gen_iterations))\n",
    "\n",
    "    if t % 50 == 0:\n",
    "        torch.save(netG.state_dict(), '{0}/netG_iter_{1}.pth'.format(args.experiment, t))\n",
    "        torch.save(netD.state_dict(), '{0}/netD_iter_{1}.pth'.format(args.experiment, t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
