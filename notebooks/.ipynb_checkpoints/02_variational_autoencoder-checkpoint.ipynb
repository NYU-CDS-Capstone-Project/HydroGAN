{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code taken from: https://github.com/pytorch/examples/tree/master/vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modify to accept hard coded arguments\n",
    "batch_size = 4\n",
    "epochs = 2\n",
    "no_cuda = False\n",
    "log_interval = 10\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "# kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HydrogenDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, h5_file, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h5_file (string): name of the h5 file with 32 sampled cubes.\n",
    "            root_dir (string): Directory with the .h5 file.\n",
    "        \"\"\"\n",
    "        file_size = os.path.getsize(root_dir + h5_file) / 1e6 # in MBs\n",
    "        print(\"The file size is \" + str(int(file_size)) + \" MBs\")\n",
    "        \n",
    "        # self.subcubes = h5py.File('../data/sample_32.h5', 'r')\n",
    "        self.subcubes = h5py.File(root_dir + h5_file, 'r')['sample32']\n",
    "        self.h5_file = h5_file\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        # Function called when len(self) is executed\n",
    "        \n",
    "        #print(len(self.subcubes))\n",
    "        return len(self.subcubes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        This can be implemented in such a way that the whole h5 file read \n",
    "        using h5py.File() and get_sample() function is called to return\n",
    "        a random subcube. This won't increase memory usage because the\n",
    "        subcubes will be read in the same way and only the batch will\n",
    "        be read into memory.\n",
    "        \n",
    "        Here we have implemented it so that it can be used with data\n",
    "        generated by get_sample() function.\n",
    "        \n",
    "        The output of this function is one subcube with the dimensions\n",
    "        specified by get_sample() implementation.\n",
    "        \"\"\"\n",
    "        \n",
    "        sample = self.subcubes[idx]\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file size is 268 MBs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.HydrogenDataset at 0x11c64a588>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_subcubes = HydrogenDataset(h5_file=\"sample_32.h5\",\n",
    "                                    root_dir = \"../data/\")\n",
    "sampled_subcubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "train_loader = DataLoader(\n",
    "        dataset=sampled_subcubes,\n",
    "        #batch_size=args.batch_size, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, \n",
    "        **kwargs)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "        dataset=sampled_subcubes,\n",
    "        #batch_size=args.batch_size, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, \n",
    "        **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The Encoding Layers\n",
    "        nn.Conv3d \n",
    "        nn.MaxPool3d \n",
    "        \n",
    "        out_channels is the number of different filters we convolute \n",
    "        over the whole sampled subcube.\n",
    "        \n",
    "        So the first convolutional layer's in_channel should be 0 (?)\n",
    "        \n",
    "        In addition, the next layer's in_channel should be equal to\n",
    "        the previous layer's out_channels (all examples show that\n",
    "        this is the case)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.encode_group1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=1, \n",
    "                      out_channels=8, \n",
    "                      kernel_size=(4,4,4),\n",
    "                      stride = (2,2,2),\n",
    "                      padding=(1,1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), \n",
    "                         stride=(2, 2, 2)))\n",
    "        #init.xavier_normal(self.group1.state_dict()['weight'])\n",
    "        \n",
    "        self.encode_group2 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 8, \n",
    "                      out_channels = 16, \n",
    "                      kernel_size=(4,4,4),\n",
    "                      stride = (2,2,2),\n",
    "                      padding=(1,1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), \n",
    "                         stride=(2, 2, 2)))\n",
    "        \n",
    "        self.encode_group3 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 16, \n",
    "                      out_channels = 32, \n",
    "                      kernel_size=(4,4,4),\n",
    "                      stride = (2,2,2),\n",
    "                      padding=(1,1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), \n",
    "                         stride=(2, 2, 2)))\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Fully Connected Layers after 3D Convolutional Layers\n",
    "        First FC layer's input should be equal to \n",
    "        last convolutional layer's output\n",
    "        8192 = 8^3 * 16 \n",
    "            8^3 = (output of 2nd convolutional layer)\n",
    "            16 = number of out_channels\n",
    "        \"\"\"\n",
    "        \n",
    "        self.encode_fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=512, \n",
    "                      out_features=2048), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        #init.xavier_normal(self.fc1.state_dict()['weight'])\n",
    "        \n",
    "        self.encode_fc2 = nn.Sequential(\n",
    "            nn.Linear(in_features = 2048,\n",
    "                      out_features = 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        #init.xavier_normal(self.fc2.state_dict()['weight'])\n",
    "        \n",
    "        \"\"\"\n",
    "        The last fully connected layer's output is the dimensions\n",
    "        of the embeddings?\n",
    "        \"\"\"\n",
    "        self.encode_fc3 = nn.Sequential(\n",
    "            nn.Linear(in_features=2048,\n",
    "                      out_features=32))\n",
    "        \n",
    "        \n",
    "        # Grouping convolutional and fully-connected layers in Encoding\n",
    "        self._encode_conv = nn.Sequential(\n",
    "            self.encode_group1,\n",
    "            self.encode_group2\n",
    "        )\n",
    "\n",
    "        self._encode_fc = nn.Sequential(\n",
    "            self.encode_fc1,\n",
    "            self.encode_fc2\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        The Decoding Layers\n",
    "        nn.Conv3d -> nn.ConvTranspose3d\n",
    "        nn.MaxPool3d -> nn.MaxUnpool3d\n",
    "        \"\"\"\n",
    "        \n",
    "        self.decode_fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=32,\n",
    "                      out_features=2048))\n",
    "        \n",
    "        self.decode_fc2 = nn.Sequential(\n",
    "            nn.Linear(in_features=2048, \n",
    "                      out_features=2048), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        #init.xavier_normal(self.fc1.state_dict()['weight'])\n",
    "        \n",
    "        self.decode_fc3 = nn.Sequential(\n",
    "            nn.Linear(in_features = 2048,\n",
    "                      out_features = 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        #init.xavier_normal(self.fc2.state_dict()['weight'])\n",
    "        \n",
    "        \n",
    "        self.decode_group1 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(in_channels=32, \n",
    "                      out_channels=16, \n",
    "                      kernel_size=(4,4,4),\n",
    "                      stride = (2,2,2),\n",
    "                      padding=(1,1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxUnpool3d(kernel_size=(2, 2, 2), \n",
    "                         stride=(2, 2, 2)))\n",
    "        #init.xavier_normal(self.group1.state_dict()['weight'])\n",
    "        \n",
    "        self.decode_group2 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(in_channels = 16, \n",
    "                      out_channels = 8, \n",
    "                      kernel_size=(4,4,4),\n",
    "                      stride = (2,2,2),\n",
    "                      padding=(1,1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxUnpool3d(kernel_size=(2, 2, 2), \n",
    "                         stride=(2, 2, 2)))\n",
    "        \n",
    "        self.decode_group3 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(in_channels = 8, \n",
    "                      out_channels = 1, \n",
    "                      kernel_size=(4,4,4),\n",
    "                      stride = (2,2,2),\n",
    "                      padding=(1,1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxUnpool3d(kernel_size=(2, 2, 2), \n",
    "                         stride=(2, 2, 2)))\n",
    "\n",
    "        \n",
    "        # Grouping convolutional and fully-connected layers in Decoding\n",
    "        self._decode_fc = nn.Sequential(\n",
    "            self.decode_fc1,\n",
    "            self.decode_fc2,\n",
    "            self.decode_fc3\n",
    "        )\n",
    "        \n",
    "        self._decode_conv = nn.Sequential(\n",
    "            self.decode_group1,\n",
    "            self.decode_group2,\n",
    "            self.decode_group3\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # Encoding part of VAE\n",
    "    def encode(self, x):\n",
    "#         h1 = F.relu(self.fc1(x))\n",
    "#         return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "        out = self._encode_conv(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self._encode_fc(out)\n",
    "        return self.encode_fc3(out)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Reparametrization Trick\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    \n",
    "    # Decoding part of VAE\n",
    "    def decode(self, z):\n",
    "#         h3 = F.relu(self.fc3(z))\n",
    "#         return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "        out = self._decode_fc(z)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self._decode_conv(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, \n",
    "                                 x.view(-1, 784), \n",
    "                                 reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        # if batch_idx % args.log_interval == 0:\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      #recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
    "                                        recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-5c689a6e5971>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#for epoch in range(1, args.epochs + 1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-eac8a9b2c741>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #for epoch in range(1, args.epochs + 1):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(64, 20).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28),\n",
    "                       'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
