{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code taken from: https://github.com/pytorch/examples/tree/master/vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modify to accept hard coded arguments\n",
    "batch_size = 1\n",
    "epochs = 20\n",
    "no_cuda = False\n",
    "log_interval = 1\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "# kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HydrogenDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, h5_file, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h5_file (string): name of the h5 file with 32 sampled cubes.\n",
    "            root_dir (string): Directory with the .h5 file.\n",
    "        \"\"\"\n",
    "        file_size = os.path.getsize(root_dir + h5_file) / 1e6 # in MBs\n",
    "        print(\"The file size is \" + str(int(file_size)) + \" MBs\")\n",
    "        \n",
    "        # self.subcubes = h5py.File('../data/sample_32.h5', 'r')\n",
    "        self.subcubes = h5py.File(root_dir + h5_file, 'r')['sample32']\n",
    "        self.h5_file = h5_file\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        # Function called when len(self) is executed\n",
    "        \n",
    "        #print(len(self.subcubes))\n",
    "        return len(self.subcubes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        This can be implemented in such a way that the whole h5 file read \n",
    "        using h5py.File() and get_sample() function is called to return\n",
    "        a random subcube. This won't increase memory usage because the\n",
    "        subcubes will be read in the same way and only the batch will\n",
    "        be read into memory.\n",
    "        \n",
    "        Here we have implemented it so that it can be used with data\n",
    "        generated by get_sample() function.\n",
    "        \n",
    "        The output of this function is one subcube with the dimensions\n",
    "        specified by get_sample() implementation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # default version -> error in training because of dimensions\n",
    "        #sample = self.subcubes[idx]\n",
    "        \n",
    "        # reshaped version to add another dimension\n",
    "        sample = self.subcubes[idx].reshape((1,128,128,128))\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file size is 268 MBs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.HydrogenDataset at 0x11b8b16a0>"
      ]
     },
     "execution_count": 1281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_subcubes = HydrogenDataset(h5_file=\"sample_32.h5\",\n",
    "                                    root_dir = \"../data/\")\n",
    "sampled_subcubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "train_loader = DataLoader(\n",
    "        dataset=sampled_subcubes,\n",
    "        #batch_size=args.batch_size, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, \n",
    "        **kwargs)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "        dataset=sampled_subcubes,\n",
    "        #batch_size=args.batch_size, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, \n",
    "        **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1502,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The Encoding Layers\n",
    "        nn.Conv3d \n",
    "        nn.MaxPool3d \n",
    "        \n",
    "        out_channels is the number of different filters we convolute \n",
    "        over the whole sampled subcube.\n",
    "        \n",
    "        So the first convolutional layer's in_channel should be 0 (?)\n",
    "        \n",
    "        In addition, the next layer's in_channel should be equal to\n",
    "        the previous layer's out_channels (all examples show that\n",
    "        this is the case)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convolutional Layer 1\n",
    "        self.encode_conv1 = nn.Conv3d(in_channels=1, \n",
    "                                      out_channels=8, \n",
    "                                      kernel_size=(4,4,4), # == 4\n",
    "                                      stride = (2,2,2), # == 2\n",
    "                                      padding=(1,1,1)) # == 1\n",
    "        nn.init.xavier_uniform_(self.encode_conv1.weight) #Xaviers Initialisation\n",
    "        \n",
    "        self.encode_relu1 = nn.ReLU()\n",
    "        self.encode_maxpool1 = nn.MaxPool3d(kernel_size=(2, 2, 2), \n",
    "                                             stride=(2, 2, 2),\n",
    "                                            return_indices = True)\n",
    "        \n",
    "        # Convolutional Layer 2\n",
    "        self.encode_conv2 = nn.Conv3d(in_channels=8, \n",
    "                                      out_channels=16, \n",
    "                                      kernel_size=(4,4,4), # == 4 \n",
    "                                      stride = (2,2,2),\n",
    "                                      padding=(1,1,1))\n",
    "        nn.init.xavier_uniform_(self.encode_conv2.weight) #Xaviers Initialisation\n",
    "        \n",
    "        self.encode_relu2 = nn.ReLU()\n",
    "        self.encode_maxpool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), \n",
    "                                             stride=(2, 2, 2),\n",
    "                                            return_indices = True)\n",
    "\n",
    "        # Convolutional Layer 3\n",
    "        self.encode_conv3 = nn.Conv3d(in_channels=16, \n",
    "                                      out_channels=32, \n",
    "                                      kernel_size=(4,4,4), # == 4 \n",
    "                                      stride = (2,2,2),\n",
    "                                      padding=(1,1,1))\n",
    "        nn.init.xavier_uniform_(self.encode_conv3.weight) #Xaviers Initialisation\n",
    "        \n",
    "        self.encode_relu3 = nn.ReLU()\n",
    "#         self.encode_maxpool3 = nn.MaxPool3d(kernel_size=(2, 2, 2), \n",
    "#                                              stride=(2, 2, 2),\n",
    "#                                             return_indices = True)\n",
    "\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Fully Connected Layers after 3D Convolutional Layers\n",
    "        First FC layer's input should be equal to \n",
    "        last convolutional layer's output\n",
    "        8192 = 8^3 * 16 \n",
    "            8^3 = (output of 2nd convolutional layer)\n",
    "            16 = number of out_channels\n",
    "        \"\"\"\n",
    "        \n",
    "#         self.encode_fc1 = nn.Sequential(\n",
    "#             nn.Linear(in_features=2048, \n",
    "#                       out_features=5096), \n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5))\n",
    "        \n",
    "        self.encode_fc1_linear = nn.Linear(in_features=2048, \n",
    "                                           out_features=5096)\n",
    "        self.encode_fc1_relu = nn.ReLU()\n",
    "        self.encode_fc1_dropout = nn.Dropout(0.5)\n",
    "        nn.init.xavier_uniform_(self.encode_fc1_linear.weight) #Xaviers Initialisation\n",
    "\n",
    "        \n",
    "#         self.encode_fc2 = nn.Sequential(\n",
    "#             nn.Linear(in_features = 5096,\n",
    "#                       out_features = 5096),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5))\n",
    "\n",
    "        self.encode_fc2_linear = nn.Linear(in_features=5096, \n",
    "                                           out_features=5096)\n",
    "        self.encode_fc2_relu = nn.ReLU()\n",
    "        self.encode_fc2_dropout = nn.Dropout(0.5)\n",
    "        nn.init.xavier_uniform_(self.encode_fc2_linear.weight) #Xaviers Initialisation\n",
    "        \n",
    "        \"\"\"\n",
    "        The last fully connected layer's output is the dimensions\n",
    "        of the embeddings?\n",
    "        \n",
    "        PyTorch VAE example uses output of 20 dimensions for mu &\n",
    "        logvariance\n",
    "        \"\"\"\n",
    "#         self.encode_fc31 = nn.Sequential(\n",
    "#             nn.Linear(in_features=5096,\n",
    "#                       out_features=20))\n",
    "        \n",
    "        self.encode_fc31_linear = nn.Linear(in_features=5096, \n",
    "                                           out_features=20)\n",
    "        self.encode_fc31_relu = nn.ReLU()\n",
    "        self.encode_fc31_dropout = nn.Dropout(0.5)\n",
    "        nn.init.xavier_uniform_(self.encode_fc31_linear.weight) #Xaviers Initialisation\n",
    "\n",
    "        \n",
    "#         self.encode_fc32 = nn.Sequential(\n",
    "#             nn.Linear(in_features=5096,\n",
    "#                       out_features=20))\n",
    "\n",
    "        self.encode_fc32_linear = nn.Linear(in_features=5096, \n",
    "                                           out_features=20)\n",
    "        self.encode_fc32_relu = nn.ReLU()\n",
    "        self.encode_fc32_dropout = nn.Dropout(0.5)\n",
    "        nn.init.xavier_uniform_(self.encode_fc32_linear.weight) #Xaviers Initialisation\n",
    "\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        The Decoding Layers\n",
    "        nn.Conv3d -> nn.ConvTranspose3d\n",
    "        nn.MaxPool3d -> nn.MaxUnpool3d\n",
    "        \"\"\"\n",
    "        \n",
    "        self.decode_fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=20,\n",
    "                      out_features=5096))\n",
    "        \n",
    "        self.decode_fc2 = nn.Sequential(\n",
    "            nn.Linear(in_features=5096, \n",
    "                      out_features=5096), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        #init.xavier_normal(self.fc1.state_dict()['weight'])\n",
    "        \n",
    "        self.decode_fc3 = nn.Sequential(\n",
    "            nn.Linear(in_features = 5096,\n",
    "                      out_features = 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        #init.xavier_normal(self.fc2.state_dict()['weight'])\n",
    "        \n",
    "        \n",
    "        self.decode_conv1 = nn.ConvTranspose3d(in_channels=32, \n",
    "                                              out_channels=16, \n",
    "                                              kernel_size=(4,4,4),\n",
    "                                              stride = (2,2,2),\n",
    "                                              padding=(1,1,1))\n",
    "        self.decode_relu1 = nn.ReLU()\n",
    "        self.decode_maxunpool1 = nn.MaxUnpool3d(kernel_size=(2, 2, 2), \n",
    "                                                     stride=(2, 2, 2))\n",
    "        #init.xavier_normal(self.group1.state_dict()['weight'])\n",
    "        \n",
    "        self.decode_conv2 = nn.ConvTranspose3d(in_channels=16, \n",
    "                                              out_channels=8, \n",
    "                                              kernel_size=(4,4,4),\n",
    "                                              stride = (2,2,2),\n",
    "                                              padding=(1,1,1))\n",
    "        self.decode_relu2 = nn.ReLU()\n",
    "        self.decode_maxunpool2 = nn.MaxUnpool3d(kernel_size=(2, 2, 2), \n",
    "                                                     stride=(2, 2, 2))\n",
    "        \n",
    "        self.decode_conv3 = nn.ConvTranspose3d(in_channels=8, \n",
    "                                              out_channels=1, \n",
    "                                              kernel_size=(4,4,4),\n",
    "                                              stride = (2,2,2),\n",
    "                                              padding=(1,1,1))\n",
    "        self.decode_relu3 = nn.ReLU()\n",
    "        self.decode_maxunpool3 = nn.MaxUnpool3d(kernel_size=(2, 2, 2), \n",
    "                                                     stride=(2, 2, 2))\n",
    "        \n",
    "        \n",
    "    # Encoding part of VAE\n",
    "    def encode(self, x):\n",
    "#         h1 = F.relu(self.fc1(x))\n",
    "#         return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "        print(\"Starting Encoding\")\n",
    "        print(\"----------------------------\")\n",
    "        \n",
    "        out = self.encode_conv1(x)\n",
    "#         print(\"First Conv output shape = \" + str(out.shape))\n",
    "        #print(out.shape)\n",
    "        out = self.encode_relu1(out)\n",
    "#         print(\"First ReLU Layer output shape = \" + str(out.shape))\n",
    "        size1 = out.size()\n",
    "        out, ind1 = self.encode_maxpool1(out)\n",
    "#         print(\"First MaxPooling output shape = \" + str(out.shape))\n",
    "#         print(\"Ind1 shape = \" + str(ind1.shape))\n",
    "#         #print(\"Size1 = \" + str(size1))\n",
    "#         print(\"----------------------------\")\n",
    "        \n",
    "        out = self.encode_conv2(out)\n",
    "#         print(\"Second Conv output shape = \" + str(out.shape))\n",
    "        out = self.encode_relu2(out)\n",
    "#         print(\"Second ReLU Layer output shape = \" + str(out.shape))\n",
    "        size2 = out.size()\n",
    "        out, ind2 = self.encode_maxpool2(out)\n",
    "#         print(\"Second MaxPooling output shape = \" + str(out.shape))\n",
    "#         print(\"Ind2 shape = \" + str(ind2.shape))\n",
    "        #print(\"Size2 = \" + str(size2))\n",
    "#          print(\"----------------------------\")\n",
    "        \n",
    "        out = self.encode_conv3(out)\n",
    "#         print(\"Last Conv output shape = \" + str(out.shape))\n",
    "        out = self.encode_relu3(out)\n",
    "#         print(\"Last ReLU output shape = \" + str(out.shape))\n",
    "        size3 = out.size()\n",
    "#         out, ind3 = self.encode_maxpool3(out)\n",
    "#         print(\"Last Conv Layer output shape = \" + str(out.shape))\n",
    "#         print(\"Ind3 shape = \" + str(ind3.shape))\n",
    "        #print(\"Size3 = \" + str(size3))\n",
    "#         print(\"----------------------------\")\n",
    "\n",
    "        \"\"\"\n",
    "        From here on, the convolutional layers' output is flattened\n",
    "        into a rank 1 tensor of size x & put into a fully connected \n",
    "        network to output ??????\n",
    "        \n",
    "        https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "        PyTorch's own example uses just 2 fully-connected layers\n",
    "        to output mu and logvar predictions, below we use 3.\n",
    "        \"\"\"\n",
    "        #out = out.view(out.size(0), -1)\n",
    "        out = out.view(1, -1)\n",
    "#         print(\"Last Conv Layer output shape after reshaping \\n \\\n",
    "#                 (Input to first FC layer) = \" + str(out.shape))\n",
    "        \n",
    "#         out = self.encode_fc1(out)\n",
    "    \n",
    "        out = self.encode_fc1_linear(out)\n",
    "        out = self.encode_fc1_relu(out)\n",
    "        out = self.encode_fc1_dropout(out)\n",
    "        \n",
    "#         out = self.encode_fc2(out)\n",
    "\n",
    "        out = self.encode_fc2_linear(out)\n",
    "        out = self.encode_fc2_relu(out)\n",
    "        out = self.encode_fc2_dropout(out)\n",
    "        \n",
    "        \n",
    "#         out_mu = self.encode_fc31(out)\n",
    "        \n",
    "        out_mu = self.encode_fc31_linear(out)\n",
    "        out_mu = self.encode_fc31_relu(out_mu)\n",
    "        out_mu = self.encode_fc31_dropout(out_mu)\n",
    "        \n",
    "#         out_logvar = self.encode_fc32(out)\n",
    "\n",
    "        out_logvar = self.encode_fc32_linear(out)\n",
    "        out_logvar = self.encode_fc32_relu(out_logvar)\n",
    "        out_logvar = self.encode_fc32_dropout(out_logvar)\n",
    "        \n",
    "        print(\"Encode - Forward Pass Finished\")\n",
    "        print(out_mu.shape)\n",
    "        print(out_logvar.shape)\n",
    "        print(\"----------------------------\")\n",
    "        \n",
    "#         return out_mu, out_logvar, [ind1,ind2,ind3], [size1,size2,size3]\n",
    "        return out_mu, out_logvar, [ind1,ind2], [size1,size2]\n",
    "    \n",
    "\n",
    "    # Reparametrization Trick\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        torch.exp = returns a new tensor with the exponential of \n",
    "                    the elements of input\n",
    "        rand_like = returns a tensor with the same size as input\n",
    "                    that is filled with random numbers from a normal\n",
    "                    distribution with mean 0 and variance 1\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"Reparametrization...\")\n",
    "        print(\"logvar (in reparametrization) = \\n\" + str(logvar))\n",
    "        print(\"mu (in reparametrization) = \\n\" + str(mu))\n",
    "        \n",
    "        std = torch.exp(0.5*logvar)\n",
    "#         std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "        print(\"0.5*logvar (in reparametrization) = \\n\" + str(0.5*logvar))\n",
    "        print(\"std (in reparametrization) = \\n\" + str(std))\n",
    "        print(\"eps (in reparametrization) = \\n\" + str(eps))\n",
    "        \n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    \n",
    "    # Decoding part of VAE\n",
    "    def decode(self, z, indices_list, size_list):\n",
    "#         h3 = F.relu(self.fc3(z))\n",
    "#         return torch.sigmoid(self.fc4(h3))\n",
    "        print(\"----------------------------\")\n",
    "        print(\"Starting Decoding\")\n",
    "#         print(\"z shape = \" + str(z.shape))\n",
    "        \n",
    "        out = self.decode_fc1(z)\n",
    "#         print(\"1st FC output shape = \" + str(out.shape))\n",
    "        out = self.decode_fc2(out)\n",
    "#         print(\"2nd FC output shape = \" + str(out.shape))\n",
    "        out = self.decode_fc3(out)\n",
    "#         print(\"Last FC output shape = \" + str(out.shape))\n",
    "        \n",
    "        out = out.view(1, 32, 4, 4, 4)\n",
    "#         print(\"First Deconv input shape = \" + str(out.shape))\n",
    "#         print(\"After last convolution (encoding stage) output shape = \" +\\\n",
    "#                   str(indices_list[1].shape))\n",
    "        out = self.decode_conv1(out)\n",
    "#         print(\"First Deconv output shape = \" + str(out.shape))\n",
    "        out = self.decode_relu1(out)\n",
    "#         print(\"First ReLU output shape = \" + str(out.shape))\n",
    "        # maxunpooling needs indices\n",
    "\n",
    "#         out = self.decode_maxunpool1(out,\n",
    "#                              indices = indices_list[1])\n",
    "        out = self.decode_maxunpool1(out,\n",
    "                                     indices = indices_list[1],\n",
    "                                     output_size = size_list[1])\n",
    "#         print(\"2nd MaxUnpool ouput shape = \" + str(out.shape))\n",
    "        \n",
    "        out = self.decode_conv2(out)\n",
    "#         print(\"2nd Deconv output shape = \" + str(out.shape))\n",
    "        out = self.decode_relu2(out)\n",
    "#         print(\"2nd ReLU output shape = \" + str(out.shape))\n",
    "        out = self.decode_maxunpool1(out,\n",
    "                     indices = indices_list[0])\n",
    "#         out = self.decode_maxunpool2(out,\n",
    "#                                      indices= indices_list[1],\n",
    "#                                      output_size = size_list[1])\n",
    "        \n",
    "        out = self.decode_conv3(out)\n",
    "        out = self.decode_relu3(out)\n",
    "        print(\"Last ReLU output shape = \" + str(out.shape))\n",
    "#         out = self.decode_maxunpool1(out,\n",
    "#                              indices = indices_list[0])\n",
    "        # there is no last maxunpool in https://github.com/pgtgrly/Convolution-Deconvolution-Network-Pytorch/blob/master/Neural_Network_Class.py\n",
    "#         out = self.decode_maxunpool2(out,\n",
    "#                                      indices= indices_list[0],\n",
    "#                                      output_size = size_list[0])\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, x):\n",
    "#         mu, logvar = self.encode(x.view(-1, 784))\n",
    "        mu, logvar, indices_list, size_list = self.encode(x)\n",
    "        print(\"logvar (after encoding) = \\n\" + str(logvar))\n",
    "        print(\"mu (after encoding) = \\n\" + str(mu))\n",
    "        \n",
    "        z = self.reparameterize(mu, logvar)\n",
    "#         print(\"z = \")\n",
    "#         print(z)\n",
    "        reconstructed_x = self.decode(z, indices_list, size_list)\n",
    "    \n",
    "        return reconstructed_x , mu, logvar\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1503,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1504,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for param in model.parameters():\n",
    "#     print(param.name)\n",
    "#     print(type(param.data), param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1505,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1506,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    print(\"--------------------------------------\")\n",
    "    print(\"Calculating Loss...\")\n",
    "#     print(\"recon_x shape = \" + str(recon_x.shape))\n",
    "    \n",
    "#     BCE = F.binary_cross_entropy(recon_x, \n",
    "#                                  x.view(-1, 1, 128, 128, 128), \n",
    "#                                  reduction='sum')\n",
    "#     print(\"BCE Loss = \" + str(BCE))\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    https://towardsdatascience.com/teaching-a-variational-autoencoder-vae-to-draw-mnist-characters-978675c95776\n",
    "    This article is about reconstructing MNIST dataset (2D images with 1 channels)\n",
    "    And it uses squared difference for the reconstruction loss, thus\n",
    "    it is safe to say that for 3D reconstruction we might use the same thing too.\n",
    "    \n",
    "    But the MSE loss seems to be around 1e20 magnitude, thus suggesting some\n",
    "    bug might exist in it.\n",
    "    \"\"\"\n",
    "    MSE = F.mse_loss(recon_x, \n",
    "                     x.view(-1, 1, 128, 128, 128), \n",
    "                     reduction='sum')\n",
    "    \n",
    "    print(\"MSE Loss = \" + str(MSE))\n",
    "\n",
    "    \"\"\"\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    \n",
    "    # https://vxlabs.com/2017/12/08/variational-autoencoder-in-pytorch-commented-and-annotated/\n",
    "    # normalises the KLD loss by batch_size\n",
    "    \"\"\"\n",
    "#     print(\"logvar (KLD Loss) = \\n\" + str(logvar))\n",
    "#     print(\"mu (KLD Loss) = \\n\" + str(mu))\n",
    "#     print(\"logvar.exp() (KLD Loss) = \\n\" + str(logvar.exp()))\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    KLD = KLD / batch_size\n",
    "    print(\"KLD Loss = \" + str(KLD))\n",
    "\n",
    "    \n",
    "#     return MSE\n",
    "#     return BCE\n",
    "#     return BCE + KLD\n",
    "    return MSE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1507,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(enumerate(train_loader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1508,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(enumerate(train_loader))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1509,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(enumerate(train_loader))[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1510,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(enumerate(train_loader))[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 128, 128, 128])"
      ]
     },
     "execution_count": 1511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(train_loader))[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1512,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "#     for batch_idx, (data, _) in enumerate(train_loader):\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "#         print(batch_idx)\n",
    "#         print(data)\n",
    "        \n",
    "        #print(\"Batch size = \" + str(data.shape))\n",
    "        \n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        \n",
    "#         print(\"Reconstructed Input = \\n \" + str(recon_batch))\n",
    "#         print(\"Real Input = \\n \" + str(data))\n",
    "#         print(\"Reconstructed Input Shape = \\n \" + str(recon_batch.shape))\n",
    "#         print(\"Real Input Shape = \\n \" + str(data.shape))\n",
    "\n",
    "#         print(\"logvar = \\n\" + str(logvar))\n",
    "#         print(\"mu = \\n\" + str(mu))\n",
    "        \n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        loss_history.append(loss.item())\n",
    "        \n",
    "        optimizer.step()\n",
    "        # if batch_idx % args.log_interval == 0:\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "#         for i, (data, _) in enumerate(test_loader):\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n], \n",
    "                                        recon_batch.view(batch_size, 1, 128, 128, 128)[:n]])\n",
    "                                      #recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
    "#                                         recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "#                 save_image(comparison.cpu(),\n",
    "#                          'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 / 20\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0.0000e+00, 0.0000e+00, 4.5429e+06, 4.0098e+05, 3.9971e+06, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9019e+05, 0.0000e+00, 1.4348e+06,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6227e+06, 5.0755e+06, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[4.8991e+06, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 4.5237e+06, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.7581e+06, 0.0000e+00, 0.0000e+00, 1.2391e+06,\n",
      "         0.0000e+00, 0.0000e+00]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0.0000e+00, 0.0000e+00, 4.5429e+06, 4.0098e+05, 3.9971e+06, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9019e+05, 0.0000e+00, 1.4348e+06,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6227e+06, 5.0755e+06, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[4.8991e+06, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 4.5237e+06, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.7581e+06, 0.0000e+00, 0.0000e+00, 1.2391e+06,\n",
      "         0.0000e+00, 0.0000e+00]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0.0000e+00, 0.0000e+00, 2.2715e+06, 2.0049e+05, 1.9985e+06, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4509e+05, 0.0000e+00, 7.1740e+05,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 8.1136e+05, 2.5378e+06, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1.0000, 1.0000,    inf,    inf,    inf, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "            inf, 1.0000,    inf, 1.0000, 1.0000, 1.0000,    inf,    inf, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.7261,  1.1456, -0.3825, -0.4785,  0.9009, -0.0952,  2.1472,  1.5053,\n",
      "         -0.6261,  1.2031, -0.2690,  1.3064, -0.6834,  0.5597,  0.9461,  0.1690,\n",
      "         -0.1202,  0.9094, -0.1968, -1.7940]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(7129966218999496704., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(inf, grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [0/32 (0%)]\tLoss: inf\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0.0067, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0332, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0272, 0.0165, 0.0000, 0.0000, 0.0180, 0.0000, 0.0000, 0.0000, 0.0158,\n",
      "         0.0000, 0.0000]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0.0067, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0332, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0272, 0.0165, 0.0000, 0.0000, 0.0180, 0.0000, 0.0000, 0.0000, 0.0158,\n",
      "         0.0000, 0.0000]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0.0034, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1.0034, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.6317,  0.3267,  0.7348, -0.2910,  0.2520, -0.4122, -0.1374,  1.3288,\n",
      "          0.1197,  0.4881,  0.8917,  0.3181, -1.0064,  0.4353,  0.0532,  0.7743,\n",
      "         -1.0202,  0.1599, -0.2468, -2.3120]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(129342009107569180672., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(0.0014, grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [1/32 (3%)]\tLoss: 129342009107569180672.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0130, 0.0013, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0080, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0130, 0.0013, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0080, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0065, 0.0007, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0065, 1.0007, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.0211,  0.3217, -1.8052,  0.1765, -1.4409, -1.3088,  0.6238, -0.0994,\n",
      "         -0.1341,  1.2435,  0.3451,  1.0443, -1.4033,  0.6435, -0.2536, -0.9305,\n",
      "          1.1877, -0.4275,  0.9791, -1.7073]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(92985538641920., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [2/32 (6%)]\tLoss: 92985538641920.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0.0000, 0.0000, 0.0021, 0.0023, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0.0000, 0.0000, 0.0021, 0.0023, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.5361,  0.3941, -0.3976, -1.9831, -1.0168, -1.1205,  0.4651, -0.8740,\n",
      "          0.8145, -0.5798, -0.4902, -1.0462,  2.1096, -0.8703,  0.1195, -1.1309,\n",
      "          1.0069, -0.0815, -0.1380, -0.3435]])\n",
      "----------------------------\n",
      "Starting Decoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(3682479121067147264., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(4.9472e-06, grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [3/32 (9%)]\tLoss: 3682479121067147264.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.1890, -1.0221, -0.8348,  0.7948,  0.7083,  0.7714,  0.1178,  1.3044,\n",
      "          0.0223,  0.4428, -1.2414,  0.8905, -0.1100, -0.3048,  0.8278, -0.4714,\n",
      "         -1.6581, -0.7537, -1.0330, -0.6892]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(11413887271431045120., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [4/32 (12%)]\tLoss: 11413887271431045120.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.2232,  1.4984,  0.7458, -0.9689, -1.2568, -1.1356,  0.4701, -1.4626,\n",
      "         -0.2925,  1.1495, -0.4704, -0.7191,  0.8309,  0.2804, -1.3414, -0.5050,\n",
      "         -0.9411, -0.5919,  1.0600,  0.2341]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(171314803644563456., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [5/32 (16%)]\tLoss: 171314803644563456.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.8701,  0.3890,  0.8505, -0.5402,  1.0202,  0.7216,  0.2618, -0.1743,\n",
      "         -0.9862,  1.6491,  1.1155,  0.6326,  2.5036, -0.4502,  0.8001, -1.7061,\n",
      "         -0.1163, -0.7745, -0.3665,  0.2783]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(56907392532476854272., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [6/32 (19%)]\tLoss: 56907392532476854272.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.1644, -0.6142,  2.3279, -1.9563,  0.2576,  0.8899,  0.3845,  1.9257,\n",
      "         -0.0346, -2.0502,  0.9642,  0.3439,  0.6153, -0.2046,  0.1352, -0.8213,\n",
      "          0.1374, -0.4857, -0.8255,  0.6143]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(2520466579443941376., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [7/32 (22%)]\tLoss: 2520466579443941376.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.8052, -0.9453, -0.4172,  1.1577,  0.1760, -1.4790,  0.3572, -0.6024,\n",
      "         -0.0915,  0.1395,  2.1156,  0.2492,  0.9990,  0.5430,  1.8689, -0.1424,\n",
      "          0.2493, -2.2260,  0.5922, -0.2003]])\n",
      "----------------------------\n",
      "Starting Decoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(6439071595012030464., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [8/32 (25%)]\tLoss: 6439071595012030464.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.7193,  1.7187, -1.4380,  1.1109, -1.1618,  0.3730,  0.2692,  1.1777,\n",
      "         -0.0223,  0.3605,  0.5255, -0.1656, -1.9777,  2.0941,  0.8578,  0.2379,\n",
      "          0.9121,  0.1345,  0.3505, -0.1133]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(5961383270766608384., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [9/32 (28%)]\tLoss: 5961383270766608384.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 2.4396, -0.6600, -0.3355, -0.2461,  0.1081,  2.0281,  1.0096,  0.2315,\n",
      "         -0.3979,  0.4998, -0.4613, -0.5120, -1.1508, -0.6711,  1.0364, -1.3204,\n",
      "          1.3193,  1.5102, -0.3474, -0.2588]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(431491278052524032., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [10/32 (31%)]\tLoss: 431491278052524032.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 1.1763,  0.7602, -0.4941, -0.6676,  0.7326,  0.1603, -0.1139, -2.5305,\n",
      "          1.3467, -0.0039,  0.9097, -2.0763,  0.7837, -1.1525,  1.2200, -0.1281,\n",
      "         -0.3424,  0.2246,  0.1051, -0.6152]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(20418227795939819520., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [11/32 (34%)]\tLoss: 20418227795939819520.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.3485,  1.2315,  0.4248, -0.0218, -0.0066, -0.1553, -2.2527,  0.9871,\n",
      "          1.9014, -1.7233,  0.8084, -0.1127, -0.5265,  0.2509,  1.6951, -0.2501,\n",
      "         -1.2873, -0.1041, -1.2079, -1.4654]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(58316856488122908672., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [12/32 (38%)]\tLoss: 58316856488122908672.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.0879,  1.2670, -0.9327, -0.6465,  0.1554,  0.3939, -1.7179,  0.6574,\n",
      "         -2.2662, -0.1657, -0.4805,  0.7541, -1.3019,  0.2895, -0.2243,  0.4825,\n",
      "         -0.4362,  0.6446,  1.4023, -0.8413]])\n",
      "----------------------------\n",
      "Starting Decoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(40561705228284461056., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [13/32 (41%)]\tLoss: 40561705228284461056.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.4855,  1.1372, -1.6411,  0.2387,  1.3799,  0.2720, -1.0857, -0.9818,\n",
      "          0.1284,  0.1108,  0.9303, -0.7335, -0.8265, -0.5553, -0.3681, -0.1383,\n",
      "         -1.7915,  1.1120,  1.6440,  1.4232]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(144827926697352364032., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [14/32 (44%)]\tLoss: 144827926697352364032.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.9677, -1.2020, -1.2131,  0.5749, -1.2141,  0.4661,  0.4505,  2.3019,\n",
      "          0.2494,  0.9443,  0.1950, -0.0974, -0.8213, -1.1662, -0.8307,  1.0728,\n",
      "         -1.1486,  0.0960,  0.6645, -0.6010]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(50599472333693386752., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [15/32 (47%)]\tLoss: 50599472333693386752.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.0023, -0.8530, -0.0827, -1.4775, -0.6060, -0.8640, -0.5145,  0.2599,\n",
      "         -0.6300, -1.7710, -0.5382,  1.1978,  1.8005,  0.5808, -0.3302, -0.6693,\n",
      "         -0.0721,  1.0936, -1.3106,  0.1338]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(10166051525068062720., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [16/32 (50%)]\tLoss: 10166051525068062720.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.0076,  0.4691,  1.7383,  0.5391,  0.2288, -0.4859,  0.0570,  0.6152,\n",
      "         -1.0679,  0.7829, -0.5749, -0.4282,  3.0715, -0.6055,  0.4668,  1.0462,\n",
      "         -1.4508,  0.6149,  1.2283, -0.2772]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(119241552247190978560., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [17/32 (53%)]\tLoss: 119241552247190978560.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.6437, -0.4349,  0.1722, -0.3630, -1.7066, -0.4421, -0.0319,  0.8159,\n",
      "          0.3642,  1.4164, -0.1530, -0.0366, -1.2453,  2.2242, -0.9553,  0.7260,\n",
      "          1.8197, -0.0834,  0.9328,  0.4595]])\n",
      "----------------------------\n",
      "Starting Decoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(1386609781829009408., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [18/32 (56%)]\tLoss: 1386609781829009408.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 1.1357, -0.1122,  0.0555,  1.4310,  0.0726, -1.4282,  0.4771,  1.6662,\n",
      "          1.5873,  0.4230, -0.1015,  1.5657, -0.8068, -0.1820, -0.7660,  1.1191,\n",
      "         -0.7234,  0.4104, -1.4624, -0.5798]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(5458832239838101504., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [19/32 (59%)]\tLoss: 5458832239838101504.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.7902, -0.2972, -0.4363, -0.7988, -1.5548,  1.0854,  0.4981,  0.7885,\n",
      "          1.2516,  0.2437, -1.8613, -0.5913,  0.1838,  0.1268, -2.2471, -2.4012,\n",
      "          0.3341,  0.4050, -1.0703, -0.3630]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(568431053245513728., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [20/32 (62%)]\tLoss: 568431053245513728.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-2.0943,  1.7245,  0.4051, -0.1913, -0.3012,  0.9792,  0.5047,  1.1783,\n",
      "          1.4215, -1.6933, -0.1336,  0.1226, -0.4944,  0.7109, -0.6893, -1.1758,\n",
      "         -0.2850, -1.1253, -0.0890,  0.3801]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(9442163477708800., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [21/32 (66%)]\tLoss: 9442163477708800.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.3986,  0.2291, -0.2830,  0.9492,  0.4759, -0.0861,  0.8566, -1.3752,\n",
      "         -0.1730, -1.2255,  2.9108,  0.1029,  0.4251,  0.9415,  2.2225,  0.1620,\n",
      "         -0.8476,  2.2929, -2.0565,  1.1089]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(15249034406648610816., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [22/32 (69%)]\tLoss: 15249034406648610816.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.1044,  0.1785, -0.5489,  0.9457, -0.1977,  0.3057, -0.3507, -0.6456,\n",
      "          0.3666, -0.4732,  0.4755, -0.6530,  0.3969, -0.7348, -2.0986, -1.3783,\n",
      "         -0.0473, -0.8989,  0.3479, -1.0896]])\n",
      "----------------------------\n",
      "Starting Decoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(13218407953960271872., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [23/32 (72%)]\tLoss: 13218407953960271872.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.4839, -0.2305, -0.4565, -1.4066, -0.0562,  0.4986,  1.5097,  0.4709,\n",
      "          1.7145,  0.5850,  0.7549,  0.4037, -0.1911, -0.0926,  0.5221,  0.1281,\n",
      "          0.5137, -0.7257, -0.3726,  0.4873]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(1026129259665555456., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [24/32 (75%)]\tLoss: 1026129259665555456.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 1.2197,  0.9022, -0.5884, -1.3467, -0.4417, -0.4659, -0.4880, -0.1610,\n",
      "         -0.4674, -0.3763, -2.4246, -0.4526, -0.1276,  1.1165, -0.0089,  0.2696,\n",
      "         -0.8072,  0.3545,  0.8504, -0.5946]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(30881037509344624640., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [25/32 (78%)]\tLoss: 30881037509344624640.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.5494,  1.0085,  0.8853, -0.4840, -1.0697, -0.4426, -1.2093, -0.4258,\n",
      "          0.0735,  1.0757,  0.0973,  0.3657,  0.3303,  1.6644, -0.0897, -1.1678,\n",
      "          0.4231,  2.1314,  0.5944,  0.0185]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(673125930893312., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [26/32 (81%)]\tLoss: 673125930893312.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.9933, -0.6304, -1.3468, -0.5711,  0.1500, -0.5942,  1.3605,  2.4105,\n",
      "          0.6230, -1.3635, -0.2658,  0.8162, -0.6301, -0.8528,  0.0553,  1.0694,\n",
      "          0.8686, -0.7626, -1.0811,  0.7256]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(1024848.2500, grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [27/32 (84%)]\tLoss: 1024848.250000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.3013,  0.2595, -0.2500, -0.5018, -0.6079, -0.3113,  1.6161,  2.1701,\n",
      "          0.2658, -0.8569, -0.3414,  0.7859, -0.7619,  0.9699, -1.0123,  1.3861,\n",
      "          0.9582,  1.9976,  0.8523, -0.4553]])\n",
      "----------------------------\n",
      "Starting Decoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(1425600938051108864., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [28/32 (88%)]\tLoss: 1425600938051108864.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.6980, -0.1507, -2.2080,  1.4952, -1.7708,  1.1263, -0.4340, -1.4798,\n",
      "          2.1170,  3.1820, -0.5613,  0.3653, -0.4623,  0.3154,  1.7985,  0.0628,\n",
      "          0.4606, -0.5810,  0.7212, -0.8812]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(4799747036998008832., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [29/32 (91%)]\tLoss: 4799747036998008832.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 1.9099,  1.1079,  1.3015, -0.5672, -0.4474,  0.1087,  1.5610,  0.0187,\n",
      "          1.0401, -1.4730,  0.0240, -0.4312,  0.1342, -0.1666,  1.6433, -0.7963,\n",
      "          0.1538, -0.9897,  1.0777, -0.6039]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(681490707193528320., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [30/32 (94%)]\tLoss: 681490707193528320.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.1208, -0.4303, -1.4554,  1.4086,  0.0396, -0.1150,  1.8842,  0.1923,\n",
      "          0.8054,  0.2651, -0.2264,  0.5825,  1.5130,  1.3421,  1.6423, -1.6758,\n",
      "          0.5723, -1.1711,  0.0387,  0.6653]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(201689698293369536512., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 1 [31/32 (97%)]\tLoss: 201689698293369536512.000000\n",
      "====> Epoch: 1 Average loss: inf\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6wAAAHdCAYAAAAZ2vQJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116e3bc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2 / 20\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.7342,  2.0704,  0.8731,  1.1279,  0.1502, -0.0579, -0.0626,  1.2011,\n",
      "          1.2139,  0.8060, -0.6187, -0.4719, -0.0028, -0.8656, -0.1620, -0.5534,\n",
      "          0.2011,  0.4010, -0.5165, -0.3158]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(681490707193528320., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [0/32 (0%)]\tLoss: 681490707193528320.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.2928,  0.1431, -1.0469,  0.6894, -0.5105,  0.3671, -0.8669,  0.2990,\n",
      "          0.4499,  0.3753, -1.5653,  0.3374, -1.5407,  0.6840, -0.8306,  1.4479,\n",
      "         -0.8432, -0.5083, -0.8407,  0.5071]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(129342009107569180672., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [1/32 (3%)]\tLoss: 129342009107569180672.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.3261, -1.1965, -2.9508,  0.4698,  1.6150,  0.8145,  0.9552, -0.6198,\n",
      "         -0.1259,  0.1889,  1.9768,  0.7426, -0.8687, -0.0637,  0.6842, -0.5279,\n",
      "          0.5268, -0.1736, -0.5617,  0.0307]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(40561705228284461056., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [2/32 (6%)]\tLoss: 40561705228284461056.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.8886, -0.4215,  0.2636,  0.7197,  0.8503, -2.1728, -1.4677,  0.8017,\n",
      "          0.0943, -1.8477,  0.4429, -0.6851,  1.2190,  1.3482, -2.2806, -0.4215,\n",
      "          0.7573, -0.4977, -0.2932, -0.2869]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(30881037509344624640., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [3/32 (9%)]\tLoss: 30881037509344624640.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.2286,  0.3056, -2.7427,  0.6314, -1.1239, -1.8803,  1.6893,  0.1750,\n",
      "         -0.2040, -0.0943,  0.4746,  0.4157, -1.6473,  1.1748,  0.2049,  1.9361,\n",
      "         -0.7556,  0.5470,  1.0436, -0.1026]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(431491278052524032., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [4/32 (12%)]\tLoss: 431491278052524032.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.1239,  1.5114,  0.5456, -0.0758, -0.2394,  1.0999,  0.6829, -1.9241,\n",
      "          0.0173, -1.2745, -0.0352, -0.5432, -0.1984, -0.1429,  1.0128,  2.0784,\n",
      "         -0.0203,  1.3464,  0.5385, -0.6794]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(568431053245513728., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [5/32 (16%)]\tLoss: 568431053245513728.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 1.0566,  0.5321,  1.7237,  1.7510,  0.6977,  0.1877,  1.2508, -0.8857,\n",
      "          1.9533,  1.3233, -0.6280, -1.0704, -0.1677,  0.6132, -0.0582,  1.2116,\n",
      "          0.5735,  0.0955, -0.4578,  0.8051]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(50599472333693386752., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [6/32 (19%)]\tLoss: 50599472333693386752.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.0897, -1.6953, -0.7529,  1.6410,  0.3449, -0.0447, -2.6846, -0.8018,\n",
      "          0.6026,  1.1077,  0.7770, -0.3251,  1.5439,  0.7351, -0.4827,  0.7892,\n",
      "         -0.0867, -0.4222, -1.5149, -0.3875]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(144827926697352364032., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [7/32 (22%)]\tLoss: 144827926697352364032.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.1526, -0.6437, -1.0407,  1.3286,  0.5037,  0.9469,  1.5723,  0.9573,\n",
      "         -0.4595,  1.6939,  0.2635, -2.6523,  0.1673,  0.8155,  0.2191, -0.6782,\n",
      "          0.5273,  0.4851,  0.2439,  1.5970]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(673125930893312., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [8/32 (25%)]\tLoss: 673125930893312.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.2379,  0.2824, -1.6246, -2.4495, -0.4836,  1.1549, -1.7241, -1.1493,\n",
      "          0.6683, -0.9381,  0.5548, -0.1829, -0.5854,  0.8263,  1.2455, -0.9755,\n",
      "          0.8487,  0.3930, -0.4664, -1.7372]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(13218407953960271872., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [9/32 (28%)]\tLoss: 13218407953960271872.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 1.1163, -1.6849,  1.0695, -0.0189, -0.6518, -0.5402,  0.0601, -0.5607,\n",
      "          0.1419,  1.2750,  0.9009,  0.2152, -0.1218,  0.9179,  0.1632,  0.7699,\n",
      "         -0.0569, -0.7099,  0.1026,  0.3996]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(6439071595012030464., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [10/32 (31%)]\tLoss: 6439071595012030464.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-2.2289,  0.5297,  0.3977, -1.0385, -0.7708, -0.2360, -0.0825,  1.0988,\n",
      "         -0.4202,  0.2297, -1.1203,  0.0891, -0.7983, -0.5710,  0.3933,  0.4570,\n",
      "         -1.6345,  0.0084,  0.3598,  0.7820]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(171314803644563456., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [11/32 (34%)]\tLoss: 171314803644563456.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.8373,  0.0143,  0.0762, -0.4113,  0.5876, -0.0134, -0.4110,  1.1927,\n",
      "         -1.4105, -0.8476,  0.2347,  2.9605, -0.5577, -2.2584, -1.5342,  0.0551,\n",
      "         -0.5625,  0.4384,  1.0086,  0.1044]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(10166051525068062720., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [12/32 (38%)]\tLoss: 10166051525068062720.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.5583,  0.7371, -0.1775,  0.1332,  1.0089, -0.4088, -0.7235,  0.6146,\n",
      "         -1.0310,  0.3560,  0.5226, -0.9495,  0.5179, -1.8630, -1.6558,  0.0587,\n",
      "         -0.2996,  0.2203, -0.9125,  0.8195]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(1026129259665555456., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [13/32 (41%)]\tLoss: 1026129259665555456.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 1.4088, -0.0468, -0.0485, -2.6827, -0.9803,  1.2195, -1.4084,  1.2280,\n",
      "         -0.6599,  0.0561,  0.7253, -0.3229,  0.1405,  0.3951, -1.0131,  0.7042,\n",
      "          1.3949, -0.3314, -2.2818, -0.4850]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(3682479121067147264., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [14/32 (44%)]\tLoss: 3682479121067147264.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.4427, -0.2686, -0.6776,  0.4182,  2.1258,  0.2747,  1.5179,  0.3460,\n",
      "          0.2698,  1.5677, -0.8709, -1.7591, -0.3835, -1.9792,  0.5788, -1.6680,\n",
      "          0.6875, -3.6589,  1.2755,  0.0385]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(56907392532476854272., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [15/32 (47%)]\tLoss: 56907392532476854272.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 1.3576, -0.2145, -0.9159,  0.8974, -0.3018,  1.7983,  0.3062, -0.6325,\n",
      "          1.1306, -0.4614, -0.6458,  0.7780, -0.4690, -0.7878, -0.5754,  0.6811,\n",
      "          0.1589,  0.4883, -0.5312, -0.5138]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(11413887271431045120., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [16/32 (50%)]\tLoss: 11413887271431045120.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.4979,  0.1143,  0.2583,  0.0483, -1.0875,  0.3645,  1.9736,  0.3238,\n",
      "         -1.4407, -0.6677, -0.3168,  1.7874, -1.4844,  0.7940,  1.9756,  1.6242,\n",
      "         -0.0426,  0.5616,  1.2957, -0.4719]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(4799747036998008832., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [17/32 (53%)]\tLoss: 4799747036998008832.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.7451, -0.0047, -0.7232,  1.4535,  0.6581, -0.0879,  0.1439, -0.8507,\n",
      "          0.5057,  1.6197, -0.2711,  1.1656,  0.0846, -0.4523, -2.3755,  0.0186,\n",
      "          1.6229,  0.5108, -0.1021, -1.3538]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(9442163477708800., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [18/32 (56%)]\tLoss: 9442163477708800.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.4817,  1.9455,  0.1348, -0.2438,  1.5483, -1.3597, -0.1924, -0.4038,\n",
      "         -0.4617, -0.5580, -1.3244,  0.8250, -0.7629,  0.9709, -0.1050, -0.9762,\n",
      "          1.0242,  0.1294,  0.7232, -0.8757]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(1425600938051108864., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [19/32 (59%)]\tLoss: 1425600938051108864.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.3415,  0.1465,  1.1626, -0.3543,  0.0925,  1.0350,  1.6450,  2.2989,\n",
      "          0.6155, -1.3643,  0.0245, -0.6646,  0.8122,  0.8399,  1.2412, -1.4544,\n",
      "         -1.3127,  0.3433,  0.8346, -0.3470]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(5961383270766608384., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [20/32 (62%)]\tLoss: 5961383270766608384.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.7401,  0.9119,  0.0353, -0.4813, -3.1791,  1.7006,  1.2538,  0.9903,\n",
      "         -0.5301, -0.1803,  1.8465, -0.4357, -0.6508, -0.6260,  0.7473,  1.0020,\n",
      "         -0.7360, -1.6086,  0.3836,  0.3535]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(20418227795939819520., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [21/32 (66%)]\tLoss: 20418227795939819520.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.2518, -0.2384, -0.3564,  0.4346,  0.3544, -1.5849,  1.5000,  1.1503,\n",
      "          1.2742,  0.1671,  1.6871, -0.7555, -1.2552,  1.4169,  0.0321,  0.6047,\n",
      "          0.6304,  1.3457, -0.5678, -0.4649]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(15249034406648610816., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [22/32 (69%)]\tLoss: 15249034406648610816.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 1.5361,  1.6597, -0.6260, -0.7315,  1.5435, -1.2773, -0.4842, -0.9815,\n",
      "          0.9417,  1.3806, -1.1545, -1.7586,  1.4067, -1.3911, -0.9786,  1.2939,\n",
      "          0.3642,  0.0659, -1.2255, -0.2373]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(2520466579443941376., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [23/32 (72%)]\tLoss: 2520466579443941376.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.0014,  0.1359, -0.4730, -0.0583,  0.9311,  0.0508, -0.7656,  0.7161,\n",
      "         -1.1319, -0.7369,  1.9783,  0.3927,  0.4861, -1.3071, -0.1040, -0.5174,\n",
      "         -0.7287,  0.5413,  2.2238, -1.3798]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(119241552247190978560., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [24/32 (75%)]\tLoss: 119241552247190978560.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.6352,  1.1892,  0.8011, -0.0777, -0.8995, -2.8855, -2.6790, -1.8624,\n",
      "          1.2510,  1.6864, -0.4606,  0.4808,  0.1821,  2.6816, -0.0359,  1.5286,\n",
      "          0.0276, -1.7678, -0.7848,  0.2715]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(58316856488122908672., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [25/32 (78%)]\tLoss: 58316856488122908672.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.4971, -0.6326,  1.1653,  0.0357,  0.9162, -1.7040, -0.7497,  1.8111,\n",
      "          0.0782,  0.6628,  0.5226,  0.5762,  0.9967,  0.8469,  0.9606, -1.9010,\n",
      "          0.0089, -0.4300, -0.5270,  1.3689]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(1024848.2500, grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [26/32 (81%)]\tLoss: 1024848.250000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 2.7356, -0.6059, -1.1923, -0.5009,  1.0782, -0.7696, -0.8531, -0.2802,\n",
      "         -1.2257, -0.7626,  0.6936, -0.3043, -0.7267, -1.2255,  0.4762,  0.6344,\n",
      "          0.4225,  0.3298,  0.9068, -1.5280]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(201689698293369536512., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [27/32 (84%)]\tLoss: 201689698293369536512.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 1.2718,  1.1032,  0.0550, -0.5084,  0.2561, -1.2289,  0.7128, -0.4685,\n",
      "          0.8235,  0.7991, -0.1648, -0.7848,  2.1453,  0.9545,  0.7228, -0.1926,\n",
      "          0.1842, -0.0960, -2.0028,  1.7697]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(92985538641920., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [28/32 (88%)]\tLoss: 92985538641920.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.0382,  1.0263,  0.9228,  0.1629, -1.0906,  1.8546,  1.0350, -2.1227,\n",
      "          0.1306, -0.6702, -0.5354,  1.4832, -0.8782,  0.5899, -0.3198, -1.0668,\n",
      "          0.0175, -1.2601, -0.9476,  0.9972]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(7129966218999496704., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [29/32 (91%)]\tLoss: 7129966218999496704.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.9133, -2.1354, -0.8735, -1.1703,  0.4540, -0.0659, -0.6146, -0.3362,\n",
      "          0.1191,  0.1557, -1.3093,  0.3969,  0.8538, -1.3008,  0.8969, -0.7738,\n",
      "          0.1847, -0.0868, -0.7647, -0.4188]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(5458832239838101504., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [30/32 (94%)]\tLoss: 5458832239838101504.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 1.6793,  0.7049, -0.6798,  0.4279,  0.8250, -0.4590,  1.0961,  0.8257,\n",
      "          0.4742,  0.2428,  0.8344, -0.4521, -0.1721,  0.4100,  0.4838,  0.2949,\n",
      "         -0.5187, -1.3771, -0.8650,  0.2154]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(1386609781829009408., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 2 [31/32 (97%)]\tLoss: 1386609781829009408.000000\n",
      "====> Epoch: 2 Average loss: 29516452548412735488.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6wAAAHdCAYAAAAZ2vQJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nioiIiIiIiC8pYRURERERERFf+hv615AWPi07vgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1195426a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3 / 20\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.1605,  2.2147, -0.7131,  0.3080, -0.1802,  1.3009, -0.4135, -0.9198,\n",
      "          1.2148,  0.4052, -0.4318,  1.1912,  0.6507, -1.1108,  0.9709, -0.2531,\n",
      "         -0.1745,  2.4618, -1.0450, -1.3258]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(5961383270766608384., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [0/32 (0%)]\tLoss: 5961383270766608384.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.8043, -0.7599,  0.0591,  0.4626,  0.8741,  1.2909, -1.5699,  0.4354,\n",
      "          0.8113, -0.1739, -1.4924,  1.4987,  2.1173,  1.0435,  1.2581,  0.4348,\n",
      "          0.1977,  0.7131,  1.2801, -0.0672]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(50599472333693386752., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [1/32 (3%)]\tLoss: 50599472333693386752.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 1.9507, -1.0764, -0.4292, -0.7794, -0.2342, -0.1498,  0.6999, -0.5836,\n",
      "          1.1598, -0.9312,  0.7845,  0.0825,  0.3688, -0.3230,  0.1290,  1.3888,\n",
      "         -0.9980,  0.3197, -1.1490, -0.5303]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(7129966218999496704., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [2/32 (6%)]\tLoss: 7129966218999496704.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.7423, -0.2869, -1.2364, -0.3153, -0.0256, -0.1927, -3.0575,  1.5793,\n",
      "         -0.5058, -0.5026, -0.3570,  0.7841,  2.1671, -1.0584, -0.6613, -0.3840,\n",
      "         -0.0821,  1.4170,  0.2479,  1.4633]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(92985538641920., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [3/32 (9%)]\tLoss: 92985538641920.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.4502,  0.0979,  0.3554,  1.2900,  0.2153,  0.7187, -0.7440, -0.1024,\n",
      "         -0.2100,  0.3905, -1.1420, -0.6964, -0.4138, -0.0137,  0.8551,  0.4532,\n",
      "         -0.8205, -0.3145, -1.8929, -0.0202]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(201689698293369536512., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [4/32 (12%)]\tLoss: 201689698293369536512.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.1801, -0.4030, -1.0350, -0.0200, -2.0261, -0.8398,  0.2597, -0.2154,\n",
      "         -0.0531, -0.9703,  1.7741, -1.1909, -0.0406,  1.0171,  0.7971,  0.3080,\n",
      "          0.2326,  0.1735,  1.4303, -1.4514]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(171314803644563456., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [5/32 (16%)]\tLoss: 171314803644563456.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-2.0224,  0.2802, -1.1569, -0.6817, -0.7170,  1.2681, -0.4965,  0.1368,\n",
      "         -0.1951, -0.9256,  0.4275, -1.7659,  0.1434,  0.0210,  0.2202, -1.8600,\n",
      "          0.4652, -0.3131, -1.1186,  0.9150]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(144827926697352364032., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [6/32 (19%)]\tLoss: 144827926697352364032.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.5980, -1.1449,  0.7398, -0.1701, -0.4225,  1.8845, -0.4301, -0.1599,\n",
      "         -1.4398,  0.8177, -1.8292, -1.8625,  0.1330,  0.4513, -1.1547, -0.9380,\n",
      "          0.4298,  2.2121, -0.0778,  0.1541]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(20418227795939819520., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [7/32 (22%)]\tLoss: 20418227795939819520.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.4283, -0.8992,  0.8751,  0.6483, -1.0007,  0.5942,  1.1628, -1.4118,\n",
      "         -0.1540, -2.4758, -0.8718,  0.1759, -0.2405, -0.0034,  0.3188,  1.0557,\n",
      "         -2.0276,  2.5769, -0.7915, -1.1915]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(1386609781829009408., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [8/32 (25%)]\tLoss: 1386609781829009408.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 1.5864, -0.8126, -1.2407, -0.6544, -0.1743, -1.0748,  0.5380,  0.8861,\n",
      "          0.9677, -0.6077, -1.4524, -0.8981, -1.0173, -1.6999, -0.1665, -1.0904,\n",
      "          0.2005,  1.6716,  0.9010,  0.2573]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(4799747036998008832., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [9/32 (28%)]\tLoss: 4799747036998008832.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.1160, -0.6880, -0.8121,  1.3584,  0.1156, -1.3295, -1.3001,  0.6163,\n",
      "          1.1170, -0.1456, -0.4656, -0.1339, -1.1880, -0.9490,  0.1275,  0.4124,\n",
      "          0.2899,  0.5731, -0.7232, -0.4601]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(40561705228284461056., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [10/32 (31%)]\tLoss: 40561705228284461056.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.1126,  0.0974,  1.6649,  0.7161, -0.4226, -0.5483,  0.9963,  1.0365,\n",
      "         -1.6611,  0.4909,  0.7489,  0.9595, -0.1053, -0.4737,  1.5693, -0.1864,\n",
      "          2.1603, -0.2055,  2.8179, -0.5449]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(9442163477708800., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [11/32 (34%)]\tLoss: 9442163477708800.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.9810,  0.0073,  0.3655,  1.4969, -0.1258, -1.4731, -1.3565,  0.6658,\n",
      "          0.3716,  0.1377,  0.7722, -1.9304, -1.0209,  0.1127, -0.4275, -0.1520,\n",
      "          0.0523, -1.3804,  0.2721, -0.7500]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(10166051525068062720., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [12/32 (38%)]\tLoss: 10166051525068062720.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.7610,  1.7666, -0.9595,  0.9977,  0.2293,  1.0601, -0.3861, -1.3137,\n",
      "         -0.7176, -0.5464, -0.7259, -0.3198, -1.1410,  0.5399,  0.9109,  1.8163,\n",
      "         -0.1166, -1.6806, -1.0177,  0.7409]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(58316856488122908672., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [13/32 (41%)]\tLoss: 58316856488122908672.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.4405,  2.0726, -1.6046,  1.5080,  0.6517,  1.3479,  3.1615, -1.2878,\n",
      "         -0.2332, -0.2778, -1.2467,  0.1305,  1.0294, -0.9793, -1.0528,  0.2500,\n",
      "          0.2186, -0.2377,  1.7061,  2.1607]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(5458832239838101504., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [14/32 (44%)]\tLoss: 5458832239838101504.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.0417, -1.9646, -0.9288,  0.0058,  0.8943, -1.6034,  1.7818,  0.5023,\n",
      "          0.7484, -0.1908,  0.0707, -1.1499, -0.6205, -0.4765, -1.2933, -0.4210,\n",
      "          0.0017,  0.6932,  1.1962, -0.7768]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(1024848.2500, grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [15/32 (47%)]\tLoss: 1024848.250000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.3544,  0.0009, -0.4966, -0.0160, -1.5983,  0.4837, -0.7382,  0.4717,\n",
      "         -0.0369,  0.7811,  0.4465, -0.0147,  1.2225, -0.4880,  1.4330,  0.3332,\n",
      "          0.2035, -0.2161,  0.6199,  0.6298]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(2520466579443941376., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [16/32 (50%)]\tLoss: 2520466579443941376.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.2481, -1.3211, -1.2456, -1.3697,  0.9943,  0.5878,  0.0471, -0.1909,\n",
      "          0.5705,  0.1537,  1.2057, -0.5142,  1.3762, -0.2380, -0.3535, -0.1128,\n",
      "          1.3221, -1.2671, -0.9999, -0.2803]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(13218407953960271872., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [17/32 (53%)]\tLoss: 13218407953960271872.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.3958,  0.2161,  0.8239,  0.3878, -1.8268,  0.1094,  0.9173, -0.1078,\n",
      "          0.9859,  0.1679,  0.2600, -1.3711, -0.3190, -0.8091, -0.2270, -0.0170,\n",
      "         -1.3097,  1.2611,  0.8857,  0.1904]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(30881037509344624640., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [18/32 (56%)]\tLoss: 30881037509344624640.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.2902,  0.8472,  0.0146, -1.6286, -0.0620,  1.0684,  0.0101,  0.7811,\n",
      "          0.7097, -0.3368, -0.9205, -0.2280, -0.6153, -0.3539,  0.1755, -0.1557,\n",
      "         -0.3721, -0.1029,  2.3891, -0.5807]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(56907392532476854272., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [19/32 (59%)]\tLoss: 56907392532476854272.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.7684, -0.2853, -2.8508, -1.4475, -1.1300,  0.8521, -0.7743, -1.0429,\n",
      "         -0.5566,  1.1076, -0.1071,  2.2346, -0.8342,  1.3986, -1.7651, -0.6359,\n",
      "          0.8286,  1.4805, -0.2090,  2.0746]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(1425600938051108864., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [20/32 (62%)]\tLoss: 1425600938051108864.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-2.7214,  1.5092,  0.7486, -0.9273, -0.2688, -0.0682, -1.0955,  0.8388,\n",
      "          1.4766, -0.6864,  0.3530, -1.8730, -0.2051,  1.3411, -2.4971,  0.5767,\n",
      "          0.7021, -0.8761,  0.5593, -1.3416]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(431491278052524032., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [21/32 (66%)]\tLoss: 431491278052524032.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.6291,  1.2273, -0.0649,  0.2293,  0.4523,  1.0724, -0.3997, -0.9142,\n",
      "          0.6857,  1.3785,  1.9379, -0.1694, -0.1393, -0.3982, -0.5689,  1.1984,\n",
      "          1.1381,  1.6965,  0.1367, -0.6222]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(11413887271431045120., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [22/32 (69%)]\tLoss: 11413887271431045120.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.2443,  0.9884, -0.5920,  2.1877,  0.0058, -1.3178,  0.6791, -1.0081,\n",
      "          1.1348, -0.1330, -0.0815, -0.0488, -1.4251, -0.0719, -0.2056, -0.6349,\n",
      "          0.2717, -0.5887, -1.7912,  0.2242]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(15249034406648610816., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [23/32 (72%)]\tLoss: 15249034406648610816.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.1901, -0.9365, -0.4325, -0.2780,  0.1521,  0.9121,  0.7182, -0.4586,\n",
      "          2.5589, -0.6879,  0.7440,  1.3338,  0.8867,  0.2555, -0.5326,  0.8105,\n",
      "         -0.8444, -0.1778,  1.5175,  1.7334]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(681490707193528320., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [24/32 (75%)]\tLoss: 681490707193528320.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 1.4622, -0.5844,  2.0177,  0.0100, -0.1283,  0.8887, -0.7357,  0.2944,\n",
      "          1.1508, -1.5673,  0.0044,  1.9981, -0.1453, -1.0171, -1.1424,  0.7522,\n",
      "          1.9450,  0.4766,  0.9816,  0.7041]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(129342009107569180672., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [25/32 (78%)]\tLoss: 129342009107569180672.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.7109,  0.1180,  1.0140,  0.1170, -0.5673, -2.0610, -0.8950,  0.5232,\n",
      "          0.5509,  0.6165, -0.7170,  0.7243,  0.9413,  0.2103, -0.0270,  1.2323,\n",
      "          1.1058, -0.0452,  1.0016, -1.2249]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(119241552247190978560., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [26/32 (81%)]\tLoss: 119241552247190978560.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.4501, -0.1030,  0.2784,  0.6048,  1.5365,  1.1934,  1.5410, -0.1474,\n",
      "         -0.0377,  0.3868, -0.0452, -0.3051, -0.8082, -0.9308,  0.4494,  1.7314,\n",
      "          0.3381,  1.5753, -0.3481, -0.6279]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(1026129259665555456., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [27/32 (84%)]\tLoss: 1026129259665555456.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 1.2895,  0.5357,  1.4973, -1.0754,  1.4006, -0.4190,  0.7189,  0.5110,\n",
      "         -0.4524,  1.7437,  1.4414, -0.2607, -2.0486, -0.8078, -0.5280,  0.0909,\n",
      "          0.7744, -0.0109,  1.7215,  1.5526]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(6439071595012030464., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [28/32 (88%)]\tLoss: 6439071595012030464.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.0131, -0.2926,  0.0480,  0.5315, -0.3094,  0.2834,  0.5173, -0.9148,\n",
      "          0.4296,  0.5425,  0.9155,  1.3970,  0.2589,  0.3551, -0.3887,  1.5777,\n",
      "          0.5930,  1.1173, -2.2737,  1.3601]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(673125930893312., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [29/32 (91%)]\tLoss: 673125930893312.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.6168,  1.2710, -0.9317, -0.5436,  0.1779,  0.8415, -0.5686,  1.2224,\n",
      "         -0.3013,  2.0830, -1.9769, -0.5663, -0.2387,  0.4118, -0.2158, -0.0256,\n",
      "          0.9485,  1.0333,  2.3767,  0.8797]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(568431053245513728., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [30/32 (94%)]\tLoss: 568431053245513728.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.0534, -0.6318,  0.2079,  0.8301,  0.0928,  0.4500,  0.4431, -0.0127,\n",
      "          0.4744, -0.0103, -0.4722, -0.7520, -0.1076,  1.2577,  0.4247,  0.6639,\n",
      "         -0.7208,  0.9985, -0.8959, -0.6467]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(3682479121067147264., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 3 [31/32 (97%)]\tLoss: 3682479121067147264.000000\n",
      "====> Epoch: 3 Average loss: 29516452548412735488.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6wAAAHdCAYAAAAZ2vQJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xusffl51/fPd932uYwzv3E8xKkvtVXSVC4QE0aGhpTY\nqEodJGpRgYhFuQZZQUlV1Ao1FAkkkKqKSP2DEnCt1hhacFCVpKSqk5C0SYwTchmnJnFCnAy2g8cK\nnjGemXh++7Ju3/6x1nftffbZ67r3WXutc94vyfLMOWd+3j6zL+tZz+f7PMZaKwAAAAAApsY79wMA\nAAAAAOAQClYAAAAAwCRRsAIAAAAAJomCFQAAAAAwSRSsAAAAAIBJomAFAAAAAEzSZAtWY8wHjTEv\nGGM+2eFn/2tjzK8YY37RGPP/GGP+3Z3v/WljzK+X//nTd/uoAQAAAACnYqa6h9UY8wckvSrpH1hr\nf0fLz75L0s9aa5fGmL8g6Z3W2j9ujHmtpGclPSPJSvq4pN9jrX3pjh8+AAAAAOBIk+2wWms/KulL\nu18zxvx7xpgfNsZ83Bjzz4wx/0H5sz9urV2WP/Yzkt5Y/vV/KulHrbVfKovUH5X07pH+LwAAAAAA\njhCc+wH09AFJ326t/XVjzO+V9Hck/cG9n/k2ST9U/vUbJH1u53vPl18DAAAAAEzcbApWY8wTkr5B\n0v9hjHFfXuz9zH+hIv77TeM+OgAAAADAqc2mYFURX37ZWvv2Q980xvwnkv6KpG+y1m7KL39e0jt3\nfuyNkn7iDh8jAAAAAOBEJnuGdZ+19rckfcYY88ckyRS+rvzr3y3pf5b0n1lrX9j5x35E0jcbY54y\nxjwl6ZvLrwEAAAAAJm6yBasx5sOS/rmkrzXGPG+M+TZJf0LStxlj/oWkX5b0nvLHv1vSEyriwp8w\nxvygJFlrvyTpb0j6+fI/f738GgAAAABg4ia71gYAAAAA8LBNtsMKAAAAAHjYKFgBAAAAAJM0ySnB\nr3vd6+xb3vKWcz8MAAAAAMCJffzjH/+itfbpLj87yYL1LW95i5599tlzPwwAAAAAwIkZY36j688S\nCQYAAAAATBIFKwAAAABgkihYAQAAAACTRMEKAAAAAJgkClYAAAAAwCRRsAIAAAAAJomCFQAAAAAw\nSRSsAAAAAIBJomAFAAAAAEwSBSsAAAAAYJIoWAEAAAAAk0TBCgAAAACYJApWAAAAAMAkUbACAAAA\nACaJghUAAAAAMEkUrAAAAACASaJgBQAAAABMUmvBaox5kzHmx40xv2KM+WVjzH914GeMMeZvGWOe\nM8b8ojHm63e+925jzKfK733Xqf8PAMA5/P2f/qze87c/du6HAeAI/+WH/z/99f/rV879MGZlFWf6\nvf/9j+knPvXCuR8KgAeiS4c1lfTfWGvfJun3SfoOY8zb9n7mWyR9Tfmf90n6u5JkjPElfU/5/bdJ\neu+BfxYAZudfvfiqfu0Lr577YQA4wq9/4ct67kVex328skr0hd/a6DNffHzuhwLggWgtWK21v2mt\n/YXyr78s6V9KesPej71H0j+whZ+R9MgY89WS3iHpOWvtp621saTvLX8WAGYtyXIlWX7uhwHgCEmW\nK0l5Hffh3vd4/wMwll5nWI0xb5H0uyX97N633iDpczt//3z5tbqvA8CsxalVmltZa8/9UAAMlGSW\nwqunuCpYee8DMI7OBasx5glJ3yfpL1prf+vUD8QY8z5jzLPGmGdffPHFU//xAHBSac5FGzB3aZYr\nyXkN95GW73kU+gDG0qlgNcaEKorVf2it/f4DP/J5SW/a+fs3ll+r+/ot1toPWGufsdY+8/TTT3d5\nWABwNsTigPmLM0skuCfe+wCMrcuUYCPpf5X0L621/2PNj/2gpD9VTgv+fZJesdb+pqSfl/Q1xpi3\nGmMiSd9a/iwAzJrrrKZ0WIHZSvO8SkugG1eo8t4HYCxBh5/5/ZL+pKRfMsZ8ovzafyfpzZJkrX2/\npI9I+kOSnpO0lPRny++lxpjvlPQjknxJH7TW/vJJ/x8AwBm4i7aYLgMwW0maE+vvyf2+eO8DMJbW\ngtVa+zFJpuVnrKTvqPneR1QUtABwbxCLA+YvyRm61FfKex+AkfWaEgwAKBAJBubNWst6qgFiIsEA\nRkbBCgADEAkG5i3Lraxl0ndfRIIBjI2CFQAGcN0FBrYA85TmrGcZIqXDCmBkFKwAMEB1hjXlog2Y\no5izmIPwewMwNgpWABggJhIMzFrKOfRB3O+LKDWAsVCwAsAA24tdClZgjqp9orlVnlN8dcWEdABj\no2AFgAG2F21c6AJzFKfbgivhLHpnFKwAxkbBCgADuEKVC11gntKdriqx4O5Y6QVgbBSsADDAdugS\nBSswR7sdQrqF3bHSC8DYKFgBYAAiwcC83SxYeR135TrTrPQCMBYKVgAYgD2swLztFql0WLtzZ39Z\n6QVgLBSsANCTtXa71oZIMDBLRIKHYegSgLFRsAJAT7vDWogSAvNEJHgY9/7HwDkAY6FgBYCedqdj\nEgkG5olI8DBEggGMjYIVAHranY5JJBiYp3TndcyKlu7cTTpu1gEYCwUrAPR040I350IXmKPdrior\nWrpznVVu1gEYCwUrAPR0I0rIRRswSzGR4EFY6QVgbBSsANAT00WB+SMSPEzCHlYAI6NgBYCebhSs\nRIKBWeLG0zAuVZJkVtby/gfg7lGwAkBPRIKB+WNK8DC7nVXO8AMYAwUrAPSUMHQJmD32sA7D2V8A\nY6NgBYCemC4KzB/7lIfZPfvLLlYAY6BgBYCeiAQD88c+5WFunuHn9wbg7lGwAkBP7GEF5o9I8DBE\nggGMjYIVAHqKiQQDs0ckeBjWAQEYGwUrAPR040KXghWYpYRI8CCc4QcwNgpWAOjJXbBFvkeUEJip\nJLMKPCOJaH8f6c7vjUgwgDFQsAJAT66rcBn5XLABM5VkuS4jv/hrOqydxVmuq/L3RiQYwBgoWAGg\nJ3eRdkXBCsxWkuW6CP3qr9FNkuW6igJJRIIBjIOCFQB6Sm50WOkwAHOUZFaR7xXRfiLBnaWZpcMK\nYFQUrADQkytYryKfoUvATCVZrtA3CnxDJLiHeDdKzfsfgBFQsAJAT0kVCQ5u7CQEMB9pniv0PYW+\nx9ClHnY7rESCAYyBghUAetrtsNJhAOYpTq0C31PoGwqvHnbPsBIJBjAGClYA6Ml1Y4gEA/OVZLki\n3yj0PSLBHVlrlebbDis37ACMgYIVAHqKy4vbi5ChS8BcEQnuz73fcYYVwJgoWAGgpyTLFXhGi8Aj\nSgjMVJJaBeXQJV7H3ewehyj+nkIfwN2jYAWAntLcbjszXOgCs5SUHdaI13Fn6c7AOYkOK4BxULAC\nQE9xmhedGc+jwwDMVLHWxivW2vA67sR1oi9Dt4eVghXA3aNgBYCe0jxX5HsKA0OHAZipNLMK3dAl\nXsedpPnNSDBrvQCMgYIVAHpK0jIS7HGhC8xVnG2HLvE67iZJy0jwwq214fcG4O61FqzGmA8aY14w\nxnyy5vt/yRjzifI/nzTGZMaY15bf+6wx5pfK7z176gcPAOeQZEUkOPQ95VbKmDAKzE5SFaxEgrty\nkeCrkCnBAMbTpcP6IUnvrvumtfa7rbVvt9a+XdJflvST1tov7fzIu8rvP3PcQwWAaUhyW0WCJS7a\ngDnajQTTKezGRYIviQQDGFFrwWqt/aikL7X9XOm9kj581CMCgIlLyqFLoVe8hVKwAvNTJCU8BZ5H\n4dWRiwS7zjSFPoAxnOwMqzHmSkUn9vt2vmwl/Zgx5uPGmPed6n8LAM4pzbdRQmm76gHAfCRZkZSI\nAgqvrpKywxpWU9L5vQG4e8EJ/6w/LOmn9uLA32it/bwx5rdJ+lFjzK+WHdtbyoL2fZL05je/+YQP\nCwBOK87KoUsBHVZgroozrEwJ7iNJi99TxNlfACM65ZTgb9VeHNha+/nyv1+Q9AOS3lH3D1trP2Ct\nfcZa+8zTTz99wocFAKeVpOWFbhkJjrnYBWZnNxJM4dWN+z0FTFcGMKKTFKzGmCclfZOkf7LztWtj\nzGvcX0v6ZkkHJw0DwJxUkeCASDAwR9ZaJWVSImKfcme7kWAKVgBjaY0EG2M+LOmdkl5njHle0l+T\nFEqStfb95Y/9EUn/1Fr7eOcf/SpJP2CMcf87/8ha+8One+gAcB5xZnUZFZ0ZiUgwMDdpuYoq9DiL\n2YeLBIe+p8A33KwDMIrWgtVa+94OP/MhFetvdr/2aUlfN/SBAcBUJWmuqOwwSCJOCMyMK7TCwCvX\n2vAa7qIq9H1Pke9xHALAKE55hhUAHgQXCY7YwwrMkiu0As8oDAyFV0fuvW67v5ZCH8Ddo2AFgJ6S\nzFbDWoq/52IXmBP3mo0CTyGR4M7ivUgwvzcAY6BgBYCedtdhFH9PlwGYkyoSXE67za2U5byO2+xG\ngkPfU8LvDMAIKFgBoKckyxV6xR5C9/cA5iPZiQQHvI47q35vvin2sKb8zgDcPQpWAOgpyazCYNth\nTXMu2oA52Y0ER9XrmG5hm2SvM02RD2AMFKwA0FMRCfaqgjVOudAF5sQVXoHnbTusdAtbVYU+kWAA\nI6JgBYCetgUrUUJgjvan3e5+DfVcUU8kGMCYKFgBoKc0szcudIkEA/NSFaw7kWC6he3c7yjwyrU2\nvPcBGAEFKwD0kOdWaW6rtQ6SlBAJBmalOotJJLgXNyHdGKPA95iQDmAUFKwA0EOSb/cQus5MTJQQ\nmJX0QCSYbmG7tDwOIRW/u5giH8AIKFgBoIft/sadC10KVmBW4mo9y/YsOsPT2iWZrd73IiLBAEZC\nwQoAPWz3N+5ECYnFAbPibjxFO9O+Kb7auUiwVAxe4r0PwBgoWAGgh+rsW7C90E240AVmZTt0iSnB\nfSQ3IsHsYQUwDgpWAOhhu4dw50KXKCEwK/GBpASR4HZJZqvfFwUrgLFQsAJAD7uRYN8z8gydGWBu\ndiPBEZHgzpK9oUtEggGMgYIVAHrYjQRLZZeBC11gVqobT36xnmX3a6iXZLlCb/vel+VWOftrAdwx\nClYA6KE6++btxOKIEgKzkuRu2vd2SjDdwnZpZhUG2/c+iTP8AO4eBSsA9LBda7ONxRElBOYlSbd7\nWCM6rJ3Fe5FgafueCAB3hYIVAHqIq+miTMoE5qpKSvgekeAediPBgcfvDcA4KFgBoIdDkWCmiwLz\nkhIJHuRGJLi8aRdTsAK4YxSsANBDemvoEpFgYG7inUgwe1i7S7K86qxGRIIBjISCFQB62K61KS7W\nAiLBwOykea7AMzJmW7BSeLVLMlv9vogEAxgLBSsA9BDvnH1z/02UEJiXJLMKfBfrd5FgCq82SZYr\n2osE8/4H4K5RsAJAD64LEwXbWBwXusC8xGl+46aTxFnMLnYjwe4cP+9/AO4aBSsA9EAkGJi/NM+r\ndTZEgrvbjQRz9hfAWChYAaCH5FYk2BCJA2YmSbeRYN8zMobCq4sky6sINZFgAGOhYAWAHpK9SDB7\nWIH5SfJtJFjiLHpXab7TYSUSDGAkFKwA0MN+JDj0PaKEwMzsRlslKeLGUyfJ7tnfgCg1gHFQsAJA\nD1UkONiNBHOhC8xJUXiZ6u8DXsedxDuR4IAOK4CRULACQA8uNhiWkzID32O6KDAzKZHgQW5Egpmu\nDGAkFKwA0ENaDV0qugsRkWBgduLMKtgtWD06rG3y3CrLt8OqIiLBAEZCwQoAPSRZLmOKyaJSEYvj\nQheYlzTLFe1EgsPAq25G4bAkvzkhnUgwgLFQsAJAD3FmFXqejNmudiBKCMxLkuUKvO0lUHHjiddx\nk2pCOntYAYyMghUAekizm8NamC4KzE+S2WpwmsR6qi5cB9pFgrcFK4U+gLtFwQoAPSRZfuPsG5Fg\nYH6SvUhwFFCwtomzm5Fgd+OO3xuAu0bBCgA9JPnN/Y3F2Tc6DMCcEAnur5qQ7tbaEAkGMBIKVgDo\nIUn3hrV4RnGWy1oudoG5SIkE95budVgjIsEARkLBCgA97EeC3cVbmnPRBsxFnOUKvZ0bTxSsrZKa\nSDDTlQHcNQpWAOihiATfXIchsYsQmJM024v2+4abTi32I8E+a20AjISCFQB6SNL8xoWu20UYc9EG\nzEaRlLjZYY1TXsNN9jusxhhFvqeYm3UA7lhrwWqM+aAx5gVjzCdrvv9OY8wrxphPlP/5qzvfe7cx\n5lPGmOeMMd91ygcOAOeQZDcL1qjqsHKxC8xFvPc6JhLcLqnW2uzcsPMN730A7lyXDuuHJL275Wf+\nmbX27eV//rokGWN8Sd8j6VskvU3Se40xbzvmwQLAuaX7kWAGjwCzk2a2utkkEQnuYj8SXPw1hT6A\nu9dasFprPyrpSwP+7HdIes5a+2lrbSzpeyW9Z8CfAwCTEae397BKnOMC5qRYa7MtvALfU0IkuNF+\nJNj9dUKhD+COneoM6zcYY37RGPNDxpj/sPzaGyR9budnni+/BgCzlea2WucgbSPBFKzAPFhry6QE\nhVcfadVhvdmZptAHcNdOUbD+gqQ3W2t/l6T/SdL/OeQPMca8zxjzrDHm2RdffPEEDwsATq84w7rT\nmfGIBANzcijaGvmGm04t4qrDSiQYwLiOLlittb9lrX21/OuPSAqNMa+T9HlJb9r50TeWX6v7cz5g\nrX3GWvvM008/fezDAoA7sR8JdhdvXLQB83Ao2kokuN3h35uhMw3gzh1dsBpjXm+MMeVfv6P8M/+t\npJ+X9DXGmLcaYyJJ3yrpB4/93wOAc9qPBIdEgoFZORxtJRLc5tDvLaLQBzCCoO0HjDEflvROSa8z\nxjwv6a9JCiXJWvt+SX9U0l8wxqSSVpK+1VprJaXGmO+U9COSfEkftNb+8p38vwCAkdza30gkGJiV\nw9FWIsFt3O9td1hV6HtMVwZw51oLVmvte1u+/7cl/e2a731E0keGPTQAmJ402x/WYsqvc7ELzEGa\nH552a62U5Vb+TkGGLddh3V0HFFDoAxjBqaYEA8CDEGf5rbNv7usApi9Ji8Ir2CtYJaL9TerW2sRE\nggHcMQpWAOhhf0qwO8+aEgkGZqEuErz7PdzmCtZg7/dGJBjAXaNgBYAebkWCA6YEA3PiIsHRgQ4r\nN57quXP6+7833vsA3DUKVgDoId4buuT2sNKZAebhUCQ4YD1Vq6Rm6BID5wDcNQpWPCivrBL9nZ94\nTjkRJgyUZvmNDgOR4K1//W+X+oc/+xvnfhhAoyQ/FAnmDGubNMtljG4MpTpmuvKP/coX9Oxnv3Sq\nhwfgHqNgxYPy//7qF/Q3f/hTeu7FV8/9UDBDWW6VW+0NXaIz43zfLzyvv/IDn9Qmzc79UIBabm/o\n/j5RifVUTeLyOIQxe2ttBr73ffePfErv/8lPn+rhAbjHKFjxoDzeZOV/p2d+JJijw0NHygtduvbV\n64qpoZgyNyTo0I0n1lPVS7Nc4d7Kn8AbHglep5nWCTe3ALSjYMWDsoqLD8dlzIck+nMF66FIcEKR\npsfl62rD7wITFjfceOIser0kyxUGNy8bo8AM/p1tklwrClYAHVCw4kFxhSodVgzhOgm7Q0eIBG8t\n4+J1RcGKKXM3l25Ou3WvY5ISdeLMVkPmnGMiwRs6rAA6omDFg+Lu5tJhxRDuwmy3y1CtwyASXEXu\nN1yEYsIORYK3a2242VKnGDh3ukjwJs0pWAF0QsGKB2VVdoAex3RY0Z+LvoUHOjOc29x2WIlVYsoO\nnUVnPVW7Q5HgMBg+JbgoWPl9A2hHwYoHxXVWlxvu6qI/10nYXYdhjFHgDb9ou0+qM6xchGLC3Ov4\nxln0wA1dIilRJ8ntjeMQkhR63qD3vjTLleWWieIAOqFgxYOyLONHdFgxRHqgw+r+nkiwtNxwhhXT\n1zjtmxtPtZI0P/jel9ti5VcfrpNNhxVAFxSseFCYEoxjHIoES8WFL5Hg7euKrgmmLDnwOnaRYArW\nekl2oGANhg2dcykMzrAC6IKCFQ/KiinBOMKhSLBURAvTnAtdl1wgEowp276Ob0eCmRJcL83trfe+\ncGCh71IYaW4ZdAWgFQUrHpQlU4JxhKZIcJJyoevOhjO4BlO27bDeHrpEh7VefDASPOzs724KY006\nBUALClY8KNWUYDqsGMAVYvu7CAOfoUtxmle/HyLBmLJDN57c9FuGLtUrOqz7733DCv3dIxTEggG0\noWDFg7LkDCuO4C5mXXzQiXxPyQMfurTaeU0RCcaUxeXreHfibbWe6oHfeGpSnGG9/d4n9f+9bShY\nAfRAwYoHpTrDypRgDHBoWItUdlgfeKxt9zXFlGBMmSu8jNkpWIkEt4rTvOqoOuHAdUA3IsHc4ALQ\ngoIVDwp7WHGMpCYSHDJ0ScsbBSuvL0xXenDaLZHgNmlub+yulYaf/d1NYdBhBdCGghUPhrVWK/aw\n4ghJTSQ49L0qZvhQPd65CcSKH0xZktkbcWBpGw8mElwvyfIbu2ul3f21PTusO79nbnABaEPBigdj\nN3bEGVYMUd9hJRJMJBhzkWS5ouB2SkKiw9okzW4PXXJnWo/rsPJ+AaAZBSseDBdZvAg9pgRjEHcx\nGx642H3wkeCdDisFK6YsyfJbN518z8j3mPbdJD4UpR44JfjmGVZuIANoRsGKB8N1VV/3xEKbNGdZ\nOXqLD+xvlIrVDg8+ErzbYeUCFBOWZLYaFrQroGBtdGhKcFB1WPsOXdr+nle8XwBoQcGKB8N9KL7u\niYUkacmHJHqqpgTvdWci3zz4GyC7MXvOAWLKkgOdQqlcT/XAbzw1ORQJjgZ3WIkEA+iOghUPxire\nK1iZFIyemiLBD70z42L2X3ERsIcVk5Zk+a2bTlK5nuqBv46bxA1Dl/oeiYjZwwqgBwpWPBiuA/T0\na6Ly7znHin7iaujS7UjwQ+/MuBtCT11HnGHFpKU1kWDOojdLs/z2WpuygI3TY/awUrACaEbBigdj\nlRQF6ldelx1WJgWjp6rDemBS5kPvzDyOM0WBp+soYE0FJi0+MHRJKtdT9Sy8Hoost8rt7fe+wZHg\nZHetzcN+7wTQjoIVD8Z26FLRYWVSMPpKsryaJror9IgEL+NU15GvRehxAYpJSzN7q1MoFTee6LAe\nVq30OjBwTuofCd6kRbfWGDqsANoF534AwFiqgvU1dFgxTLEO40CUMDAPfn/j402mqyjQIqBgxbQd\n2sMqcRa9ifu97Bf61R7WAZHgRejJyyhYAbSjw4oHY703JfgxZ1jRU1LbmfEe/GTcZZzqeuErCnwK\nVkxa3ZTggEhwLXdGf/+GXRUJHjB0aRF4ugh9pgQDaEWHFQ/GkinBOFKS5bcmBEt0ZqTiDGvVYaVj\ngglLDqxnkcr1VESCD3Jru/bf/1wkOOl5k2qT5loEvoLc0mEF0IqCFQ+GK1hfe12eYaXDip5qI8E+\nkeDlpuiwLgLvxsoKYGqKDuvhKcEP/cZTHZcgOTRwTlLvKembssOaW6s17xcAWhAJxoOxilNdhr6e\nWBT3aTjDir7qOjOB5ynNrfL84Rat2w4rkWBMW5rXvI598+DXU9XZTkg/vIe1byR4kxRTxYtIMJ/F\nAJpRsPb0mS8+1rOf/dK5HwYGWMaZriJfUeAp9A1TgtFbXWfGDXDpe9FmrdXnvrQ8yWM7NzclOGLo\nEiYuTvNb024lOqxNktoOq4sED+iwhj4FKybj5WWsV5bJuR8GalCw9vSBj35a3/6//8K5HwYGWMWZ\nLiNfknQVBXRY0VuaHx7W4orYvrHgj//GS/qP/+aP61+9+OpJHt85Pd5kulq4KcG8tjBdSZbXDk+j\nYD3MRYL399f6npFn+q+1idNcC9/TRejd2MkKnMtf/Mef0J/84M/KWlIWU0TB2tNFyMXYXK2SosMq\nSdeRT4cVvcVpfSRYUu+L3Re+vJEkfeGV9fEP7szYw4q5qIsEcxa9nvu9RMHtznQwYEq6W2tzEfpa\nc02FCfjiqxv94vOv6Oc+Q4pyiihYe7oIfe4GztQyznQZFedXrxZ0WNFf7bAWFwnuebHronCvzvzm\nSZ7bMnJfnGGN05y71JispCYSPKTweijqIsFSsdpmUCQ48HQREAnGNLj1Sn/vpz573geCgyhYe7oI\nfMVZruwBD1eZq1Wc6TIsnvLXkc+UYPRWFwmOqkmZ/Vc7SPMfALYqLzjdlGBJXPhjspL8cCQ48j06\nrDW2e1gPD6vqGwl2a20uQo89rJgEl578p7/yb+7NbIn7hIK1p0VZ8LC2YX6WSaor12GNAvaworck\ntYc7MwMjwfelw+pu/rg9rJKIBWOykuzw6zj0DWdYa7jfy6FI8JCzv5s0KzqsDF3CRKyTXO/62qdl\njNHf/+nPnvvhYE9rwWqM+aAx5gVjzCdrvv8njDG/aIz5JWPMTxtjvm7ne58tv/4JY8yzp3zg53JR\nXozxBjs/y52hS9cLOqzoL6kbujQ4Elxc5M39PLW7+bPbYeXoBKYoz62y2rU2DF2qk9QMXZLKSHDP\n9744zau1NiuupzAB6yTTW153rW/5Ha/XP372c7O/kXzfdOmwfkjSuxu+/xlJ32St/Z2S/oakD+x9\n/13W2rdba58Z9hCn5SIsCh6GBMzPOs50FTIlGMPVThf1hkWC3Y2vuResNzusxWuM4XSYIrd6qvYs\nJpHgg5JqD2vd/tohkWCvGNLGzS1MwCYpYup/7hvfqi+vU33fx58/90PCjtaC1Vr7UUm1I7OstT9t\nrX2p/NufkfTGEz22SXKRYM5czM9yd0rwginB6K8uEuwu4vqef3Ox2cczv3nibv5cR0H1HkkkGFO0\nLbwORfuJBNc5eSQ4KfewMhcEE5DnVnGW6yL09PVvfkpf96ZH+tBPf1Y5z8vJOPUZ1m+T9EM7f28l\n/Zgx5uPGmPed+H/rLC7oHszWMs50wR5WHKEtEtx30NC96bCWj/9qd+gSBSsmKG2YdhsGDF2q44Yq\nHRy65JlenWlr7Y0zrBLXVDgvd4PVPR//3O9/iz7zxcf6iV974ZwPCztOVrAaY96lomD9b3e+/I3W\n2rdL+hZJ32GM+QMN//z7jDHPGmOeffHFF0/1sE6uigTTYZ2VLLeK01xXYTF0yU0JZvUG+jh1JNhd\npM39rMwcRkTxAAAgAElEQVRuhzVi6BImzN1UCg7uYS3W2vC5cJtbW+Nuzu2Kgn4d1jS3yq3KgpXU\nGs7P3Tx2c2r+0O/8an3VVyz0wY999oyPCrtOUrAaY36XpP9F0nustf/Wfd1a+/nyv1+Q9AOS3lH3\nZ1hrP2CtfcZa+8zTTz99iod1J7aRYO4GzsmyOmNXdlgXgazlQxL9pHXTRYNhkWD3/Jt7t7/qsEb+\n9gwr75GYIPcajQ69jssbTykxwFtcoe9+R7vCnuuAXPoi2umwck2Fc3JzadzzMfQ9/an/6C362HNf\n1K994cvnfGgoHV2wGmPeLOn7Jf1Ja+2v7Xz92hjzGvfXkr5Z0sFJw3PiLsZ4c52XVVkQXFaR4OK/\nmRSMPpLscCQ4OHLo0r3psC5Ya4Npa5p2O/TG00PQFKUOPNPrOIR7b3B7WCWuqXBe7uaxa0pJ0nvf\n8WYtAk9/76c+c66HhR1d1tp8WNI/l/S1xpjnjTHfZoz5dmPMt5c/8lclfaWkv7O3vuarJH3MGPMv\nJP2cpP/bWvvDd/D/YVQXDBSZJTc2/2rnDKskdrGilzitOcPqD9vDWg1dmnnB+ji+3WHlDCumyL1G\nD0Vb3Y2nvmfRHwJ3RvVQwqRvJNgdhVgEXjUXhLQTzsk9J93zUZJeex3pP//6N+j7f+Hz+tLj+FwP\nDaWg7Qeste9t+f6fl/TnD3z905K+7vY/MW/EV+bJdYAuy39/13RYMUCa24PTRaPBe1iL5+XcI8HL\nTSbPFBegnGHFlCUNkeCo6rDy3N3XtA4o8EyvrvRmp5vFqkBMgbth4p6Pzp/5hrfqwz/3OX345/61\nvuNdv/0cDw2lU08JvveqiXbcDZyV5X4keFF2WClY0QOR4MMex6muo0DGmJ1IMBegmJ7GSLA/7MbT\nQ1ANXapJmPTrsO5GgmkC4Pzc8283EixJX/v61+gbf/vr9L/9899g5dWZUbD2xMXYPLkzrC4KXHVY\niQSjI2utkszWTheVhhSs9yMSvNxkuloUryn2sGLKqj2sDZFgLkxvS/NcnpH8Q0OXekaCq6FL/nZK\nME0AnFNVsAb+re/9uW98i/7Nb631Q5/8N2M/LOygYO2JtTbzdGtKcESHFf24yaEHp4sO7My4G1/L\nOJv1gnLXYZXElGBMWtIw7XYb7efzfV9cky6Rit9ln/e+6gxryJRgTMN2D+vt5/g7//3fpre+7lof\n/BjDl86JgrUnt6OJN9d5cUOXXCT4ekGHFf0kjfsb3TqMYR1WSVrO+D1lGe90WMv3SAbXYIqahy4R\nCa6TpLa+YPW9Xud+D0aCSa3hjKo9rOHtDqvnGf2Zb3iLPvG5l/UL//qlsR8aShSsPQW+J98zvLnO\nzDYSTIcVw1RRwkMFqyvSesZgN2kxrEiadyz48SatXlPV0CVSKJigtOl17BMJrpPm+cGBc1JxXRQP\n6bAG3s5aG37nOJ9NzdAl54/+njfqNReB/t5PfXbER4VdFKwDXAQeF2Mzc2tKsOuw3tF01ldWiX7i\nUy/cyZ+N83AXsQcjwQM7M+sk12uvI0nzHry0jLPqXHjgGXmGM6w4znMvvKpPfv6Vk/+5cTV06fBZ\nTImC9ZC6gXNS8Z7Ya+jS7pRgdttjAtY7N1EOuV4E+uPPvEkf+aXf1G++shrzoaFEwTrARejTYZ2Z\n/UjwReDLGGl5R0XC9338ef3ZD/28vrxO7uTPx/g6RYJ772HNqoJ1zjuBl3FaTd4uJgX7DKbDUf6H\nH/pV/eXv/6WT/7muwxoduDB1N57SGZ8nvytJdrpIcJztDl1iLgjOr63DKkl/7Jk3KcutPvbrXxzr\nYWEHBesAF6HPm+vMLONUvmcUlR+4nmd0Ffp3tv/yy+tU1s5/vya2mqKE/oDpolleTB3+yuuFpPvT\nYZWKzgkdVhzjy+tEX3x1c/I/N2nqsLpIMM/dW4oOa30kuNfQparD6lcdrRUdVpxRdYa1psMqqbq5\nTBrgPChYB1gEXIzNzSrOdRX6Mmb7gXu1CO4sErxM0vJ/lze2+8J1BQ5dtBlT3Azpc47Lfeh95RPF\nh+B9OcMqFe+Rfc/zArvWaa6Xl6dPqGxfxweGLvkMDKuTZPnBdIlURoLzXNZ2e//bPcPqeUZR4DFV\nHGe1TjMFnql9jkvbhB43V86DgnWARehzh2VmVklavdk415F/Z0OX1mWhyhvb/dHUYS2+bnrF4tx7\nyOueKDqsj2c6AMxaW3RYF9vXV8RNPRxpHWdaJdnJP2ubIsEugZMyJfiWpkhw4HuytkiNdLGdElz8\neReBxzUVzmqd5LXnVx3XfV3FfLadAwXrABchb65zs4yzWwXrVRTc2VobV6jyPLk/kobOjORicT0K\n1vKizcWM5rpiKc5ypbnd67ByhhXHce+hp+6yNkaCA6YE10my/ODAOan/HmpXsLqbBhyzwrlt0qzx\n/KpUfMaHPltCzoWCdYAFU4JnZxln1YRg53pxdx3WJR3We6eaLtpw0Zb0GNbiInDbgnWeHVY3LOrG\nGVbeI3Ekd7Pv5VV80j+30x5Whi7dkma2Ni5Znf3tuIe6Klj9nYKVIgBntE7y1oJVKgZ2ctTrPChY\nB7gI6R7MzSrOqh2szlV0d2dY3cUWF+33RxUlbDrH1SMG6zoKc19r46LMbkqwVJ5hpUuFI7ibfS89\nPnWHtYz2e/WRYIYu3RY3DF0Ke/7eNmmmReBVMyVIreHc1kmmRdheEl1EXP+fCwXrABcB8ZW5WcY3\nh8JIZYf1jooEd7FFh/X+OH0keLtq6eoOz1PfNZcmuN55fUV0WHEkV8C8clcd1gPFl0tPEAm+rWkP\nq/t613VAm73zgkSCcW7FGdYOHdbQo8N6JhSsA1yEHvGVmVkl+cEzrHe1dsb9udw1vj+S1kiw6RUl\n3I7R93UVBXp1pmdYXZT5arEbCeYuNIZLs7zqhL504jOsaZbLmO0qql1Vp5BI8C1p49Cl4nfZdTL4\nJs212IlfFk0A3i9wPsUZ1vaS6JKbK2dDwTrAIvDpHszMKk5vn2GN/DubzLriDOu9k7REgkPf6xUl\nrCZlhp6eWPjzPcN6oMPK6i8cY73z3Dn10KU4swo978aKM4c9rPWa9rBWUeqOnek4zW+8jy5C78a/\nc2BsmyTXRacOq8913ZlQsA5Ah3V+lofOsC6CamDMqbm7xURH7o/2DqvXORInbYcuXQS+rhfBbCPB\nVYd1d+hS6FOwYrDd982Xl6eNBKcdzmKmHYcHPSRNe1jde2LnSHB687zgZeizhxVnte7YYb1greXZ\nULAOwBN2flYH1tpcR77iLO8cY+rDdZ24aL8/2s6whr7pd4a1TGlchJ6uo2C2Q5eqDuv+0CWe+xho\n9/P1Ltba1E+77bee5SFJMtuYLpF6RoJ3ullcU+Hc1kn7WhvJRYJ5rp4DBesAi8DTOsllLR9qc2Ct\n1TI5PCVYupsu6IoO673TFgkO/H5FWnWGNfR1vfBnu4fVxep319pEgccZVgy2e0H40ok7rElefxYz\nZOhSrTTLD+6ulbbvid07rPtDlzzOBeKs1nuDwOrwXD0fCtYB3LAA1jbMQ5JZZbk9OCVY0p2cY3UX\nXNyJuz/Slkhw1DcS7M6wBp6uF8F8z7CWhfb+WhvO+WOo3TNiL69O3GFN6yPBxhgFXr+kxEMRZ/bg\n7lqp/3TlTZLdnhLMDS6cUTF0qVuHlTOs50HBOoB7UnOXZR5cl3N/6JIrYE99djDZmXDJG9v90b7W\npm8keNthfWIR3NkAsLvmHvfu66uYEsz7I4Zxn62vWQQnP8PatJ5Fcq9j0lP7kixXWNNh7buHNc5y\nRbfW2vBZifNZJ3mngpXn6vlQsA7gDmYzJGAelkl5Qb1/htV1WE8cxdwtUrmpcX/E5UVs6DVMCe5x\nobs9w1qstZlrJHgZZ7oM/RtrQhaBpzjLlbMeBAO499DXP3lx+jOsua3tsErudcz79r60cQ9r2WHt\ntYd1d60Nx6xwXuvk5iCwOkwJPh8K1gHcGy3FyDy4oTB1Z1hP3dnaPbfKnbj7w0WCw6A+Etyrw5pm\nCn0j3zPFWps4neUF2+NNWt38cdwHP8cmMMR6r2A95euiiATXX/r0fR0/FElDJLhvh3V/SrA7ZkUq\nA+dgrb01CKwOHdbzoWAdoOqwcuZiFuoiwW5v5KlX2+wWrNyJuz9OHQne7TJcLwJZO8/nS7Ey6ub5\ncDeEhQtQDOEuCP+dJy8VZ/lJXxdpw9AlqXgdp0SCb7DWKsnbI8Fd1wHdHrpUFqw0AXAG7nOqy1qb\ny9BXktnqBjbGQ8E6wAUd1lnZdlhvXlRf3dHQpZuR4PkVIDjMRYLrJmWGvtfrQnd375sbWDTH1TaP\nN+mt9MK2Y8LzH/25m36vf/JCkvTSCWPBxVqb5kgwyYCbstzK2uaVXtL2PbLN7bU2xZ87xxt2mD93\no+SiU4e1eK6uuRk7OgrWARbVE5Y31zlwH4K397C6oUun/fe4pMN6LxVnuIyMqStYTa8L3XWSVRdt\nT9zReeoxLOPsxg5WSVX3hI4JhnA3+r66LFhPOXgpbokE9z2L/hC430fr/tquQ5f2O6xVE2B+73+Y\nP3ct32lKcMRz9VwoWAcgvjIvqwNTTKXtG8+p14m4N7InL0O68PdIkuUKagYuSa7D2i8S7O7Wupsn\nc1xt8zg+0GENOMOK4Vbl++brq4L1dB3WNLe1u5Sl4sYTcb+bktwdhzh8sy7oHQm+vdZGogmA83DX\nbJ32sJY3V1YnbnSgHQXrANwNnJf6oUv+je+finsje+oq5DlyjyRZl+mifaYEb/e+uQ7lHAvW5Sar\nCm6HDiuOsTslWDptwdolEszQpZtc5zSqHbrUPRK8HXCzW7CWqTXeL3AG2zOsHSLBEcddzoWCdYAL\nIsGzUlewhr6nKPBOfoZ1WV5sPXUdUbDeI8ne7sB9Qc9I8O5FW1WwznAX6+EOKx/qGG6TFOe7n7qK\nJEkvnTASXNx4anodEwnel+bu/P7h35vrWHfpTCdZcR52Ee6eYaUJgPPZ7kTvNnRJklYxN1fGRsE6\nAGtt5qWaEhzdvnt2HfknnxK8Lv/3vvI64gzrPdIWCY56RoJ3O6zuDOurMz3DerW/1sZ1WBlMgQFW\nSbHb98nLUJL0yuq0HdampETUc9r3QxCn3SLBXX5v7ibW4Q7r/N7/MH+7O9Hb0LA6HwrWAVhrMy+u\naNyfEuy+dldTgh9d0WG9T9LM1u5glYruQ26LiZpdFFOCb0aClzOMBD/epLcjwSEFK4ZbxcVr4yL0\ndRn6eunxKTuszUOXAo9I8L62lV6ukO3SmY4PxItpAuCc+pxh3XZYubYbGwXrAIuQN9c5WcaZosCT\nf2AdyfXi9B1WF0F+7XWkdZIr71jAYNrilgtdV8x2vdhd7wxdcjdT5rbWJs1ybdL81s0gdwEaU7Bi\ngHWaVxeGT12FevmEHda0JRIcBkSC97lIcG3B6vXpsBY/c3APK00AnME2Etylw0p8/VwoWAcgvjIv\nqzi9NSHYucsOq4uz0WW6H9LMVhdmh0Q9YnGSm5RZdliridXzek9x57Wv9yLBUUAKBcO5DqtUJFVO\nutaGSHBvbZFgzzPyvW6/t23Bun3PYFUIzmk7dKnDlGDXYeW5OjoK1gEi35MxFCJzsYyzW0NhnOuF\nf/Ipwevy/NUVH8L3SpLlLZHg7rE46WaHNfA9XYSeljMbuuTSCbc7rEwJxnDrcuiSJD26Ck+71qZD\nJDilw3pDW4e1+J7p9Hs7eIY1YEowzmcbCe5+hpXPtvFRsA5gjNEi8LShEJmFZZIdHLgklR3WE8cw\nl3Gqy8jnTtw9E7ftYQ26T8qUig/J3Q/I6yiYXSTYpRP2O6zbKcF8qKO/9c579lNX0cmnBLe9jumw\n3tR2hlUqYsFdpqS7C/1FeDsSzM1dnMO6x1qbS67rzoaCdaCL0OfNdSbWTR3W6PQd1lVcnL+65EP4\nXkkzW8V+D3EXc11X22yS/MYH5PXi9DdP7toqbumwEgnGAG5KsCQ9eRWedEpw3JKUCL1+66keAreH\ntXF/bcdC3/1uI//QWht+7xifaz4tiARPGgXrQIvA4811JpZxpqvw9oRgSbpaBCePYbruAG9s90tb\nJLjPpMw8t4qz/EYs7noRzG6tjSuwr/f3sJYf/AxdwhCrJKuGGz5VRoKtPU1MN83yxrPooU8keF/S\nIRIceB0jwQc6rL5nFPqGVSE4i+oMa6dIMI2Ic6FgHegi9OkezMQyyXQxYod1WQ552g7n4qL9Pkjy\nliih3z0SvDkQQSqei/PqsLrXztXi5g0h14kmEowhNsl2SvCjy0hpbk8Sl89yq9y2RFsDhi7tcx3W\ntoRJp0jwgTOsUlEsUATgHNZJJs/UDxXb5XtGke/RiDgDCtaBijdXPtTmYBWnumqYEryMs5OunlmV\nHVYiwfdLkrYPa5G6RYK3Y/RvdljnFgmuzrDu3RAK/GKNFDf1MMRuJPjRVTFt/RSDl6qzmC37lClY\nb0rz9khwFHTrTB+aEiwV6wK5psI5FEPefBnTXrBKxec2Q5fGR8E60EXoEV+ZibYpwdJpY7ursjtQ\nRYJZMH0vJG3rMMqL4C4Xbe69Y7fD+sRifkOXqinBi9uR+2IwHR/q6K9Ya+OmBEeSTlywNq2nYg/r\nLXHWLRLcba1NTYc1ZJAlzmOd5Leej00uQp/rujNo/TdkjPmgMeYFY8wna75vjDF/yxjznDHmF40x\nX7/zvXcbYz5Vfu+7TvnAz20R+FyMzcQqbp4SLOmku1jd3tdqtxw3Nu6FNLctax2672F1nYTdDuvV\nHcTT71pdh1UqLkgZXoO+rLVap9sO61Nlh/UUk4LTqvBqXk/lOooopNWU4KYz/B2HLrl48a2C1Sdm\nibPYpFmnCcHOZeRzXXcGXW4pfEjSuxu+/y2Svqb8z/sk/V1JMsb4kr6n/P7bJL3XGPO2Yx7slCzo\nsM7GKmnvsC5POOzG/e+5A/zcibsf4hNGgrddhptTgmfXYa2ZEiwVF6Tc1ENfmzSXtarmDlSR4BNM\nCnYFVdBy4ynJ7MmGPN0Hndba+KZTZ3obCb7dYeX4DM5hvTexv80lHdazaC1YrbUflfSlhh95j6R/\nYAs/I+mRMearJb1D0nPW2k9ba2NJ31v+7L1wwXmLWbDW3jgPte9uOqy5LiJfF1E5dInBM/fCSSPB\nBzqsT5RnWOd0ofx4kyr0za1uiVSmULiph56q892BK1hdJPj4Dut2pUpz4SV1m/b9UHSJBHftsG6n\nBN/8TGYuCM6l2InePRK8CH2u687gFGdY3yDpczt//3z5tbqv3wvF+SwuxqZunRR36y8PdIAk6br8\n+imjmFUk2A1d4k7cvdAWCXYd1m6R4JsX5VLRYc3tvCbrFufDD7+2FoE3q/8vmAZXtLgjFU9enm7o\nUhUJblxPVU77JhZc6RoJ7jZ0qe4MKzFLnMc6zW/dQGlyGXpc153BZIYuGWPeZ4x51hjz7Isvvnju\nh9OqWGvDB9rUuTUhdZHgqzISfKrprK6je7Wzh5WY0/2QpHnjlMztGdYuHVa3qHy3YC3+ek6x4Meb\n9OD5Vak4NsF7JPpy5xjdDb/Q9/SaRXCSM6xVJLjDeqokpcPqdIkEB77peBwil2eKs8K7ikgw7xcY\n3zrJdNFz6BI3V8Z3ioL185LetPP3byy/Vvf1g6y1H7DWPmOtfebpp58+wcO6W5y3mAfXOa0bunTq\nDmuc5cpt8YYW+p4CzzBI4p5I8rwxSugiwd0mZd4+x+Wei3NabbOMs4MTgqUiEhxTsKIndzZsNy7/\n5FV4oinBXaKt5euYDmvF/d4a19r0GLoUBd6tFSKL0Ce1hrPYpJxhnYNTFKw/KOlPldOCf5+kV6y1\nvynp5yV9jTHmrcaYSNK3lj97L7Dkeh7cv6PaDmt02g6rexNz3QEmH94fSXYHkeDwZiRYmlmHNa7v\nsEa+xxlW9HZo5dNTV9FJzrAmHaOtuz+LbuuAAt903sO6v4NV4poK57NJshs3yNrQYT2Pw7fGdxhj\nPizpnZJeZ4x5XtJfkxRKkrX2/ZI+IukPSXpO0lLSny2/lxpjvlPSj0jyJX3QWvvLd/D/4SyKKcF8\noE3ddopp3ZTg03ZYV3sFMsO57oc8t8py2xwJLrulnS7aDgxdqiZWz+jO7XLTcIY19PT48XyKb0zD\nOr5dsD66CvXSKfewNkZbiQTvS7JcvmfkecevtdmkhwfcXHBNhTMphi5177AWe1h5ro6ttWC11r63\n5ftW0nfUfO8jKgrae+ci8JXlVmmWN47Ix3ktD1z87Ko6rCeaErzaiyATHb8fXDywMUpYXsx1Ocd1\nqIs01w7rVz95cfB7C9baYID9M6xSMSn4+ZdWR//ZRIKHSTPb2JWWyoK1w+9sk+RaHOhmXYZ0WHEe\nxVqbPh1Whq6eA5XWQNVAHe4ITtoqcUOX6ieZ+p452R7W/QKZD+H7wV3oNq/DcB3WHqsddjoNTyxm\neoa19rXldyregV37U4Il6dFleNKhS43rqYgE3xJnzTuopXIPa4eudG0kuPysnNNaL9wPmzTrf4aV\n67rRUbAO5O4QUoxMW1sk2Bijq8g/WYd1/8wsZ1jvB1eEdokE95kSvPsh6Z4zp7p5MobHm7SKMu9j\n9ReGWB1Y+fTUVahXVony/LhiJu2QlAj87tH+hyJtOb8vlWttunRY08PD6y5CT7ll/y3GV3RY+0WC\n09xyU2tkFKwDuQ9T1jZM2zK+HS/bdx0FJysS9uNsdFjvh7jL2beekWDfMzf+vCdmGAlu6rBG7GHF\nAFXBGu1OCY5krfRb6+POscZp+7Rb130lHbCVZHmnSHCXqeCbNDsYCd6m1vi8xHistVrXnKuu467v\nuLYbFwXrQHRY56FtSrBU7GI9VYd1PxJ8EflacY5v9rZn39qni3bpzKyT/Nbet+uZRYKttY1TghcB\nu6rR3+bAGdanrkJJOnq1jeuIdIn2Jzx3K3GWN+6ulcpIcOcpwbf/rAVFAM4gznJZWz/n5JCL8jOP\n9Ny4KFgHuuDNdRba9rBKZYf1RJNZb0WCiUXeC2mHDqvvGXmm+1qbxd4HZOh7igJPj2cyJXidFB/0\ntXtYQ9baoL9VzZRgSUefY+0SCa5uPB0ZP75P0swqaulABT0iwYfX2hR/PoPaMKZDO9Hb8Fw9DwrW\ngdyTm5Ul01Z1PBtGll9F/un3sJYF62XEGdb7oMs6DPf9bqsdbndYJen6hM/Fu+ZSCfUdVk9JZo8+\nd4iHZZVkCvbi8o+uIknSy6sjO6xEggfpGglOMts6NGmTZAeLX5oAOIdD8yTaXNJhPQsK1oHck5sO\nwrSt4lSXod+4P+56cboO6/6Z2YvAr4pYzFeXSLBURA27Dl069AF5vQhmU7C6c99NZ1glLvzRzzrJ\nb80ceHTpIsHHdVjd2pVu07650eIkmW2NBEfle2NbZzrODkeCtwUr7xcYz3Yneo9IcMDNlXOgYB2o\nKlh5c520YihM8xvR5QmnBFdDl3Y6rLypzV/XDmvgm46R4PxWJFgqBi/NZehS1WGtnRLMeyT6Wx2I\nyz/lOqzHnmFN3bTvDmdYudFSSbK8moJeJ+j4e9skdWttin+erhXG5K7Peg1dch1WmhGjomAdaBsJ\n5gk7Zaskazy/KhWRxlNNCV4nmTyzvYO/CD3uGN8DSdZ+oSv1We2QHVxUfhX5J+v237Vl3L7jWCKF\ngn7WSabL6OZr4ysuQxkjvXT00KX2pISLC1OwbiVZrrAhpSTtDqtqiQSnefOUYK6pMCJ3hrXfWpvy\n+p/BbKOiYB1oGwnmCTtlqzhrXGkjFRfcp5wSfBn6Mqb4cL8MfcVZroxzfLPWNRJcrHboMCkzORyL\nu55Th3Vzc8DYvm3Bynskulsnt9+zfc/oKy5CvXKiSHBTUiKqOoW8Zzvd9rCWhX7LDbtNzQoRYpY4\nh+0Z1h5Dl0I6rOdAwTrQBWttZqFLJPh6UXS12oZFdFF0dLcdJ/Z13Q9d1mFIbrVDtz2sh+7oPjGn\nM6xtHVZu6mGAVc357kdX4fEd1tTdeCIS3EfcIRLc9fcWp3nN0CW6VhjfesgZVmbYnAUF60DcDZyH\nVdweCb6KAmW5PcmF9Tq+GWcj5nQ/nDoSvE6yg5Orr064YumuuQ5r3RlWV9zzoY4+VnFdwRodPSU4\nzXN5pujY1nGR4JSCtZLm3SPBTcOqrLX1a234rMQZDDrDSof1LChYB1pwN3AWlkla2wFy3FqOUxQK\ny70IcvXGxofwrHWNBAcdI8HrJD8YQXpi4c8mEtzeYSUSjP7W6e0pwVIxKfjYKcFxlne66VT8LJFg\nJ0m7R4KbpoK77x0qDqr3Cz4rMaJ12n+tDTdXzoOCdaALJmDOwrJLh3VRXHCfIoq5HwleEB2/F7pO\nCY5803no0qEug1trc4p4+l17HDd3WBcsV8cA6/jwQLKnrsKjpwSnme0U6y9+luetk+R54+5aqVsk\n2N28Yq0NpqJaa3Pg87jOthHBc3VMFKwDeZ5R5HvV3RlM0zrOdNVy5+y6LDBP0WFdJZkudy62LvkQ\nvhfSrP3sm/t+17U2hy7KrxeB0hPF0+/acpPKmPoPeleQs4cVfawODF2SikjwS8cOXcpOU3g9NEmW\ntxb6gecK/fqbba44OLTSi3kPOIdth7V7OeRuuJCcGxcF6xEWgceb68QtO6y1uSo7RKeYFLw/lfiC\nSPC9EFcd1rZIsGld6yCVZ1gPXLSdMp5+1x6XN4O8mrNt2w7r9P+/YDrWNe/Zj65CfXmdHtX5TLK8\nfZey56Kt0085jKVTJDhwUeoOkeADf1boe/I9QxMAo3LNhEOJpzqeZ7QIPD7bRkbBeoRF6NM5m7gu\nkeCqw3qCXayrJLtxps/9b3NjY966RoJD32td61ANHjlUsJ4wnn7XlnFWxekPYa0Nhlglh+Pyjy5D\nSdIrRwxeSjpEgo0xCn1DJHhH2iESHHUYuuQu8A/tYZWki4C95RjXuuU5Wecy8mlEjIyC9QgXoccE\nzLfamTwAACAASURBVAnLcqs4zXUVNg9duqq6WqfpsF4cGro0g44Z6p0yEtx0juuJsgCcw+ClZZxW\nHeFDXNFBwYo+6jqsT11HknTUpOAukWCpe7T/oYjT7p3poWdYpSKRxM1djGmT5jKm35RgqTgKw3N1\nXBSsR7gIfQaKTNh2imnbHtYTn2G9sdaGadL3QdIxElx0ZpqjhJuGvW9X1XNx+gXr403WOIF7OyWY\nD3V0k2a5kswePMP6ZNlhPWZScJq1R1ulovhKiARX0ty2v/d1iARvC9bDn8kXpNYwsk2SaRF4Mqb9\nRtauosPKc3VMFKxH4AzrtLm4Rnsk+LRnWHcv4qvJh3RYZy3p0WFtGzLUNOThifI89asniKfftWWc\n1k4IlrZ3rGNu1qAjd2Pv8JTgssN6xKTgOMurTmCTKKDDuqvL2d/Q6xEJrulmLUIGWWJc65ojCG24\n/h8fBesRLkKfuNuEuRjuobv1u6qu1pFFQp5brfaG6VQFKx/Cs9brDGtbwVp+yB2arjunM6yP4+YO\na8QZVvTU9J796KrosL50RMGaZHn1vGwSeBSsjrVWSYfOdBi0R4Ldzby6fwcXgc8gG4yqbmJ/m8uI\nSPDYKFiPcBFyh2XKXMS3LRLsLo6O7bC6C/NLzrDeO0lWnHPxW7ozXSLB1VTCQ2ttovkUrMtNc4fV\nDWHh2AS6qm7m1Ky1kcaJBIdB++v4oUhzly45wR7WlomsxTUV7xcYzyY9PLG/DWdYx0fBeoSLwKdz\nNmGuYG2LBPue0WXoH32G1UWQdwtk1trcD106DJIUdBq6dD86rMuWDmvgewo8wxlWdNZUsL5mEcgz\n40SCu0T7H4rO6RLPFawNkeC0/madxNAljG+d5LW7xJswJXh8FKxHWHA3cNJWVYe1eUqwJF0v/KOL\nBDcoZ7fD6ntGkc/zZO6SLG9dhyEVXcW2YS3rhqFL19VO4Ol/ED5umRIsFed8iASjq2ruwIHXhucZ\nPbqK9PLqmA5rt0hw6Hl0WEvu/Sw4QSTY3bxqnBLMDS6MaJ1mgyLBpAHGR8F6hIvAp3swYYc6nnWu\nouDoDmvVHdj73yM6Pn9p53UYpvsZ1gMfkovAV+ibeay12TTvYZWKXdUMXUJX7gKwLhXz6DI88gyr\n7dZhDdpfxw+F+z1ELe9/QTV06ZgpwR7HZzCqoUOXLkKf5+rIKFiPsGAE+6S5jmeX8wlX0fEd1lVc\nPBeuwv2ClZjT3MUnjAQ3xR6l8ubJxAvWOM0VZ3lrhzXy2VWN7lYNN3OkYvDSK0cOXeq21oZIsOPe\nz9o6rC6BEjd0pt3Nq6ahS1xTYUybNK+NqDcphq7y2TYmCtYjMNZ62lYdhy5JxdnBYzusVSR473+P\nsw7zl2S5wo5n35LMytoO57hqLtqeWASTX2vTNW6/CIkEozv3vKq7mfPUVaSXjhi61LVgjXwiwU7a\ndaVXGQlu7rC2rbWhCMC4iinBA86w0mEdHQXrEVhrM21dpwS7nzl2SvCqpnPGNLn5S7NcYaezb+VF\nW15/sdvWYT3Feeq75l4rTVOCpfIMKx0TdOSKlbpVZE9ehUcNXSqGpxEJ7iOuhi51iwR3mxJcd4aV\nc4EY1yYZNiX4MiwaEU03p3FaFKxHuAg9xWmuvOHiFOdTDfDo0mGNgqP3sK5rzsxeRL5WfAjPWtcp\nwa6obbpoW7dMyryKgqNvntw1lyZo7bAGPtFKdNalw3rcWpvukeCEz3VJPTqsZUHbFAnepLl8z9TG\nizk+g7EVZ1iHDV3KbfNUbJwWBesR3IcqXdZpWsZpNaW3zdXi+A7rsmbp/UXgaU10ZNaSHuswip9v\nuGhr6bA+sQim32Etb+506rAS8UNHTVOCpWLo0uM4GzzIK85s61lMqYz287kuqftaG2OMAs+0RoKb\nioOLwFea28Y/AzildZoPnBLMysKxUbAewb3xckdwmlZxrqvQlzHthcb1CaYE13V0LyNG9c9d0nUd\nht9ltUNzLK6IBE/7+fK4Y4c1IhKMHlqnBF9HkjR4tU2a563TbqVu074firgautT1DH/97y1Om99H\nL6PymoqbBRjJJskG7WGtGlZc/4+GgvUIdFinbZWkt1bM1Lk6wblBF2e7VbByOH/2Oq/D8N1qh+Yz\nrMaotvN/PYdIsOuwtkaCGbqE7txNv7qbOY8uQ0kafI41SfPOHdamc+gPiXsv65JUKgr95khwY4e1\nvKaiCYCxFB3WYWdYJTqsY6JgPYKLEfDmOk3LOOs0cEkqLrw3aX5UFGlVFwlmGfrsdZ0uuo0EN5xh\nLe/o1nX+r+cQCXYd1tZIMFM/0d06yXQRerWvjaeuyg7r0IK161l032N/cKlrJNj9TFu6pGnnpet0\ncU2FMSRZriy3A8+wuucq7xNjoWA9gnvjpRiZpmWc1Z6F2ucK2+URH5SrJFPgmVsf7MWCad7U5qxv\nJLhp0FAxRr/+zyoK1mm/p7j4fGuHNeTCH92tk+b37EdXRYd1yGoba62SHpHgNOd5K+3uYT0+Etx2\nhnVRNQH43ePutU3sb+Li63RYx0PBegR30ckZrWla9emwLooL72MmBa+S7ODZq4vQ45zDzJ06EtzU\nZXhiUUzWnXKh5zrAbR3WyCcSjO5WcfOKCVewvjKgw5rlVtaq+9Alpn9K2g6Q6xIJDnzT+N63SfLa\n6egSkWCMy302DRq6RBpgdBSsR+AJO22rJGsdCuO4wvaYs4Ormo6u29eF+eoaCXZFbVssrukD0j1n\nlxM+x1rtOG65M70IKVjR3aq1w1pEgod0WN2Z1E6vY4YuVdIeHdbI9xrTJXGWNxa+27kgfF7i7rlr\n98WADqubj8K13XgoWI/gnuRMtJumZcvd+l1VkXBkh/VQR/ciLEb1cwE0X0mWVztWm3Taw9qyqPyJ\nstv/6oTPsT6OUy0Cr7VbtQh80gXorIjL1782riNfoW/08qp/hzWuzmJ2K7x4vy7EPc+wtnZYG8+w\nEgnGeNzzbNAZ1oApwWOjYD0Ca22mbRWnPYYunabDeuhi65KY0+yluVXYIRIcddjDuk7zxju6Lp4+\n5XOsy01WPc4mi6C54wLsckOX6hhj9ORlpJcHdFjdXtXuw4OIBEv9I8GtZ1iJBGMijjvDSod1bBSs\nR2CtzbT1mRJ8tTg+hll7hpU3ttlL0tNFgospwQ2R4MXxN0/u2uM47TTQbBH4SjKrjBUh6KDuPXTX\nU1fhoCnBfSPBWW6V87ztFQkOWyLB3dfacE2Fu7c9wzp8rQ3P1fFQsB6BtTbTtorbL36cqsN6TCS4\npkB2xQnDueYrzmy3YS0dIsGblg7rE1WHdboFa9FhbX9tucnKUx4ghelomxIsFYOXhpxhdc/BroWX\nJCVMCu651qZl6FLbWhuuqTAiF+dtuoFcxz1X3TpD3L1O/5aMMe82xnzKGPOcMea7Dnz/LxljPlH+\n55PGmMwY89rye581xvxS+b1nT/1/4JwWZNgny1qrZc2Z0kNO1mE9FAmmwzp7add1GF57JHjT0mF1\nq2KmXLA+jtNOA81cN4UhKuhilWStA1AeXUVHdVi7RFvdOdem4uuhcO9l7r2tSdtamzhtXg/mOl18\nVmIMbiXloKFLPFdH1/oOZIzxJX2PpG+R9DZJ7zXGvG33Z6y1322tfbu19u2S/rKkn7TWfmnnR95V\nfv+ZEz72s7tgZ9hkuRhi1ynBp+qwHoqWME16/rpGgsPAXegOH7rkOpevTvkMa9ytw+rOq3FsAl1s\nkry9w3o5LBLcd5/o7j/zkFUd1qD99xb4npKGGHXbHlY+KzEmd+0+ZK3NIvBkDA2rMXX5t/QOSc9Z\naz9trY0lfa+k9zT8/HslffgUD27qGME+XS6m0XtK8JEd1kMd3arDSnRktpK8YyS4/Jmmc1zFJNSG\nDusJuv137fGma4e1eO4TCUYXbWttJOmp60gvr4ZHgrsOXZKaX8cPRZ9IcOSbarjVIW1TgrnBhTFV\nQ5canpN1jDG6CFhZOKYuBesbJH1u5++fL792izHmStK7JX3fzpetpB8zxnzcGPO+oQ90ikLfk+8Z\nOqwTtEyKi/2ukeAo8BT6Ro+PKCrrLraqTjwfwrNkbbGS6GSR4DRrvGibw1qbZZxVqYQmRILRR5FS\nab4sefIy1DrJe3fhiAQP497Lgg5T0tsiwcX5/frfv+ta0WHFGI4ZulT8cx7X/yPqlpfs7g9L+qm9\nOPA3Wms/b4z5bZJ+1Bjzq9baj+7/g2Ux+z5JevOb33zih3V3FoHHm+sELcvCs2vBWvxsoOURRcIy\nzqqJwLuqsw50WGcpy62sVcehS10iwc0d1kVQ3Aib8hnWZZxW576bROxVREfWWq3TDh3Wq0iS9PIy\n0euf7P7+TiR4mCTLFfpGxnSLBKc1keA8t4qz5inBxhiuqTCa7VqbYfNnL0M6rGPq8m/p85LetPP3\nbyy/dsi3ai8ObK39fPnfL0j6ARUR41ustR+w1j5jrX3m6aef7vCwpuEi9ImvTJArDrus3nCuI39w\nhzXLreI011V4+yKe3XLz1msdhtd8oesuypvu6BpjdBX5097D2rvDynskmm3SXNbq4E2/XY+uQknq\nPSm4T7Q16LBP+aFIc1u9r7UJfVMb/3fx6qahS1LxeckNLozBPc+aEk9NiufqdD+n75su70I/L+lr\njDFvNcZEKorSH9z/IWPMk5K+SdI/2fnatTHmNe6vJX2zpE+e4oFPxQV3AyfJ3fXqOnRJKiYFDz03\n6J4Dl9Htl9QlBeusxdWFbnuHIWq50I2z8qK85UbKE4tgsh3WPLfljuPuZ1iJBKNN1/NkrmDtO3ip\nmnbb6XXcvk/5oYjTvNPvTCqORKQ1q4DcTau24uCSIgAjcc+zpq5/EwrWcbVecVhrU2PMd0r6EUm+\npA9aa3/ZGPPt5fffX/7oH5H0T621j3f+8a+S9ANllCSQ9I+stT98yv8D57YIfc4mTpCLBB8qIOtc\nR371zw3+3zu01obx57OWZt07rC4SXHehu71oa/6zrheBHk906JJ7HveZEszQJbRx3Y623dmPLl0k\nuGeHtcfQpbakxEOSZM2raHaFgam9WeduWrW9911wTYWRbMo1S16H89mHcIZ1XJ3aT9baj0j6yN7X\n3r/39x+S9KG9r31a0tcd9QgnjvMW07QqL/YvD0R06xRnWIf9u9x2WJsiwbyxzVGvKGHLhW51R7el\nw3o94UiwK6S7dFhdx5lIMNq4GyHtU4LLDuuqX4fVdf66xFvDgEiwk2Z9IsFe7ZTgTdLtZh3XVBjL\numUnepvLiDOsYxr+bwqSOMM6VUOGLl0v/MFdraaLLfcBzRvbPLnuYLdhLa7DWtNlcHvfunRYJxoJ\ndjd1unRYL1hTgY62q8iaXxuuw9r3DGtcviajDvtEQyLBlSTLO+1glcqCtS0S3HJDgpglxrJJs9bn\nYxPi6+OiYD1SEQngCTs1g6cEHxsJPhBB9rxi8iELpuepzzoMY4wCz7R2WNvOsF4vgsmutenTYa3O\nsPLcR4t12u21cRn5WgSeXul5hjXtkZRwP8Nam2IHddhj6FLt+f2yYG17H70IverGHnCX2ib2t1kw\nJXhUFKxHWgQ+F2MTtI3o9uywDiwStlOJD1/EEx2Zrz6RYPdz9QVrxzOs0fBu/11zN2euOxWs5RlW\nOlVose4x2f3RVTh4SnCn9VSstakkad75vS/wPGW5VX5gtU11hrWlQCjOsPJZibu3SbPWIW9NLkOf\nmysjomA90kXoEXeboKYhSHWO6bC2FcgXgc8e1pnqEwmWmrsMm45dpOvF8PPUd83d1LnqMnSp6rDy\nHolmq47pA6nYxXqXU4KDcggLN1qKor3re58bznQoFtx14NxFQMwS4yg6rMML1ovQoxExIgrWI3He\nYpqWcabI9zrdTXdcV8va/jGwtgL5MmLy4Vz1iQRL3TqsXdbaTDUS3KfDGrGHFR11nRIsSU9ehgMK\n1jIp0SHe6p63RILLSHDn976isD30e+u61obJqxjLOskGr7SROMM6NgrWIxV3A3lznZpVnPaKA0vF\nHlZrh03z3e59remwhnRY52pIJLjuQnd7hrV96NImzatzd1NSdVg7vL62BSvPfTTrOiVYKjusq2GR\n4LDDBSqR4K0kzTvfrGuakh537bBSBGAk6yQ7ssNaHPUa0uRAfxSsR1qEHuctJmgZZ70GLklFh1XS\noLODbXG2IjrO82SOtmffOkaCg4ahSx0jwVfVc3F6z5mqw7po77D6nlHoGzqsaLVKup1xlNwZ1ruP\nBFOwFuuAur/31Z9Z77WHlYIVI9ikxw1dugh9WUuCaCwUrEe64ND1JC2TrH+HtYw4Djk7WO19rfnf\nvKTDOlvbC92OHVbPqz371nUX4RNlMTjF1TbbKcHdXl+LwK+6K0CdTY8O66OrSK8sk16djSGRYPaw\nFuuAur/3NUSCk26R4AWRYIxknRy31sbdeKYGGAcF65EugqLDSiRgWtZDOqyLIzqscXn+qrbDyuTD\nuUpSFwnuvouwNhLcY+iSNM2CdbnJ5JermrpYBKQL0G67h7XblOA4y3sNyUszK98z8rzuHda0Zqfo\nQ5Jmea/3PulwZ3q7h7V96FKc5coOTBoGTmmd5EefYZXE4KWRULAeaVFGApgmOC3LOOs1IVja6bAO\njARHgSe/5mKIDut8uYvWzqsd/KY9rOXQpZYug7t5MsXBS4/jVFeRL2O6Tw7lDjTarJJMgWc6vc6e\nugolSS+vuseCkyyvCtE2VbSVZICSrPtam7ChM90nErz788Bd2aTHnmEtnstE2MdBwXqkBVMwJ6mI\nBLefsdtVdVgHRoKbCmRiTvMV940E+56Smu7AuuM5vevq5sn0PgiXm6zThGCn6LDy3EezdZJ3vsn4\n5GUkSXrpcffBS3HWfXhQ5BMJdpIBkeCmoUtRa8HqigDeM3C31kl+9B5WiQ7rWChYj+TuznCHZVpW\ncaqrkTusTRFkxp/PV5r1iwRHvlfFiPd13UXoIsGT7bB22MHqLAKfbglarZJMFx2PcbgO6ys9Oqxp\nZjtNCJZ2IsEkp3rtYe0SCW67acA1FcZSdFiPGLoU8VwdEwXrkTh0PU3DpgS7c4MDOqwt3YHLcvw5\n5qfvWpumSPCm3PvWFqed9BnWuGeHNfSIVqJVsWKi22vs0VXZYV1277D2iQT7npExTAmWit9B57U2\nvuuwHo4EB55p3Y1+ScGKEWS5VZLZ4yLBAR3WMVGwHsl1SnhznZb1kCnBZddoUIc1Thvf+NyofoZz\nzY+LBPfpMjRFgrt8QG4HgE3vfeXxJu11MyjyiQSj3TrpPnegOsPaY7VNn2irMUahV/86fkjSzHZ+\n74uaOqwdB9wQCcYYquM5xwxdimhYjYmC9UjbAQE8YadkyNClqsM6oEhojQRHvnKGc82SiwV27TKE\nvqmNBK+TbnvfprzWZhlnnXawOouQghXtVj0K1iergrVfh7VrrF9qfh0/JPGAoUsH19qkeacVIu5n\nmKqPu+QK1lMMXaLDOg4K1iMxJWx6rLWtBeQhF6EnY6TlgCJhGTd3dLedeC6A5qZvJDj0vdp1GOs0\na91DKBWxOGOmWbC6KcFdcYYVXazi7jsRF4Gvq8jv2WHtXnhJRfFFJLhnJLhl6FKXP8fFLLmmwl1y\nN1GPOcNKfH1cFKxHWlRvrnywTcU6yWWtek8JNsboOgqGdVjj5qjnJYfzZysZEgmumS666dhhrZ6L\nA85T37UhU4I5w4o267T7lGBJenQZ6qWekeC285O7AiLBkvpFgt0NgUNJok2atU5Hl7YFBDFL3KXT\ndFg5wzomCtYj0WGdHncGtW+HVSoKyyFnWNctHV3uGs9X1WH1ug8eqSvQ1j32vl0v/Ol2WHtNCSYS\njHbrnsc4Hl1FvSPBUY9IcEQkWNZapXmPtTZ+SyS40xlWPitx91yT6ZgzrNvn6sN+nxgLBeuROMM6\nPW53Zd+hS5J0HfmDulptZ2bdY+FO3PwkWS7fM/I6ThiNmiLBSdZ579v1ItCrA26e3CVrbe8pwVHg\n0S1Bq1WPKcGS9NR1qJf7rLXJ+0WCA99T+sA7rEnvHdT1keCiYG1/77vgDCtG4J5fXY8hHELDalwU\nrEeiczY97t9F36FLUrGLdege1sZIsIuOTHDqK5qlme01rKVYa1M3JTjvFIuTVEaCp1WwbtJcWW57\n3QziDCu66DvZ/dFl1G+tTdo92ioVxddDH5KX9NxB3byHNWNKMCbD3UTtegP5kMj35Bmu68ZCwXok\nd/HJ3cDpcB3WIZHgIobZ/99lWyR4wYfwbPWZkim5M6w1e1g7dhmk4rm4nNgZVvfauv7/2Xvz6Mau\n+0zwu28DCHAFuNWmIstWabG1umTJdrwlSo8Tt+2xkzOxk+50Vk/S6T5zZk5PJjPTSc/pTmfrPsn0\nJHHSTnqSTPbNTrzGiWxZdmxL1mZJlkoqlWsvVpEECZLY3zp/PNwHkMTy7n3vAfcB7zvHxxIFgCT4\ncN/93W9jGlgTSXCC/uh36HcQsxkVuwweVq7P8ZhftzyBc+7zDh/Y6aYNzc/AmpAACQYAumcPErpE\nCPEqCxNEj2RgDQi6uCaSN3EQRBLMw7Aalg3DcnpLghNfTmxhMvQ3Au6pa9eBlUH2OJlSUBaMYaWM\nb4ax1iYJXUrQD347iilmM64k2G+3NaskWE0kwW2Bc2FJglk8rMmakSA6NLweVn6GFXD3donVazBI\nBtaASBhW8VAzaOgSW0ow0GRYGeUddLHqNSAnQRLxBWt/Y29JsP9NeUZTUBHMw9piWFlSgmWYtuP1\n2SZIcBCmj0O/g5jLaLBsByWfhzqGySbtV2Uy9rU2htdB7e99U3pJgg1/6pJWBVxyr0wQHeiBSBCG\n1X2+nByuDAjJwBoQSb+meAgiCc5oCnMPa90HozuRxJ/HFjxSQst2YHdgZ+qmv1obwA1dEq3Whg7Q\nLCnBVAY47n7ABN1R5+hEnJlQAQA7FX+yYIMjdGncB1aa9qv4TEjXekiC/dbaSBKBpkgJCZAgUjQ8\nSXAwhjWtSsnhyoCQDKwBQQhxPVrJBSsMqAGeJ3Qpq7EzrFUf3y+ROcUXrJJgz8fVISm4YVi+JUiT\nAtbaUE8taw8rkNgmEnQHz5o9l9EAADs1f8FLBuPBk9ajT3lcQA+ZVJ/VH5TB7qSm8CsJBoC0InkH\nwQkSRIEWwxpQEqwlHtZBIRlYQ0BalZNQEYHgR6LbDZkUu4e15iOVOGFY4wtWSXBr0xacYa0ZFiyB\nfHQ8Hcd0QE/WyATdQDd8rB5WACj6DF7iSfsedxk7refyKwmWpe4eVr+hS0Ais0wQPeqehzWgJFhJ\nPKyDQjKwhoBEEiAWAqUEazIMy2EKifHlYdUSX05cYVi2b0kc0L3awbDcShjfPaxNFpOnZikqeB5W\nltAlKglOBtYEXcA3sDYZVp/VNoZl+w4PAtzPsT7mDKthskmCCSHQurxvLAnpaVVOJMEJIkXCsMYP\nycAaAlJKcsEOAr/56Hl88KNf6/s4uqnm6dfKcAwJfuRsmiyBkGRgjSMMy/EtiQNawSMHPZusm3I6\nFIrkY6UeVpZaG8qqJF2sCbrBj0rlICjDuuOTYdVN2/NY+kGvtO9xAaskGOjOTPvtYQVGlwR4/MIW\n3vSLn8de3X8dU4JoUDctqDLxVAG8SCkyaokaYCBIBtYQkFaTnsFB4MlL23j8wjbW9+o9H1fTTUyo\nMiSOhSjbDJOpMvhnaj5Clwghbvx54suJHQzL9i2JA1ryuYOSYHqi6yd4BGhdiyJV21APK1OtjTew\nJmtkgs6gnw0WG8fsBJUE+2NYTTuRBLOC/v4qw720Uw+1ZTswLMc3wzoxopLg567u4MZuHVe2qsP+\nUcYeDcPmIjUOYkKTkwybASEZWEOAqMXBpmWP1MK4tlMD4J5S9kLNsLjkwAAnw2r4kyAnMqd4wrSc\nUCTBXiohoyRYpOAlyrCyMGEplXpYx/PaNy0bl7cqw/4xhEbNUx+wKRmm0opvhpVHEjzuoUv092dh\nWFWZwDjgu6d2AL+HdSlB91RBUSg3AADbFX+HLKKjqpu4sVsb9o/BhbrP1Op+SCtS4mEdEJKBNQSk\nFEnI08CPP3sdD//qY9itxV9+4jgOrhfpwLrd87FVna2Avh2U1WKRYVLWtN/3dBlW8a6TBL2hWzaX\nJPjgwMrOsDYHVsE8rBOqzCSjGneG9RPPreHhX30MxRHZpEYBv2voQeSymq/Nv+O4DB9b2nfSw0qT\nzhVWhvXAZ50OrH4l2e7h7ui994Wye62OysD6a/9wDt/zka8O+8fgQp0hsb8XEg/r4JAMrCFAVObs\narEG3bKx0UdCGwfs1U2vbuaJi30YVj04w8oyJPj1X6XUpFsujjAsm0kSRyXBB9kZVg/rpIge1obp\nHer4xbgPrJe3qjAsB9d34slEDAKUfWetIpvNaL4kwWaT8QsqbR030MGTtdarm7rE72FdekSrAjdL\nLsNKmda447lru1jbrcfyc9Iw/Cf290JaTVKCB4VkYA0BKUUWsmOQnuhvjcBpHpWd3HN8Bhc2Kz2H\n8GqAgdVLZmVhWD1JcG9f34QqJ91yMQRrDyuVD3fdtPlkazMe2y8Ww9rvOj8Ibcx7WCmbQjerCQ6D\nl2Gdy6i+JMEGR3hQIgluHbr5raMBXO/vQUkwPaxiSgkewSFglCTBjuPg3HoJgP/gM5HQMPmVeO2g\nFUyOM95rxSCQDKwhIC0oc7ZdHR35CfWvfuD+4wCAJy52lwXXdIurgxVoGxIYGFYa0NRvEJlITuJi\nCYNREkwfe3Cz22CM0Z8UUBJcaZjMh0GtHtbxvPaTgbU/eFKCASCX8ScJ9ryYLAdPiSTY62FlkQRr\nHSTBrId1bkrw6L33dA0YhT3ZZrnhDap+g89EQt2wQxpYx1tBNEgkA2sIEJ1hHYXF8fqOy6h+551L\nmEwpPYOXqobJzAJRtLov/W+u64aFtCr1TSUe1VPjUYdhs0mCVU8SfMDDarKxSK1aG3EG1qpujBfR\nugAAIABJREFUMXWwAokkeKviblI3SvG3ZkQFnpRgwJUE++lh9RhWprTvRBKsc0iCFZl4EmwKz7/v\ne2AV02YVBKZleyQC9bLGGa+ul71/3orh7+N6WIOPQPSQLWmAiB7JwBoC3Fob8S7WYvP0axQG1rWd\nGlSZYHk6jTMrc/0ZVs6TMx4ZZs2nTNL1Ooz3BiiOMEzWsJbeoUt+fTMZldbaiLO2VHQOhrX5++pj\nOrDS9XcjYVi7gjKsrBvIuYyKim71vf+aPAyrJMF23EqWcYXn/Q3oYaV9rn6lxaN4uLtd0UFVo9uV\n+K8Fr9wsef8cS4Y1JEkw3WuO2gGLiEgG1hBANeyiYZQY1hs7NSzPpCFJBA+dyuP8RrmrxC6QJFhl\n72Gt+hyQ0+poBkmMOlxJMFtYC9Cph5Wt1kaSCDKaLBbD2rA8FYJftCTB4q2RgwBlHzb24r9JjQpU\npUIIW3f2XFYD0N9DRwcoprRbpbNSYpzAw0yrsnTocKphMHpYm80Lo+QL3Gz6V1OKNBJ7slc3St7n\nKY6/T5ihS0DCsA4CycAaAly/hVgXq+M4nvxkFEKX1nbqODozAQB4cDUHAPh6F5a1GqCHVZElpBSJ\nyTdIN1v9kHhY4wnDspl6WOlNXD8UusRWawO4smCWTuCoUdFNT4XgFy1J8Phd+7bteOxDIgnujrrB\np4qZy7gDaz+Gh5XhA1oVLOM8sHqSYMYe1oOSYNaU4FZ38+i891QGfHppKpYS2oM4t17G3cdnAMRz\nYK2blu/D416gA6uIpNWoIRlYQ0BKkWHaDkyBbmxV3fJuNqPQ/3d9p4ajs+7A+vpjM8hqclcfazUA\nwwo0hwTGlGC/kmDRDjYS9IdhOWwbXSUchhVwg5dEkgRXdXaGlW78RfT5R42dmgG6d98ckSqLKFDj\n7M6ey6gAgGKlN8NKP4s8B08HP8fjhFYdUNBaG3YPKzBaawZVhJ1emkKpYcb6AM9xHJy7WcLrjs5g\nKq3Ec2A1bKbD426gZEVCRkSPZGANASKmhLUvIHFnWC3bwc29Oo7OpgG4N8Q3rOQ69rFatgPdtJFR\n+UKXALgyTKaUYNMXOzChJQxrHGHaNpuUsK+H1f/GXDRJcKXBzrBKEoEmS4cY53EA9aotT6exsdcY\nKYljmKjxMqxZfwwrl7RVSRjWVg+r//dNkQ7XAbHX2rjv/Sj5Ammlze3LUwD6H7KIjJt7dZQaJk4v\nTSKX9deFLBrc0KXwPKyJ3St6+BpYCSHvIoS8Qgg5Twj5mQ7//R2EkF1CyDea//s5v88dBbQkAeJc\nsHQByWW12Bv8N0sNWLbjMawA8NCpHM6tl7F1gLVodaIGYFg1VobVRtrH90v6uuIHx3FgMPewdpYE\n1zmCZbIpRZiB1bRsNEybmWEF3N95lNgSv6DSv9uPTKFh2tiri/G3FA28FRN+JcFcPay0T3mMQ5fo\n7y6z1Nooh+uAdFaGVRk9X+BmqYEJVcaJXAZAa4CNI841E4JvXZrCnM9qKdHQCK3WpnmtCrT/H1X0\nXT0IITKA3wTwXQDuBPAhQsidHR76Zcdx7m3+798zPjfWoItwXUCG9bULkyhWjFgPSdebHaztA+uD\nq3kAh32s1O/nZ4DshkyKjWGt65YX1tQLIjLxCXqDMgVhSIIbpg1N7l9/1I7JlCJMD2s1wGGQpoiZ\npB41qLrljiPTAIDNxMfaEX5zAA5itikJ7h+6xCFtpaFLY7xeG5a7ZrGEYXWWBLP2sI5e8mqh3MD8\nlIb8pHvIEschj+LVdTch+PTSFPLZ+A2stu1At8IJXaL2s8TDGj38/LXeCOC84zgXHMfRAfwZgPf5\nfP0gz40NRGZYX7M4Cd2yURaEpeHBGh1YZ1oD693HZzChHvax0hNZPwNkN2Q1Nlarapi+PLNJX1f8\nYNrs6aL0sYclwRazZ8aVBItxvVDVAU/HcUqRxvKghg6sVAaYJAV3Rs3gyx1IqzIymtx3w8ybdtv+\n3DjAcRz8+P/3FB55aT2U1zNMm+k9A1xJ8KHDOtaUYCoJHqEhoFBuYGEyhXw2/gPrKzdLmJ9MIZfV\nMJfVYpeTwipR7wVPDSDQ/n9U4Wf3dAzA1bZ/v9b82kG8mRDyPCHks4SQ1zE+N9bwahsEWlypP+I1\nC1kA8V4cvYG16WEF3M1Epz5WWkcTRBKcy2pMvt+a7k9aMoqnxqMOw+ToIezifWtw9L5NCiQJpmsI\nDbphQUqVx3Jg3W5Kgm9rDqxJ8FJn8KYEA64s2K8kWGHsYXWfGx91Uqlh4h9eWseXXt0M5fVM22F6\nzwBXEhw0IV1EEiAoNksNzE+mkM+mAMRcErxRxumlSQBN21nMPKxeAGIYoUsaPVwZnWtVVIQVuvQM\ngFscx7kbwK8D+BvWFyCEfJgQ8hQh5KnNzXAW20FBxICAYlWHRIDVeXdgjXPw0o3dOqbSCqbS+zfK\nD53K4+WbpX3DOB1Yg6QEH5lJ48Zu3beMuu6zRidhWOMHnYOZadVhHEwJZpcgieRh3Wp64fOTKebn\nphQJukDr46CwXWlgOq14doaEYe2MmmF5VSasmM2oviXBGsPwpcWwh5VeX+t74UjPdctmOqwDKMPa\nWRLs9/0fxYG1UNYxP5XC9IQCRSKxJRFs28H59RJOL7mHcLmshrphC1W/1g90rx6mh3WUrlVR4Wf1\nuA7gRNu/H29+zYPjOHuO45Sb//wZACohZN7Pc9te46OO45xxHOfMwsICw68wfFCGVaQLdruiYy6j\neZvLuEk22nF9p4Zjbf5VilYfa0sW7EmCOWSLFMszaeimjWKfTRDgSrD8pgSPosxp1EElwTyhSx0l\nwYwSpKwmo6JbsAUIfqGMwHzTg8WCcZYEz0+mMJVSkFKkpIu1C+o6P8Oa8+GhM73QJba0W6C1BsQB\nG81BdT2kgxHTYpcEux7W/euV3pQW+/Xvj9q90rBsbFd0LEymQAjxdc2Kius7NVR0qzWwZuIncW54\nif0heFgTImJg8PPXehLArYSQVUKIBuCDAD7R/gBCyDJpuvIJIW9svu6Wn+eOAkQM0ylWdcxlNc8v\nEWeGdW2nhiMz6UNfv/v4LNKqhMcvtGTB1EfAu/kB4H2vG7u1vo/VLRu244/RTdLk4gceSbAsERCC\nDiwDH8MKtAKPhgmaeMvDsGpjmhK8XdGRy2oghGBxOoWNUsKwdkLdtLnX7NmMhp0+kkSqlGDpYaWf\ned0c/mGRX9DrayMkhpU1IR1wDwU6SYJZDuu8HtYRUWXQYW5+yl07c1kNhXI892SvbtDAJVcS7FVL\nxaimp+6FgAVnWFVZgiIRoRSWo4q+K5HjOCaAfwXgcwDOAvgLx3FeJIT8BCHkJ5oP+14A3ySEPAfg\n/wHwQcdFx+dG8YsME2kBe5i2KzpyGc1bTOJ0+nUQazu1fQnBFJoi4Q0n5/YFL1FZShBJ8HIz3Onm\nbv+bfl13b8z+GFbxrpMEvWHQ0CUGloEQAlWSoB+SBFteQINfeAOrALLgzXIDmixhOs0TuiSPzOaT\nBVtld2AFgMWpNDaTgbUjajpfSjDgeqr7qWHMAJLgWDGsTQZ/o9QIRZVh8DCsXSTBLHVeoyazpJ/7\nheZhX34yvnWD7ZU2AJDLulatrRj9PvUQGVb3dWTU9PisE3GFr51HU+b7mQNf++22f/4NAL/h97mj\nBq/WRiAGoVgxsDKfQVaToSlSbAfWmm6hWDU6DqwA8NBqHr/6yDnsVHXMZrQ2SXCAgXWaMqz9B9aq\n4X9AnkgY1tiBynpZNrqA63k9uGlzvc5sw95kc2AtN0wsMj0zfGyVdeQnNaaKC4qUIqFYFWd9HBS2\nKjruPzkLAFicSuHVjfKQfyLx4DiOmxIcIHRpt2bAtOyuAUGt0CV2SXAcPaym7WC76srRg8Dg8LCq\nsgTbASzb8fpbG4bNNrAKuKcKAhq2tjDlHl7lsyk8X9wZ5o/EjXM3S1ieTmNmwh1Uc80QqX7BZyLB\nC10KgWEF3IE1YVijR1ihS2MNEU8Dt6stKVoce7Io1pqy3E4eVgB48FQejgMvLTiMlOCFqRRkifhi\nWOmA7GezRYfaZGCND6gkmDUpU1UOdxHWGTdtQOs6FqHaplBucG+AU6oEXSDLxCBg2w6K1XaGNRWa\nVHOUQK00vN3ZNLV6t9adZW3V2oy2JHi9jcEPI3iJRxJMDwXa1z/dspm6rEXcUwVBwWNY3cPwXFbz\nLBZxw7mNEm5tyoGBdg9rfCTBrdTqsAZWCfXEwxo5koE1BLT8FmJsyBzHQbEZugS4J9CxHViblTad\nPKwAcM+JGaQUCU80fayehzXAwCpLBItTKV8MK8v3S3vhXGJcJwn6w/BCl9i7CA9KgnlrbQCgIkAC\n41ZZ5wpcAqgkeLyu+726Act2PAZiYSqFvbo5MpvwsBCU7fA8dD1kwTwpwfQzHytJ8F7dOxQLI5Ha\nsGwmVhpoT0lvvW8uw8ruYR2VeyVlWOc9hlVDqWHGziZh2Q7Ob5RxW1MODABTaQWyRGIlcaZrDusB\ncjdMJAzrQJAMrCGglWgnxgVbbpgwbcc72c9PsvWKioRWB2tnhjWlyLj/lpaPtaZbkCXCLOE8iOWZ\ntK8TahaGlfZ1JQxrfGCYfJJgraMk2PbdQ0hBPawiVNsUyg2uwCXAff/itjkLCrrm0iF/cco9dEt8\nrPsR9JCRHsz2kiTySILVDoOX6NgsNXDHkWkAYTGsPJLg5qDfdmDXMC2mtU+WCFR5dIJsCiUdGU32\nLCGt9ob4sJIAcHW7irphewnBACBJpEmKxOd3afWwhicJTlKCo0cysIaAlGDMGV0EZ5s38lxWi22t\nzdpOHYS4A2Q3PHQqj7M397BbNVBt1iPw+Oza4Xax9k8JZmJYk9Cl2MG0w5ME8zCs2ZT7+PKQB1bH\ncZoMK78keNwYVir5oweHC9Pue5ckBe8HvW8G8bACvavb6OeYqZ7Kk7bGSBK8V8frj9GBNfh1ZloO\n82Gd0olhNdntEGlFFoYECIrNcgMLU621k64JtCosLji37iYEt0uCATd4KU57zDBrbYAmwyrI/n+U\nkQysIYCeBorCIGxX6UaJmuLjLQlemkr33Gg8eCoHxwG+fmkbNcMMJAemWJ6ewI3dOhyn92aFycOa\n9HXFDrrFKwkmhza6dcPmTwke8jWzVzehW3YASfD4eVipRK7dwwoAm0kX6z7Q9ZB38zjb9LDu9JAE\n02uP5XPcSdoqMioNExXdwvG5DHJZDeshXGdBJMH6oYGVbe1LjdAQUCjt9//nJ+PZ3kBD425tY1iB\n+NnO6F49LIY1pUqJcm4ASAbWkOCeBoqxuNKTLnrynMtoKMfQLwG4oUtHZruzqwBw74lZaIqEJy5s\noapbgQKXKI7MpFHVLZT6MFssDKsqS5CTvq5YwTDZw1ro4zsxrHGVBG9RDxYvwzqGHlYqCc63eViB\nhGE9iFpAeR49ENjuIwlWJMKkvPEkwTG5bul1tTiVCi3gSw8QutQuCdZNttAlwD3AGBU1UqHc8Cpt\nANfDCsRvYH3lZgnHZie8bAWK/KTW8/MnGlq1NuEMrC7DOhrXqshIBtaQkFIlYQYRugjSG3kupqd5\ngCsJ7uZfpUirMu47MYvHL255kuCgoBLkfknBLAwrfVzS1xUf8EgJ6ePbB1bLdmBYDjPDmlHFkATT\nkvs8J8OqKRIs2znk6x1lbDffs7mm0iWfTUEi4YThjBLoUMK7bmc0GZos9fSwmrbDzBR6g1cIfaaD\nAB1QF6fSWJpOhyQJ5uhh7SgJZuthBUarKmSz3PACl4DWIVYcJcGnD8iBAZcciZMkOOzQpXQysA4E\nycAaElIC+S3ojZumJ8b1NM9xHKzt1LpW2rTjoVN5vLS2h429emgMK9C/i5WyA36/5yjdhMcBBqck\nWJXJvo1uK+SB3Q+WVqWhM6yFwAyr+3uPE8u6VdExlVY8KaQsEcxPprCRSIL3ISjDSgjBbEbFTo/Q\nF93k6xMF9ktbRQattFmaTmFpOjXE0CU6sLaHLnF4WFVpJOwzumljp2p4lTYAMD2hQJFIrPZkpmXj\nwmYFp5enDv23XFZDsarDjsnhTt20mla+xMMaJyQDa0hICxQqsl3RoUgEU03ZBq1ViNPiCLg/b8O0\ncbRH4BLFQ6fysB3g+eu7oXhYl6Ypw9o7eKmqs222kr6ueIFuung2be2ezSCphJMpBZUhXzNUEszL\nsI7rwEoPCykWp1NJSvABhFFFlsv2liSats0cHkQ/82ZMQpcOMqyFciOwosGwHCgSX0pwkFobQCyb\nVRBsVfZX2gDuIUvcskUubVWhWzZOL3YeWG2ndxeySHDzJMIbf9KJh3UgSAbWkJBWZWH8FsWqjrms\n5vl1aPhSnBZHwJUDA8ARHwzrfbfMQpMlOA4woSp9H98PdGDtx7DWDQuE+JeWTKhysrDFCC2GNZgk\nuE6LyjluktmUMnSGdbOsg5BWSTwraEH7OAUvbVcani2DYnEqnXhYDyBoSjDgBi/t9PKwmuySYFki\nkEh8Qpc2Sw1oioTpCQWL02nYDgLX2RmWDU0JSRLMqC4ZFTVSoUTrrfarU3JZzbNaxAGvNhOCTy91\nHliB3j5ykcCT2N8LaU0cheUoIxlYQ0JKkYQ5Ddyu6Ps2lpRh3YrR4ggA15sdrH4kwWlVxr23zALw\nL8/tBU2RMD+Z6uthreoWMgw1OonXIV4ISxLcCMCwZrThD6xb5QbmMhpzvQ9Fi2Edn2t/q6x7ay/F\nwmQqGVgPgB7gsQ407ZjLaCj2SAnmkbYCTaVETAbWjVIDi1MpEEKwRAO+AvpYed63TnVAusnOcKdH\nRGZJ7RTttTZAM6ioEp+14Nx6GYQAr13s7GEF4kOK1A12iXovTKhuqGBcJNFxRTKwhoS0KguzGStW\nDS/oAwBmJ1RIpHexuoigPaj9QpcoHlrNAQhnYAVcH+vNPj6gmmExSdkShjVeoJsu1kFNOSQJ5u99\nm0zJqDSGe80Uyg3uShsAXkLoOEmCt7tIgrfKDVjJxsZDnTG4rhPm+nSNGzZ72i3gDqxxkQSv79U9\nZRD9/6A+VpNDEtypDqhh2hwM62ikBFMLwMIBhjWfTcVmwAPcwKVbcpmO+51czHJS6kbIDGvztUZB\nESAykoE1JIh0Glis6N6JFwBIEsFcRgssDxo01nZqSKsS5jJq/wfD9bECwbxQ7VieSfdlWOs628KX\nUsVh4hP0B910sbIDmiztD10yKYvEfm1mUwoq+rAZVp07cAmA519rjMm17zgOilX9kOd3cSrlSjVj\nlg4aJYL4uynmMip2akbX3mzDZE+7BVylRFwkwZRhBdoG1oABX7plQ+WUBJu2+76Zlg3Tdtg9rANW\nI63t9M6r4MVml8C6XFaLlert3HoJt3bwrwKtgTUuScF1w+a6F3cDPWxL9nbRIhlYQ4IrCRbjdIV6\nWNsxl9W8moW4YG2njqMzE77ltvfdMoesJgfaWLfjyEy6r4eVtfc16euKF2gHI6v/7eBGN0iM/uyE\niuvF2lD9n4VyA/lAA+t4SYL36iYMyznkYV2YcgeJRBbcQs2woARM7JzLaLBsB3v1zgc7ps0nCVZk\naZ+0VWRs7NW9gXV+UgMhCFxtY9oOVEaGla6Vuum+b1RSzZMSXB/QmvfUpW28+Ze+gFdulkJ/7c1S\nA5Mp5dBBej6rodQwY7Em6qaNi4VKx0oboDWwxoUUcT2s4YYuAUjUcxEjGVhDgigBAbbtoFg1DoWj\nxC2RDnA9rH7lwIDLrH7uf34bfuQtq6F8/+WZNHZrBqo92K2awdb7OpGY82MFo8mSKhJrh6PkDbtA\nSwrLwyK9795j2Kro+PQLa8zPDQsuw8ovCaab1XEJXeqWqrw47Q4USVJwC3XDDtydTRVF3Rge3XK4\n/NfagfA0UVE3LOzVTSw2mVVFdjMYNgJIgi3bgcUhpT4oCaaqCuaBdYBVgS83B9VvbZZDf+1CuXHI\nvwrAOwAs9qhjEgUXCxWYtoPbOlTaAO59LaPJsWFYGyF7WD1JcLK3ixTJwBoSXL/F8G9spboJy3YO\nMaz5PrH/IuLGbg1HZ/tX2rTj+FxnjwUPaBdrL1kwq4c1rSQe1jjBsNywEL8sP4UqS96wC7SFLjHK\n4gDg7acXcOviJP7rYxe6Sh6jRN2wUGqYwSTBzRv6uHhY6eFgp9AlAEkXaxtqhoV0wDWbZjZ0y2kw\nTBvaCEuCabjSYttgFLSL1QucY5QEKwckwZRh1TglwYNY864VXTlwP0UVD7r5/ykrWYiBPeBcMyG4\nmyQYcA+N4rLHrIedEtx8rVHoDRYZycAaElIDPA3sBbpg5LL7fZ9zMWNYddPGRqnBxLCGjeVp93v3\nHFh1doY1WdTiA9OymeXAQCdJMH/okiQR/PjbTuHlmyX84/kC8/ODgsq8AoUuyeMlCabv2cHQpYWQ\n0ltHCW4ASrCtyGyTYd3pkhRs2jZzeBDgDl9xCF2iByCUYQWApal0IEkw9eCzSoK9HtamJJibYVUl\n2A4GIsmmjQT9etd5sFlqdDzso+qLOOzLXl0vQSLAqYVs18fEScVXNyyuw+NumPAOZMfj/jYsJANr\nSEgN0G/RC3TBmMscZliLVT026ZTre3U4DnB0ZogD60z/LlZWhlWU6ySBPxhWOOminoeV81T3ffce\nxeJUCh/90gWu5wdBoSlfzWeDMKzjlRLcYlj3r8NpVcbMhOoFsSRwPxtBJcG5PrUauuVA5ZAAxqXW\nhnqi2xnWxel0ICafWhrYK72akuAmw9ow+WqLBpm8eq1YBQDcjOAgqVDWO0uCY5Ss+8p6CSvz2Z6s\nZK5PUrdIaJh2yB5WyrCKv1bEGcnAGhLSigxdgB6mYpeNUi6rwXGA3Zr4fgmgdeI5XIa1KQnuIaty\nGVbF92tOqO51EpeDg3GHbvGmi+7f6HoeVk7fTEqR8UNvWcGXXy3gpbU9rtfgxVazK3C+w6bLL1Jj\nVmvTbWAF3KEiYVhbYM0B6ATPw9pFkmha/JJgMw4Da/MetW9gnUqhUNa5Jc104GT1/noDq0kHVsqw\nsv2NUwP0BVJJcNgMa8O0sFszOjOszQPAOEiCX10v43QPOTDQZFjjIgk2LObrsRcmEg/rQJAMrCGB\nnrAM+zSW3rAPMqytnizxF0egvYOVzcMaJiY0GbMZ1ftZOsFlWP1/jNKJdCRWMC2+dNFuKcFBfDM/\n8MaTyGgyfufLg2VZC6XO8lYWeLU2YzKwFspuMminv/fCVCrxsLahpluBKyam0gok0l0SbFh8kmA1\nJinB66UGVJnsu+/TahvegC/6e7NWenmS4ObzPYaVOXSpecgVcTZI3bC89yhsDyutrenEsE5PKFAk\nIjzDWjcsXNrqnhBMMZeJTxNF3QibYU1SggeBZGANCfSCHfYJizewdmBYAcSm92ttx71xDJNhBVyW\n9eZu9xs+s4c1MefHCkEkwY4Dj0lveVj5N+YzGRXf98AJfPK5tcg6AzuhW48gCzRv8zke1/12Re/I\nrgJNhjVJCfZQN4OnBNOu8W4Mj8ktCY5P6NLCZApSW5r5UjORmjd4yeQMXTosCaahS5yS4IjXDLqW\n0pCqMFVydBDutHYSQmLh+/zWZhm2A5zukhBMkZ/UUNGtoe+B/cD1zYcfuhSH3z3OSAbWkEAZhGEX\nB29XDGiyhOwBX2UuRn4JwJUE57JaqIsKD47MpHFzr/Nw4DhOk2FlkwQDSHysMYHBGbqkeCyD+3eu\nmxZUmUBmrMc5iB95yyocAL/3lYuBXocFW2UdWU0OlL49jpLgrgPrdBobpcZQEp9FRJ3x0K8bZjMq\ndroMrLplQ+X47KkxqbXZKNWxML1fjUQZVt7gJfp7szLTLUkwZVh5Q5cGs6ei9qMzJ3MwLCfULlEq\n9+3EsALuvqwgOInw6rpb9XN6qffAOtcn+EwUOI6DhmkHVnW0g94bk4E1WiQDa0hIe6EiQ2ZYKzrm\nsuqhGg7ql4iLx2Bth73SJgosz0x0TQmmN1KWzRYNnkgY1niA1tqw4mAXYViemRO5DL77riP4069f\nxV59MBuDQrkRyL8KjN/AulXWu0qoF6dS0E0be/Xu/c7jhFoIKcGAu2Hu1mlpcEv74yEJ3thrYOnA\nZ5R2/vLKz/XmwMn6vskSASGtWptWSjBrrU1TtRbxnor6V8+szAHgZ6Q7oeCpUzqvBflJTXib1rn1\nEhSJYCXfPSEYaDVTbAn++7Q60aPoYR2P+9uwkAysIUGUC3a7qh/yrwKtnrq4eAxu7NSHmhBMcWQm\njUJZ73gQQf0KEwwLX2LOjxeCSILp84FwUwk//NZTKDdM/OkTV0J5vX7YqjQC+VcBV/6mKRL0MRlY\ntyu6V1txEJRt2Ux8rACaKcEhdGfPNZPwO8GVBHMoJaSYSIJLdW9ApchnU5Alwi8JtvlSgt3ntELn\nAqcER3yvvF6sQZYI7jkxCyBcH2svSTDg/o1EV72dWy/h1EK2r6Sbdk53OzQSBbwHKL1A/daJhzVa\nJANrSKAMwrAHkWIXKVpKkTGZUkKVu0QJl2Ed/sBKq206pXrSxSnDIAlOvA7xQmiS4BBTCe86PoM3\nncrj975yaSADYKGkB/KvUqQUaegKlEHAcZymJLjze5Z0se5HLSQ/2VxG7Tqw6ryhS4r4kmDdtFGs\nGlic2q9IkiWChclUYEkwz4GdJkvBJcHKYPIerhWrODKTxvFZ2rseXj5AoaxjKt05fA1wJcGi54qc\nWy/j1j5yYKDFsIqu4qOMfZgMqyJLUGWSDKwRIxlYQ4Iog8h2VT8UuEQRB4M/AOzVDZQaphCS4CM9\nulhruivpSzOwA5RJSBa2eCCIlJA+H3BPdcO8QX747adwc6+OTz2/FtprdsNWJbgkGKADq9ib/zBQ\napjQLbuHJLh5CJYELwEILwBlLqOhWDU6eoNNy2EO/QHcwcsUvIKMhqItdviM0iAhHlB1CM/6p8jE\nY2h17tAlKgmO3sN6bHYC+ckUFImEzrAu9Djsy2c1lBqmsAd5Nd3C1WK1b6UN0PKwbguXjLgtAAAg\nAElEQVRe0+Ml9ofIsALuDDDs/f+oIxlYQ0LLwzrkWpuK7pWoH0Suh2RKJKwJ0MFKQbtYO1Xb0JJo\nFg9rWpBwrgT+YFoOlyROOyAJDrv37R2nF3B6aRIf/dKFSMN7LNtlC+cDSoIBV+URdUWFCKC2i+6h\nS8G8haME07JhWE4ooUtzWQ26aaPagZFza204JcGCH7LQgXRp+vAB7+J0mpvJbzGswcKqeHtYB0UC\nXCvWcHwuA1kiWJpOd82s4MFmH/9/flJsGe35jTIcB7htuXelDQDMZjQQAmwLHrrU8rAmA2vckAys\nIaGVEjy8C9ayHezWjK4Maz4G8hPA9a8CggysTYa1002sJQlmYVgTr0OcwMuwHpQEh+lhBVxP6I+/\n9RRevlnCl18thPa6B1Gs6rAdhMawDrunehCgtotcFw/rVEpBWpW4+zFHCZQ9C2VgzbiSxIOHso7j\nwLQ5veiKBF3w0CU6kHZKol2a5u/8DSIJViUSvIeVdpZHeK/UTRvre3Ucm3P3GsszadwMOXSpF8NK\nD7UKgrKSr6yXAMCXJFiWCGYnVBQFV/HRPTrr9dgPE6qcEBERIxlYQ8Kg5Cu9sFczYDutG/dBzMVE\nEkxj5o8JMLBOpVVMppSOMqEqlQSzMKyCSMcT+IMeOHSp5WEN+0T3vfcexeJUCh/90oVQX7cddCOV\n7+LHZIGmSGPRw0rX2Pku7xkhBAtJFyuAlj8xjMOc2S61Gi1pKwdTKLWkraKChncdDF0CgKWpNIpV\ng0tySt83Hg9/u/e3FXLDKQmOcAi4uVuH7QDH2wfWsCXBPRlWsesGX10vQZMlnMxlfD0+DnvMMDrR\nOyGtSkn7Q8RIBtaQQBnWYW7IqNm9mxQt31xMRO//W9upQZFIKEEvYWB5Jt3RB1T3UoKTgXVUYVo2\npySOMqxNSbAZ/sCaUmT88FtW8Y/nC3hxbTfU16agioxutQwsSKny0C0TgwCtqejGsAKujzUJXWrz\nk4Xw2ejWNR6IKZQl4SXBG6UGJNL5UInKhHmuNfq+8dR6HZQEa4p0qGqvHwZxr7y2UwUAL3DpyHQa\nN3broeyR6oaFUt3suXbmu1yzouDcegmvWZyE4vMayMdiYA0/dAloMqyCepFHBcnAGhK8xXWINzcq\nxehUawO4N3TdslER/BRobaeG5Zk0ZA7PURQ4MpPuHLrEIwlOBtZYIWjokukxrHboEiQA+P4Hb0FW\nk/E7EbGsHsMaRkqwPB4pwYXmkN+rCmhxil+qOUrwDv3CqLXpIgk2A4UHSTAED11a36tjYSrV8X4Z\nxC9tegwrx/vWJgnWTRspzjVUlkikQwDtYD0+5zKIyzNp1AwLe7XgHcmtDtZeoUupfY8VDefWyzi9\n1N+/SuEGn4k9sEblYU2pcsKwRoxkYA0JtGNsqAxrpTfD6p1AC+5jXdutC+FfpVjuEsRAwz1YNlt0\nkaSBTeME23Zwdbs67B+DCYbl8NVhNDdo7V2EYd8gAWBmQsUH33gLPvn8DS+sLEzQ4auXD8svUup4\npARvV3RkNLnn33sxkQQDaB36hZHY2U0SrAcID9Jkt4dVZFXSRqlxqNKGgjKsPNU2gd63dkmwaTF3\nsFKkFSlSSfC1Yg2EtLIq6P/f2Au+lnprZw9J8PSEAkUiQrKS5YaJ6zs1nPbhX6XIZTXhqxMj9bCO\nwf1tmEgG1pCQFiB0iZ5s9aq1AdyaCpGx1oyZFwVHZtLYKNU9toyi5b/yv9mSJQJNlsYydOkz37yB\nd/znL0YyWEUFw7KhKSFIgkOutWnHD79lBZbt4OPPXg/9tQvlBlSZYHrCf9dwN6QUaSC9scPGdpcu\n7HYsTqdRqptjr7Sgw0gYDOvsRLMHMmRJsOO4gYaiYmOv0bHSBmgfWNkZ1iCSYEUiHkPbMG3uhPSo\nk1evF2tYnk57lTtHeoQssoKGqvViWAkhwtYNnr2xBwDMA2tRcNtZmDaEdkyoMuoJwxopkoE1JKgy\ngUSGW2uz3YxG71VrAxyWTIkEy3Zwc7fu3ThEwPLMBGyn1XdHUeeQBAMu0zSOG9UX1/Zg2Q7ONZMH\n44DwJMHh1tq04/hcBsvTaXxrsxz6axdKDeSzKWb/WSeklPHwsG5V9J5yYKDFWI97UnAtxM2jIkuY\nTivY6SIJ5pK20s+xyANrqdExcAlwZdKqTLgY1iDvmypLbeoSfjtEOuLk1WvFqhe4BLj3eiCcgZXK\nfHsxrIC7LysIqHr7+sVtAMCZk3O+n5PLajBtB6VGcEl1VKAsKC/r3w1pVUo8rBEjGVhDAiFk6D1M\nxaqOtCp1Pa2mfgmRq202Sw2YtiOUJJgOzwd9rFXdgiIR5oFmYkz7ui4VKvv+Pw4wOSXB9DntSZlR\nSIIpVuYzkbyvWxXdS7IMCk0ZDw/rdqXR1/O7kHSxAgg3JRhw1UXFECXB9Dmi1jGZlo2tSndJMCGk\nGfDFz7AGlgQblsdgsiIV8RBw/YCaa3EqBUIO3+t5UChR/3/v9TM/qXlBbSLh8QtbuH15qqtirxNo\nforItrNGVAyrlnhYo0YysIaIVMR+i37Yruhd2VUAmMt2lkyJhLVdcSptKKis6uCpa82wuPoDJ7Tx\nHFgv0oF1Kz4+Vt2yoXJIgqmM2LAc2LYD3YomdIlidT4byftaKDdCS+tOKZJXcTHK2C77kAQ3WZdx\nTwqmBxhh9LACnUNfaC0Nb9ot0GIbRUOhrMNxOlfaUCxOp7DOcTBiBAmrapMEB1n70oocWS6Iadm4\nsVv3ApcA93edn0yFIwkuNzAzofZV1uSzKeH2ZLpp46lLRTx0Ks/0PJqMvi2wio+qfMK+H6eU8dzX\nDRLJwBoi0qo8VAZhp6p7wROdMJlSoMmScItjO6i/MQ4Ma92wuLxXaUUeOw+rbTttA2t8GFbDsqEG\nCF0yLDuyVMJ2rOSz2K7o2K0Z/R/MgK2yHu7AOuKSYMdxUPAhCaaM2LgHL9U4gut6YS6jHhpYDTOY\ntBVosY2igTL03RhWwO1i5ZEEB64DalOX8HtYoyMB1ksNWLaDY3P79xpHZtK4wcFIH4R72Nefncxl\nNeFUby9c30HNsPDQqRzT8yhhUhR4j1k3LBDCd4DVCy4RIeY6MSpIBtYQEbXfoh/6hX2IbPCnoAPr\nkVlxPKyzGRUpRcLN3f1hQVWdc2Adw4Xtxl4dDdMGIcDlmDCslu3AdvjrMAB30xdV71s7VuazAMKV\nWzuOg02fmy4/SKnyyIcuVXQLumn3ZVhzWQ2yRBIPa4gpwUCTYa3sP7Qx7BAkwYJet5Sh7xa6BABL\n0ynu0CWJgKtebn8Pa4CU4AjtM9eaifXHDwysy9NprIcUuuTnsC+f1VBqmELZJR6/4PpX37jKyLB6\nwZ7i7jHrhoW0IoeSy9COtCJDt2yhA9rijmRgDRGuJHiYHlajr99A/IG1jqmUgum0OuwfxQMhBEdm\n0rh54JS6pvNJgtPK+KUE00HqvhOzuLpdPZS4LCI8hiFgSjD1YEXJsK7SgTVE9rrcMKGbdsgMqyV0\ngmRQUO9Wv4FVlgjyWW3sPaxhpgQD1MN6kGENxhQC4oYuUakvta10Ak2krupsQTiG5XCx0oC7/hn7\nUoIDDKwRDXLXdzrbj9ze9XBqbfoFLgGtjuuDBy3DBPWv9lvHDsIL9hR4jxlVYv+EJjVff7z2doNE\nMrCGiNSQe5hcD2vvQU/0nqy1nZpQcmCK5Zn0IYa1xikJHkcP64XmwPrO2xZh2o63WRAZdJPKJQmW\n2hnWaDwz7bgllwEhLZ9wGKDJlaGFLskSbEfczX8YoJVhfob8xemki7UWcifiXEZFVbf2sVVBvJjC\nS4L3GiAEPVUQdJhl9Usbls0tm9zPsIopCb5W7Gw/Wp6ZwF7dRCVg0m3BJ8NKh7xCWYy1wLD4/KuA\n25igKZLgHtZoEvvpgfS4kRGDhK/ViBDyLkLIK4SQ84SQn+nw33+AEPI8IeQFQshXCSH3tP23S82v\nf4MQ8lSYP7xoSCtSZAEB/WBaNnZrI8Cw7tZwVCA5MMWRmYlDHlZ+hnX8BtaLmxVMqDIebN4Ewxys\nokKLmeFgWJVWWEtjAAxrWpVxdGYiVEnwVtn/8OUHVBY4yj5Wurb6YSbc9FYxNqnDQt2wkFal0OR5\nNMNhpy0pOIgkWPGUEmJesxulBvJZrScTutQMZGKVBZuW7f3+rFBkyTso0E2bOyU4rUSXvHq9WMPC\nVOrQuux1sQbwsdYNC6WG6ZNhbQYVCbIve/7aLmqGhQdX2fyrQNN2ltGETgmOimGl19G47e0Gib5/\nNUKIDOA3AXwXgDsBfIgQcueBh10E8HbHce4C8B8AfPTAf3+n4zj3Oo5zJoSfWVikh8iw7jTDVvx4\np0SWa6zt1HFEUIZ1fa8Ou40dCpISPG6ncBcLZazMZ7Ey7yYyxsHH2pIE8zAzrToMyhBE6WEF3Gqb\niyG+r/TEPyyGlZ5qi+oHDANbPiXBgOs7PNjtPG6oc66h3UDf9/bNfxBJsOYxrGKqAjb26j0Dl4AW\nw7rOyObrlsP1ngGAJpP9HlbuWpvogiyv7VQP+VeB7q0ALKDe9AWfHlZAnIH18QtbAIA3cgysQHOP\nKTDD6h6SRcewJgNrdPCzirwRwHnHcS44jqMD+DMA72t/gOM4X3Ucp9j818cBHA/3x4wHUkNkWOkQ\nOtcjJRgQ0+BPUdMtbFd0oSptKI7MpGFYzj45Na8kOEqZk6i4tFXFqfksFiZTyGpyPBjW0CTB4QbL\ndMPqfDZUhpVKgv1suvyAblpFXHvCAl0f/Az5C1MpbJUbYx3SwatS6YbZpiWmfcPsSfsD1dqIuV5v\nlBo9K20ANyUYAHMXqxlAEqzIkveeBfGwTkQYZHm9WOu41+jWCsACehDli2HNuo8Rxar1+IUt3LY0\n1bdLuhtEV/HVTRupCAbWCW9gFXOtGAX4WUWOAbja9u/Xml/rhh8F8Nm2f3cAPEIIeZoQ8mH2HzE+\niDLRrh/8StHmPFO8OAZ/Chp0IKIkeLnDqSu3JFiVUR+jgmnDsnFlu4rV+SwIITiZz+JyDKptPGaG\nI3RJkgjkZhchXRN4kzL9YiWfxW7NCE1BQRlWluL4XvAkwSN8Q9+uNJBWJWQ0pe9jF6dSsJ2W9Hoc\nUQuZ7ZjrJAluDk488lalTSkhIjZK9Z4JwQAwPaEgpUjMkmAjgCRYbZMENwz+AcE93A3/Xmk3cxTa\nO1gplpsDK0+yMkWh5N9OMT2hQJGIEOtAy7/Kx64C7v1C5IG1YVhIR5AnQRVU46aeGyRC/asRQt4J\nd2D939q+/G2O49wLV1L8U4SQt3V57ocJIU8RQp7a3NwM88caGNLq8HoGi80b9Gyf0CXR5CftWNtx\nbxBHZ8RjWJe9U9dWWBB36FKEyYci4up2FZbteNUrq/NZXIqBJNhset8UDobVfR7Z18MaRdBDO2hS\n8MWQDgO2yjrmMiq3LPAgNNn9/UfZw7pV0T3GpB8Wki7Wpp8sWkkwlaDzsIWqwJJgy3ZQKOt9JcGE\nECxNp5mvMyOgJFi3bDiOAz0AU5tWZZi2EzrDvVFqwLAOd7DS7zmXUQMlBVN1yvxU/8M+keoGqX+V\nJ3CJIpdRhfhduiF6hnV89naDhp9V5DqAE23/frz5tX0ghNwN4HcBvM9xnC36dcdxrjf/fwPAx+FK\njA/BcZyPOo5zxnGcMwsLC/5/A4GQGmKYDpVA+fGwAqIOrJ1T+0TAcocghhpvD6sqw7AcYYM8wgaV\n/9KB6mQ+E4tqG93klxIC7gbZaGNYowxdAsLvYi2UG9yysE4YB0nwdkX37fmlUs5xrrapcx76dQM9\nsN3pIAnmYQupF13EtWqr4srJl/pIggG+LlbDsqFwdLACrR7qhun2UvLX2jSrQkI+5Lq+07mDlWJ5\nZiIUD6vfwytR2huC+lcBIJdNYa9uCru/iY5hbaYEj5F6btDw81d7EsCthJBVQogG4IMAPtH+AELI\nLQA+BuCfO45zru3rWULIFP1nAP8EwDfD+uFFwzC9idt+PayTtNhZvFP96zs1ENK7U25YmM+moEjE\n87VYtoOGafOFLo3ZSRwdWE81B6qV+Wwsqm3oDVfjkAQDbliTYdmeBDbq0KUTcxlIJNyBtVddBiuo\nJHiUQ5e2K7rv7kIq5dwcY4a11kwJDgspRUZGkz3FEdAWnjZitTY0YXqhD8MKuF2sXLU2nBt7+r6V\nm9UwvHaIqIJsaKXN8S6H48vTqUAe1kK5gdmM6vv9y09qQkiCn7i4jdNLk4EOKnPZwz5ykRB56NII\n39+Gjb6fJsdxTAD/CsDnAJwF8BeO47xICPkJQshPNB/2cwDyAD5yoL5mCcA/EkKeA/B1AJ92HOfv\nQv8tBEG6mWjnOIOXDxUrOjKa3PeDSAdaERnWG7s1LE6luG+SUUKSXFnVevMmRm+gfB5WWjA9Hgvb\nxUIFsxnV80Ku5JtMoOCy4LAkwVT+HbUkWFMkHJ8LLyl4q6yHzLCOgSS47H9gpf62ca62CTslGHDv\nce0+7nB6WMWTBNODjn6hS4AbvMRca2PzS4IpM027TLl7WJVoB9ZOkmAgHIaVJawun00NfU/m+le3\nA8mBAbFzUgD3/hPF4TFVioxTPsmg0T8ZAoDjOJ8B8JkDX/vttn/+MQA/1uF5FwDcc/Dro4q0KsN2\n3JsbLyvDi+2q3pddBdyeOkIgZLXN2k5dSDkwxZGZtHfqSo31GU5JMDBeDCsdUgF41TaXChW8/bS4\n8v+gkmAaPDIohhVw2euwGNbNcgNvC3Fg1cZAErxVaXg5Af2QVmXMTKhj7WENO3QJAOay6j52p8Ww\n8kuChWRYm1LyfqFLgCsJrugWyg0Tkylf2z7oJr8k+BDDyl1rE83h7rViDbms1jUc7chMGlsVvVnJ\nw359uuoU/2unCJLgF67voqoH868CYtvOAHffFcXhMZUZj1M+yaAhHpUVY6SGeMEWfUrRZIlgLjP8\nxbET1nZqQgYuUSzPpD0PK/Up8Gy2xnFgpXJgAF61zSXBk4Ipw8qz0QXcAW1frU3EHlYAWM1ncLFQ\nCazyaJgWSnUzXEkwHVhHVFlQ1U3UDRs5n741wB02xtrDqkcwsGa0/ZLgAD2sIkuC1/f8V6d4XawM\nLGsYkuBKw137eF8nqnulmxDcfa9BMyt41Q+b5YavvwtFPquhVB9u3WAY/lUgDgNrtAxr4mGNDsnA\nGiJSQxxEtquG7/qJOQFT3BzH9TSKWGlD4TKsNTiO4zGsvCnBwHjEn9d0Czd2617gEgCv2ibMztAo\nEMT7BuyXBMsSCS1ttxdW5rMoN0wvpZIXW2XaJxpF6JJ4m/8w0HrP/A/5i9OpsWZY65w5AL3gDqxt\nDCsNXeJgCxWPYRVPErxRqmMuo/pii6hsmGVgNW0nQOiS+7xywz044JYEq9RGELYkuNqz7z1oF2uh\nxMaw0nV2mDLaxy+4/lWWn7sTctR2JqCH1XEc1M2IPKzK+OzrhoVkYA0R6SEyCMWKjlyfShuKfDYl\nHMNarBpomLbQkuDlmQnUDRu7NcM7ReORBHtehxFlmtpBWdTVhey+r6/MZ3BZcA9rWJLgumFHkkrY\nCV5ScED2mg5fQTcv7aAHeqMaukQPAf1KggFgcSo93qFLerihS4B7ILvfw2pDlQkIYR++NIEZ1o29\nRt9KGwrKsLIwhrppB0pIB4BSPZgk2JNZhnivdBwH14t9GNbpwzV2flHVTVR0y1elDQVlJYcVhhmW\nfxVo97CKtccE3IMnx4lG7SRJBJoyvODVcUAysIaIqE4D/aBY1THrw8MKuIujaIuJyJU2FO2nrtVA\nkuDxKZimCcHtHlb671cEr7YJKglWZeJJgqPofeuE1eb7fDEge11oJlaysIX9MOq1NnSz6Td0CXDl\nnBulxlCC+njx2LlN/MrfvQzLDvYzU6VK2AzrbEbDXt301hbT4h+86PNMIRnWhq/AJYBPEmzaDlTO\nQVPxQpeagXMBU4LDlFkWyjoapt2TYfVq7DgY1kLJ3VsxhS7R9oaAyhheUP/qg6vBB1ZVljCVVoRT\n8QFoC0CMZvRJK9LYWL2GgWRgDRGpCE4D/cCwbJTqpu+NUm5SjJLqdpy9sQcA+7yOooHe9G/u1gOm\nBI+Ph/VgByvFSl78apugkmBVlmDatptKOCCG9fjcBBSJBJZb04GVZdPVD9q4SIIZPay6aWOvZkb1\nY4WOX/37V/CRL34Lv/iZs4Feh14H6RB7WIHWgcFOzZVXGlZwaasu4MHaxl7dN8M6mVKQ0WTP9+oH\nhmVDDRi6FDQl+EjTIhT0AK4d9J5zfC7T9TFTaRWTKWVf77pfbDbXznlGDyswPN8n9a8+eCqYf5Ui\nlxVvjwm09lxRHSBPaPJY7OuGhWRgDRHDYlipX8evhzXX9PjYAU/Iw8QzV3YwnVbwmoXJYf8oXdHO\nsLZSgv0lLrZjnAbWC5sVLE2nkD2QTNmSroorCzaakmCFm2GVYJhOZL1vnaDIEk7kMoElwQUOP2Y/\njLqHlW7QcgzvGQ1miUvw0vpeHc9d28Wx2Qn87j9exB89fpn7tbwwspATO2eb1pid5n1RDxIeJIkp\nCXYcB5tl/wwr4B64rjNcZ0YASbDqeVjdgZX3/V+cSuOWXAZPXd7men4nXCu695xulTYUyzNpLoaV\nSvxZa20ADM2q9cSFbdy6GNy/SpHLakL2sHqJ/VExrKo8Fsq5YSEZWENEaxAZ7M2NGvVzDJJg22md\nQIuAZ68Uce8tc5A4T3QHgYWpFCQC3NyteZJgHoZ1IgKZk6i4tFU5xK4CwEq+VW0jKoymJFjjDV2S\nCXTLRt2wByYJBtz39mIh2EHAVrmBjCZzHch0A30fR3lgTSkSsgyMIWXI4hK89PmzGwCAj/7gG/DO\n2xbw7z7xIr50bpPrtYIE1/UCrXejScGmZXN3KUsSgSwR4STBxaoBw3J8VdpQLE6lsMGSEmw7UAJK\nqYPW2gDAmZU5PH25GJps/nqfDlaK9ho7FnjqFIa/zfSEAkUi2CoPfh0I079KkctoQ5M390LUif0T\nasKwRolkYA0RLUnwYC9YerI/l/UZujQpVux4uWHi3HoJ952YHfaP0hOqLGFhKrWPYU1r7B+hcWJY\nLxYqWJ0/zJovTKWQEbzaJkgdBuAOaK4k2IrMM9MJK/NZXN4KVm1TKDdCZVcBNx06pUgj7GHVkc9q\nTOE+dFMbl+ClR86u40RuAncemcavf//9uHVxEj/1x8/g3HqJ+bXowW7YHtaDtRqG5UAN0ItOvegi\ngXpRqU3FD5am08ySYC2AugQAygFDlwDgzMkcCmU9NDXOtWIN02kF0+ne+6XlaT6GtVBugBA2Lzsh\nZGgy2m9e30UlhP7VdsyJyrA27+mReVhVGbUkdCkyJANriBgaw9pcGHx7WAXryXr+6g5sB7j/5Nyw\nf5S+WJ6ZwM29Omq6eyPmYaBatTajvbDtVHVsV3Sszh/2CsWh2sa0g0mCFZnAMB00Iup964bV+Syq\nuhWItduq6KEmBFOkFGlke1i3yg0mOTDQqhuJgyS4qpv4x/MFPHzHEgghmEwp+G8/9ADSmowf/r0n\nmYfuVpd1uJ+Ng5JgI0DoEuDKgkWrtaGfbRaGdWk6hfW9uu+DLNMKwrA2JcHN+2QQhcmZFXdf8NSl\ncGTBbgdrd/8qxfJMGhulOnMw4GapgbmMxnzN5bLaUCTBj19w39ew/KuA68ndrujChclFzbCm1SR0\nKUokA2uIoDfeQTMInnfKpySYSqa2hxShfhDPXt0BANx7XGyGFQCONE9dazq/F2JYTPyg0Qpc6uxL\nXhW82kYPIXTJsO3Iet+6YSWEpODNUoMpPMgvNEUeaUlwjvE9m0opSKsSU93IsPDlVwvQTRvfeceS\n97VjsxP4b//iDLYqDXz4D59iWtNqEW0eD0qC3fCgAAOrIgnHsFJpr9/QJcBlWBsMAV96COnKlRAk\nwa9dmMR0WsHTl4vcr9GOa8Vqz0obiuWZNGwHzJ3WhXID8xzqlPykNhRJ8OMXtvDaEP2rgMuwNkzb\ns06JAkomJZLgeCIZWEPE8Dys7oLqt9amJQkWw8P6zOUiXrOQxYzPHtlhggYx1AwLmixxnUBLkiuN\nHPWFrVtCMMVJwattjIA9rJosebU2YQfL9AJ9v4Ow14WyjgWGHkG/GAdJMAsIIVicSsfCw/rIS+uY\nSit4YHU/E3P38Vn83993H75xdQf/5i+f8x3m1wiQtN4LGU2GpkjefdEcQUmwx7AyhC4t0mobn2x+\nEEkw9QxXAoYuAe798sxKDk+FMLA6joNrxVpf/yrQHrLIlmS/WWow+Vcp8tnUwFVvLf9qeOwq0CJP\nRFHxUbQY1ugkwaO+rxsmkoE1RAyLOStWDUymFN83hZYkePibJMdx8OzVHdx/i/hyYMC9iZUaJjZL\njUBhIeMQf36pUIFEgFtyneVXq81qm7UdMeWQpm1DIoAcoBLDTQm2uXsIeXB0dgKaLOEipz/Yth1s\nVxrRSIJVCfpIM6zsQ/7iVEp4SbBlO/jCyxt4522LHQ9w3vX6ZfzMu27Hp56/gV975Jyv14yKYSWE\nYC6jelYZPUDoEuAOX8JJgvfqmE4rTO/dUnOI8tPFatkOHAfckmCteUBQCsHDCgBvODmH8xvlwP3x\nO1UDVd3yJwmedodaVh9rocxnpxiGJDgK/yrQ2mOK5mP1qrQiZFiTlODokAysIaJVazN4D6vfwCXA\n7USbTClDi1Bvx+WtKrYrOu6LycBKC8UvFsqBmIG0MvoL24VCBcfnMl0PUk42k4J5B6uoEUQSB7T3\nsA6WYZUlghO5CW6GtVjVYTtgZgv9IDWikuC6YaGqW1xBVQtTKeFDl75xtYitio6H71zq+pgPv+0U\nPvjACfz6F87jr5++1vc1o0oJBlxZcLskmDfpG3DZQREZ1kWGwCWgFdDkJ3gpjA5qoK3WJsD7DwBn\nmvkWQWXB12hC8CwLw8o6sDa4+qvzWQ2lujlQBYrnX10Nd2ClFYsi7DHb4fWwRpDz8eAAACAASURB\nVBS6lFLlgSssxwnJwBoihpkS7Ne/SjGXVYWQazx71b0B3X9SfP8q4CYHAq7cNSjDKkroUqlu4D9/\n7hU8eyUcjxCFmxDcWQ4MtKSrlwUdWE3LCTyw6qZbazPI0CXAfW8vcVbb0E0GS/G9X2iKNJIDK33P\neIZ8l2EVe2D9h5c2oEgEbz+90PUxhBD8h//+9Xjza/L4mY89j6f7dGdGlRIMNAfWkCTBikRg2mJd\nsxulBlPgEsAW8NUaWHnVJS1JcEqRmJKzO+GeE7NQZRJYFnx9x10T/XhYZzMqNEXCTYYqoErDRFW3\nuNbOfHPILQ7QqvXERde/yiNh7gWPYR3QHrNuWPitL34LV7d73/Pq5gBqbQTz7Y4SkoE1RBBCoCmS\n96EYFFyGlW2jlBuCX6ITnrm8g8mUglsXp4b9o/jCkRn3RlesGoE2WiJ5WP/2G2v4jUfP4/0f+Sr+\nh//6NXzh5XXfPrRucByn78BKq22ChANFCTddNJj3zbQd18M6wNAlwA1eurRV4fo7FprDUxShS25K\nsBjXfZigYSmsoUuA6y0s1U1h1oNOeOTsOh48lcPMRG8ljypL+K0feAPmMho+8ui3ej6WMqxRyOXn\nsi1JsBFQEuwePAkmCS7VmQfWjKZgKq34CviiEmh+htVdNyuNcCq90qqM1x+b6XsI0g+UYfUzsBJC\ncGSGrdrG62DllAQDwNaArFqmZePJi+H7V4HBNlEUKzr+2e8+gV/+u5fxG1843/OxXuhSRIqntDr4\n/f84IRlYQ0Z6CLUNPAxrfkidXwfx7NUi7jkxw+0THDTaQy5GxcP6xVc2cHxuAj/7T+/Ete0qfuT3\nn8K7/suX8FdPX+P2G26WGqjqFk4tdB9YabWNqEnBgeswmgxrw7QH2sMKuF2sDdNmYgcoCs11IbrQ\nJbHYqjBAGVYeDytlN0RNCr5YqOD8RhkP39FdDtyOmYyK9917FF96ddOrlukEykREwbDOZjTseJLg\ngEoJRRKKYXUcB+t7DaYOVgq3i7X/mmAGlQQ3Dwh0y4YW0nBw5uQcnru2G0gye61Yw2RK6XvwQsHa\nxUql/XwMa3NgZUwl5sU31/ZQ0a3Q5cAAMJ1WIEsk8j3m5a0KPvBbX8Xz13dxemkSn395o+chLb12\nosqUmFBlGJYjnIVgVJAMrCEjrcoDT8EsVtgZ1rnM8AfWqm7i7I0S7jsRD/8q4P59qewvyEZLlPjz\numHhK+e38B23L+JHv20Vj/30O/Fr33cPJELwb/7yObztVx7F73zpAkp1NpnShSZrSitWumElnxG2\nizXoRleRJa/LNUgPIQ9OBUgKjpZhlUcydGm7zC8JpgPrZlnM4KXPn10HAN8DKwC8955jMCwHf/fN\nm10fE2UnYi6juV5s2wmulJDESgneq5nQTZtLxkm7WPuBVnrxdlCrbQd0YR3WveFkDrpp45vXd7lf\n41qxhmOzE74lykdm0rix5z8lmDKsXLU2A2QlAbfOBgi3f5XCDT7TIg1deuZKEe//yFexU9XxJz/2\nIP7lO16LQrmB567tdH0OZVijOkBuNYUMf283ikgG1pCRHrDpum5YqOgW88l+ftJNpBtmsfML13Zh\n2Q7uuyUe/lUKGrwUhGFNC5Im9/WL26gZFt5x+yIA90T9/fcdx2f/p7fi93/4AazOZ/EfP3MWb/6l\nL+APvnrJ9+v2q7ShWJnP4mpRzGqboBvd9kqIgUuCm+87T6DVVqUBRSK+WQgWpNT+tTZV3cQvffZl\nb/MXB9BNJk/o0qLgDOs/vLSO25encKJL2ncnvP7YNFbyGXziubWuj6kZFhSJBDoU6obZjArbcVNq\nw1BKGAJJgqkHlTV0CQCWptI+Q5fc35c3LKl93QyLzXpDM3jpqUv8PtbrOzVfcmCK5ZkJrO82fFsr\nKMPKW2sDDC6o6PELW3jNQpapy5cFUar4/u6bN/Chjz6OqbSCj/3Lt+DMSg7vuG0BskTw+bMbXZ/X\nMKxQPNXdkNaGU205LkgG1pAxaG8ilT3NMUqCc1kN+pCLnZ+96p6ExSUhmIKmBwZnWIe/qD36ygZS\nioQ3HYi1J4TgHbct4k8//BD+9qfegjuOTOPnP/2S7xCFi4UKNEXC0T5pjCv5DAxLzGob03K4ax2A\n/ZUQgw5dWp5OI6VInAyrW88iRSDTT8n9JcGfeu4Gfvuxb+Fn/+aboX//qLBV0aHJEiZTCvNz6YZR\nxOClYkXHU5eLTOwq4K4f773nKL52YQsbXRi9mmFFIgcGWvfDYlWHYTncTCHQrKcSSBLsdbByDEWL\n02lslOp9D6rNoAyr1M6whvM3XphKYSWfCRS8dK1Y9dXBSnFkJg3dsrHtkyncLOsgBMwWLQCYnlCg\nSMTzw0eJQrmBr5wv4O2nFyP7HlEEezqOg9/98gX85B8/g9cdncbHfvLN3qH4bEbDmZNzeKSpCOmE\nqPMkJhKGNVIkA2vIGHRxMF0Q5jJsbMggTfHd8MzlIlbyGS7f1zCxHMLAmlIl1ARIk/viK5t482vy\nPRfxe07M4t+9504YloNPvXDD1+teLFSwks/09SZTyfAlAZOCw6i1oRhkrQ0ASBLBSj6LixxJwVsR\ndbAClGHtM7C+cAMSAT77zZv4rM/rbdjYKjeQy2pcJ/f5rAZZIkJ2sX7x3AYs2+lZZ9MN77nnKBwH\n+HSXv2HdsD1GImx497eqHrzWRhar1oZKevk8rCkYluNV/nSDHtDDKknEW/vDlF+eWcnh6ctFLmXY\nbs1AqW4yMaz0PfbrYy2UG8hnNa6DTkIIcgPKFvnzJ6/CsBx8/4O3RPY9wv5dLNvB//WJF/Hznz6L\nd71uGX/y4w95ycoUD9+xhJdvlrqmBTfMaBP76WsnA2s0SAbWkJH2sSHzA8t2YPmQodBQC+aU4Mxw\ne7Icx8GzV3dwf8zYVaCVFBwodGkIXueDuFio4GKhgnfe3v+U9c4j0zi9NIm/efa679fuJwcGWtJV\nEQdWd6MbjiQ4qpCHXliZz3C9r5tlnUva6gf9PKw7VR1fPV/Aj37bKl5/bBo/+7cv9gzuEQXbFZ37\n4E2SCOYnNSElwY+8tIGFqRTuPjbD/Nxbl6Zw+/IUPtlFFuyyHdF8LmabB7g7VR2mHbyeyrREkgTz\nM6ytLtbeA5gZUBIMuHVAALr2cPPgzMk5bFd0LyOBBde9Dlb/0naqpvI7sG6Wgh325bJa5Hsyy3bw\nJ09cwZtO5fHaxcnIvk8uq/U9GPGLqm7if/zDp/EHX7uMH3/rKn7z++/veMhOD9Y+34VlrRtWaIx/\nJ1ASQwS71ygiGVhDRkoJh2H9yT96Gh/86Nf6Po5KVVg3S7lJyrAOZ5N0rVjDZqkRO/8q0LrpB/aw\nDplh/eIrrtfjHT5kQYQQvP++43j6crFvb6plO7i8VfGG0V5YnEphQpW5O0OjRKiS4AEzrIB7GHBl\nq+rr4KsdW5zF937gpgR3v+7//sV1mLaD995zDL/8PXejWNXx858+G8nPEia2KsGG/OXpNF5c22P+\nW0WJhmnhsXObePiORW55+HvvPYpnrux0ZDxq+gAkwRUDhmkHlgTrAjGsG3sNZDUZWQ75+VIz5b7f\nwGoElAQDrWE3XIbVPeB+msPHen3Hf6UNBR1Yb/hMWy+UG4E6TfOTWuSS4C++soHrOzX8s4dORvp9\naPBZGGva//GxF/CFl9fx79/3Ovyf776z63q0Op/FqYUsPv9yZx9r1J3ordAlcdaLUUIysIaMtCoF\nvlgffWUDf//SOp68VOybiFf0JMHstTYAsD3Akup2xNW/CoTnYa0Z1lBDrx59ZROnFrK4Je/vxPl9\n9x4FIcDfPNs9SAVwT7INy/GSanvBrbbhYwKjhh64h7Xdwzr4gXU1n4Vu2Vjb8Z9y6TiOK2uLjGF1\nFSjdrvtPvXADJ3ITeP2xabzu6Ax+4u2n8FdPX8OXzm1G8vOEhe2KzpUQTPGDb1rBSzf28PsMwWZR\n44kL2yg3TGb/ajvec/dRAMAnnz+8ZtTNCAfWbMvDqo+YJHijVOcKXALa/NJ92PygkmCglRQcJqN1\nan4SsxkVT15i72O9VnQPTVg8rPnJFBSJ4OauvzU0KMOaz6YilwT/0eOXsTCVwj95Hf/n2g/mshoc\nx5ViB8FmqYFPPX8DP/TmVfzgm1b6Pv7hO5bw+IWtjs0GdTNaD2s6YVgjRTKwhoxUQKmnYdn4j58+\ni1tyGaQUCX/25JWej6cD5yy3h3U4DOszl4uYUGXcvjw1lO8fBNTDmgnYw2o7rTTGQaOqm3j8whbe\neZv/0IWjsxN4aDWPjz97reegfaFQBgCszvuTG63OZ4UcWM3AHtb2lOBhSILZ5dYV3ULdsCPzsGqK\nBKfLdU/lwO++66jnBf3X334rXrOQxf/+sRdQaZih/iybpQZ+4TNnUdWDv64rCeZ/zz5w/zF8++2L\n+E+fe9lL2B42Hjm7jrQq4S2vned+jRO5DO67ZRaffO6wj7WmW5HVPdEeyGJTEhyUYRVKErzX4JID\nA630Wr+S4CAHdlQSHKYdQpIIzpycw9McwUvXijWkVYnpYEmWCJam07jhQxJMD/t4Km0oopYEX92u\n4ovnNvGhB05Eks7djrByUv7y6aswbf9+24fvWIJhOfjSucKh/1Y3rEjVTomHNVokA2vISCvB0l//\n5IkrOL9Rxs/90zvx3Xcdwd8+u9ZTOlqs6phOK8yLz2RKgSqToXlYn726g7uPzwSSXA4LJ+Yy+M47\nl/DACn9/GZVJDesk7mvf2oJu2kwDKwC8/75juLRVxTeudu86u+Sz0obiZD6Lq9viVdsE7WFtf26U\nvpluWOXoYt3yegSjkgS770OnQz0qB373XUe8r6VVGb/8PXdjbbeG//S5V0L9WX7ri9/CR790AX/4\ntcuBXqduWCg3zECsNCEEv/iBu6DJEn76r57zXaMRFRzHwSMvreOtty4EZiTee89RnL2xh/MbpX1f\nr0eYEkwIweyEm1JqheBhHRWGNa3KmM2oWO8T8GWEwbBGIAkG3D7WC4UKs3T2OmMHK8XSdMqXh/Vr\nF7ZQN2zcvjzN9PrtyGc1lOpmZPkWf/zEFUiE4EMRhi1RhDGw2raDP/v6VTy4mvPtt73/llnMZtSO\nPtaGaUeaJ5GkBEeL+E0LgsNPz2A37FR1/Noj5/CW1+bxHXcs4oMPnECpYeIzPZIyecM+aCKd35qS\nMFE3LLy0thtLOTDgskS/84NncM8Jfv8t9b82hrSwPfrKBjKajAdW2f4G77prGSlFwsd7hC9dLFQw\nmVJ8nzSvzrvVNn5OsQeJoD2s6hBrbQDXH5zRZKakYNp9GpkkuPk+dApeapcDt+PMSg7/4k0r+IOv\nXcJTHFLATijVDfzFU1cBAL/z5YuBNhh0QxY07XxpOo2fe8/r8OSl4tClwS/d2MPabh3fGUAOTPHu\nu45AIsAnDrCsdcOObGAFXEkilb4GH1jFYFgdx8FGiZ9hBfx1sRoewxpcYRL2wOr5WBlZ1ms7VRyf\n8x+4RHFkZgI3fXhYf+8rl5DLanj33Uf6PrYbaOptMQKrVsO08BdPXcV33L7oBUdGCWpTCzKwfuVb\nBVzZrjKlGSuyhG+/bRFfeGXj0CF43bAjPTxOJwNrpEgG1pARhGH9L59/FXs1A//23XeCEII3ruaw\nOp/tKQsuVnXmhGCK3AD8Ep3w4touDMuJZeBSWKCylGEwrI7j4NGXN/GW184zL97TaRUP37mETz63\n1pV1uNBMCPZ7kn2yWW0jihSSwrDsgKFL7ZLgwTOsrj+YTW5dKLvrQXQMq/t+HkxS7yQHbsf/+t/d\nhqMzE/jpv34+lM3Anz95FeWGiX/77jtQKDfw509e5X6tsAZWAPiepjT4V4YsDX7kpQ0QAl8J4v2w\nOJ3GQ6fy+ORza/usBLUIU4IBt+qNJuoGO3giwjCs5YaJqm554Uk8WJxOde3GpWgxrMEP7MIeEO46\nNgNNlpj7WK8Xa0z+VYrlmTRu7vburr2yVcUjZ9fx/W+8JdBaT9eQrQisWp994Sa2K3rkYUsU9NCz\nGCDl/U+/fgVzGRXvev0y0/O+444l7FQNPHNlvxKsEfGa46UEC1BZOIpIBtaQkeZkWM9vlPGHX7uM\nD77xFtxxxGUYCCH4vgdO4MlLRZzfKHd83nZFZw5cosgPIEK9E569QgOXxndgpQzrMNLkzm+UcX2n\nxiwHpvjAfcdQrBp47JXOQTh+K20o6GP7pQ8PGoblBA5roRhGrQ3gstcskuDCwCTB+6/7TnLgdmRT\nCn7xA3fhwmYFv/6FVwP9DJbt4Pe/egkPrMzhx956Cg+szOG3H/tWz7qdXqBraJDQJQpCCH7h/XdB\nHbI0+JGz67jvxGygxNN2vOeeo7hYqODFtT3vazXDCpS03g+zGc3rth0VSXCr0oZPEgy4TH5/hlVc\nSXBalXHX8RkmtUWlYaJYNZgSgimOzKRR1S3s1bt73f/ga5cgE4J//qZgwyAd8rbK4e/L/ujxy1jJ\nZ/BtATzpLAjKsG6U6vj7F9fxvW84znzo8bbT81BlckgW7FZpDYBhDaHaMsFhJANryEgpMgzLX4dq\nO37hM2cxocr4X77z9L6vf8/9x6FIxJOvHcRO1eAeWOcGVFJ9EM9cKeL43ESgm27cQU/5hsGwPkrr\nbG5b4Hr+204vIJfV8PFvHJYFN0wL13dqTAMrrbZhka4OAmFKgofhYQWAlXwWVxj8wXSjFAZb2Ama\nx7Duv+67yYHb8bbTC/jeNxzHbz92AS+u9U5P74W/f/EmrhVr+NFvWwUA/NQ7X4sbu3V8/NlrXK9H\ng+sOltjzYnkmjX83RGnwjd0aXri+63UahoHvev0yVJngE22drFFvHnMZzVMMBK2nMixnqInuFFTi\nHEgSPJ3CZrnRc49CQ5eCvG9RSYIBt4/1m9f3fKstaKXNsVk+hhXo3sVabpj4iyev4rvvOuJV3vEi\nH1JQ0UGcvbGHpy4X8QMPnuSuqGJFWpWR0WTu3+Uvn7oG03bwoTey+22n0ioeOpXHPxwYWBumHcn1\nSOFlkyQMayRIBtaQwZMS9ti5TXzh5Q386+947SFmY2EqhYfvWMJfP32tIwPgeljZEoIp8kMaWJ+9\nsoP7Y+pfDQvpIUpHHn15E7cvT+Eox80bcAex99x9BI+8tI69A9HxV7aqcBz/gUtAq9pGRIY1PEnw\ncJbalfksTNvBtaK/WoZCuYGZCdUbLMMGvaG3r2XFSm85cDt+9t13IpfV8NN/9Tw36/X/fuUiTuQm\n8P+3d9/xVVf348df597cbMgGQhISwg4jAcIWCuJAAcEJylJx4KrWUduqrd9a29paf7UWF+IAFQvu\n4ioiKMgMOzIDZAcySEJ2bnI/vz/uveESMu4kF3w/Hw8ecJN7bz4kJ/ee9znv835fnmROM/tF3ygG\nx4TwyvqjThX+8kSQb5sa7MgOuTusPWBe0HLH+VWr0EBfJvSJYvWe/KZdY08HrKFBhqagzNelfqLm\nxza0EeBtOVbCbz/ex48ZxR7dFbfuGHdxISW4a2d/Gk0aRRWt77LWuzMl2AM/4+HxYdQ3mtjXTts/\nK2tLG2fOsHazBKEFrbS2+WhHLhV1Ddw2LsHh524uwlJp3N2Zb+9uycLPR8cNw2Pd+rztcbZOismk\n8cH2bEYnhpMYZV+xpeYm9+/CsaKqs45WePo1R6dT+Pno5Ayrh0jA6mbWX4bmKW+taWg08afV+4mP\nCGTB2IQW7zNrZBwlVfV822y1qKa+kRpjowtnWM0V6dpKhdM0jU1Hi9l8tITM4iqXfxELymsoKK/9\nWacDg001OQ9VA2xNRa2R7ZmnmOhkOrDVzKEx1DWY+HrfibM+fszBCsFWCRFBHPe6gNX1/o0ASuHS\n87jC+nOw93tbUlnvUluG9rSUEvy//SfaTAe2FRJo4JkZA/kp/zSv/3DM4a+/N7eM7Zml3Dq2J3rL\nToNSivsm9SazpJov2ihw15qSqnoMekVnfx+HH9uas1OD957X1OBvD5wkPiLQ7qqc9pqe3J388lp2\nZJfS0GjC2Kh5tuiSTeaRO1JbW2ttU1nXwK/+s5sV27KZ88ZWxv9tHS+sOUzOKfdnjDTtsLqwkzes\nRxhKwRsbWv/9sS7cuPK6ZV2w88Rr3/B484K3vf1Y8ywLds6kBFt3WFtqBWSyHC9IiQt1SxHJzgE+\n+OiUwxWQ21JRa+TTXXlMG9Ld6bmis5xt07Mxo5icUzXcMsr5FOvJlgU327Tg2gaTxxePA3z17c6T\njxZVuvVn/HPhvndYATi+w7piWzZHCit5de7wVtMGJ/SJonuIPx9Y0k6srIfZw11ICbY+T0upLMZG\nE7/7eB+rdpydKhcWaKBrZ3+iQ/zpFhJAdIg/Q3uEMr5P+ymm1vOrssNqCVjP8w7rjxnFNJg0JjmZ\nDmyVEhdKz8ggPtmVx00j4po+bl3NTHA0YI0MYu3BkzS4WOjInRoaTU29BJ1hnbD5++gdbqXgLgkR\nNq1t+rV//6LKOreltrbEepa3zubs9hf7TrSbDmxryqBorhrUjRe/PcLkAV0caiOxdONxgv18uCn1\n7J2GK5K60rdrMIvXZTB9SHeH0uZOVZrrCLj7Z9wtxJ/fT0visQ/38s7mTG4b19Otz9+SqroGNmWU\nMG9MvNv/P5cndcXfoOPz3flNdRo8GbDavi+6mhIM5l3HAM693n+uOcyJ07WsuHM0RZV1rErL4aXv\njvCvtUcYnRjOTalxXDUo2i3ndQsravE36Ojk5/zUbVBMCDeP7MFbmzK5dlgMA7uHnHMfo1tSgq07\nrO5/PY8I9iMxKogdmfYVXsotrcFXryPKide2Lp38UYoWq9h/f7iI48VVvDg7xeHnbYm1e4M7M98+\n3ZVHVX2jy+drnREW6OtU0aX3t2YTHuTLlQOdz/KICw+kf7dOrNl/kjvGJ2JsNNFo0jzahxXMr2lt\nHfVKzyvnhlc3ERcWyH8fuKRDCjJeqLxjZngRsQad9gSs5TVGXlhzmNGJ4W3+Yup1ihtT49hwpKgp\ntQXOnHNwdtXMel6ipQP+VXUN3PFOGqt25PLApb15745RPH9jMo9d2Y+pQ6KJDQugsKKONftP8MKa\nw8xbuo2v09vfndiVXYqvj65pwvJzZc8Oa0Ojia/TT1Bd33qxB0etO1hEJ38fhsW7tmCglGJmSgxb\njpeQX3YmVSqzuIrIYF9CAhxLU0+I8L7WNsZGDYMLqbHWCVtHpQMDRAb7EuznY3daaUllnVOTOntZ\nd1usZ1gdSQe29aeZg+gcYOChD3bbvTh4oryWL/YWcFNqHJ38zx6fOp15l/XwyUr+t//c/n1tKXGy\ntZg9bhgey6R+UTz3tedTgxsaTTy6ag/1jSauHuxYVU57BPn5MLl/V77cV0ClpYCNJ383QgPP/Izd\nkhLcQrr4T/nlvLUpk5tH9mBMrwiuSe7O8oWj+PHxS3n0ir4UlNfy8Mo9jHj2W37z0V6OFbVcPNEe\nGYWVfLIrj95dgl1eTHj8yv6EBhh44pP0FnfvjSbXU4J9PVR0ySo1Powd2aV2ZR/kltXQPdTfqfOb\nvj46IoNb7sX65o/H6drZ76yNBFc5uyvZEk3TeHdLNoNiOpMce+7ChKc5c+ys8HQt3x5wrthSc5MH\ndCEtq5Sy6vqmrB5PB4j+htY7hRRX1nHXsjQCDHqOFFbywprDHr2Wi40ErG52Zoe1/ZTgl9YeoazG\nyFPTktp9A7rRsiOwMu3MbmfTDqsLKcG2z2NVVFHH7Ne3sDGjmL9eN5hHrujHuN6R3DA8lvsm9eZP\nMwfzxoIRfPHL8aQ9eTn7/3glKXGhPLJyD0dOVrT0pZrszC4zl6X34MH3C8GZM6wtj5NaYyP3vLeT\nRe/uYNG7O91SpVLTNNYdKmRCnyiXUuSsrh0ag6bBZ7vPFFI5VlzVtKvnCOuO7Plq5VFrbGzzvKKm\nadQ3mlz6Pp2ZsHXcCqpSioTIQI6X2JeeWFxZ77EerGCzw2qZPDiSDmwrItiPv98whIMnKnj+m0N2\nPeadzZmYNK3Vs2ZTB0cTHxHI4nUZDhXYOVVV57Gqykop/nLdEI+nBjeaNB5euYev0k/w5NQBDI8P\n98jXmZ7cnZKqer47aD4n68nJo+1Cro/O9R3W5r1YG00aT3ySTliggcev7H/W57qHBnD/pX1Y/+hE\nVt49hqsGdePzPflc+/ImdmY71o4FzBXU57yxBVC8OHuo0/8Xq5BAA09OG8DunDJWtNA2z9hg6cPq\n0vfNWnTJMz/j1PhwyqqNHCtufxEgt7TGqfOrVtEh/ucspmYUVrDhSDHzRse75f3UKjEqiE0Zxe3O\npeyRllXKoZMVzB3l/owJezhT2HPVDueLLTV32YCuNJo01h8qalrY9HTFfv9WdljrG0zc8+4OTlXX\ns3zhKG4Z1YMlG46x7bh7eov/HPy8owYP8Gs6w9r2qv+xokre3pTJrNS4FlNymosNC2R8nyhWpeU0\nFZJo2mF1oa0NnH3A/3hxFde/somMwkqWzB/ObDteNAJ9fXh17nACfH24a/mOcwrxWNU3mIskDPuZ\nn18Fmx3WFl7YKmqN3PrWNtbsP8n05O78cLiIxz/c63KVyv0FpymsqHO6OnBzPSICGR4fxie7cpuu\nzdGWNlbWIPd8FF7afLSES55bx8yXf2xq49Kc9XfM4I6U4A7cYQXz99ae3bn6BhPlNUaPBV9wZvJq\nPTfvaDqwrUn9uzBvdDxvbDzOjxnFbd63ur6B97dmc0VSN+LCW564+uh13POLXuzLK+eHI20/n61T\nHtxhBXNq8FPTktiWeYp/rLEvOHeEyaTx+Ed7+XxPPr+e0o87xie6/WtYTewXRSc/n6aq955sa3PW\nGVY3ZEo0XzRcsS2b3TllPDk1iZDAljNKrP3U/35jMt88NIHQQANzlmzlh8MttwRrSV5ZDbcs2Up9\ng4n37hhFLyeL0DQ3MyWGMYkRPPfVwXMKMBkbTeh1yqWKsp5qa2M1PMF6TmVKoQAAG/JJREFUjrXt\nBYBv95/kYMFp4iOcD1i7dfY/Z4f1rR8z8fXRuSWwsvXUtCQCfPXctXwH5TUtz6XstXxzFp38fbgm\npbubrs4x4UG+VNc32p0FYzJprNiWzdheEU7NI5pLjg0lMtiPbw+cbLoGT6cE+xtaLrr09H9/Yntm\nKX+7IZlBMSE8cfUA4sICeXTVHqrq3JdFdzGTgNXN/JtSgtveEfvzlwfx89Hx8BV927yfrZtHxFFQ\nXssPR8xvdqVNAatzVYKtK9CnLJP2XdmlXP/KJirrGlhx12gu7W//+YFuIf68PGcYOaeq+dUHu1vc\nCdhfcJr6BpNbihNc6PxaaWtTUlnHLUu2kpZZyj9npfDSzUN55PK+fLwrj79+fdClr7ne0jf1F24K\nWMG8y3r4ZCX7C05TUWukqKKOnlGOv9F07eyHv0Hn0dY2JpPGy+szmPPGFoL99GQUVnLTa5vPSmm2\nsu6muCcluGPPqPSMDCK3tLrdPqPWBTDPBqxnUoKdTQe29burB5AYFcQjK/dQ1sZZqY935lFeY2Th\n+LbPgV43LJboEH/+7UCvV0+mBFvdODyWm0fGsXjdUZZuPO6259U0jSc/S+fDHbk8dFkf7p3Y223P\n3RJ/g54rBnZjd465loEnJ4+274uuLDxZ02JtA9aiijqe+/ogY3tFMMPOYCAuPJBVi8YQHxHIwne2\ns3pvfruPOXm6ljlLtnC61sjyhaPo162Tc/+JFiil+NO1g6g1mnj2i/1nfc5ocu38Ppx5/fNUNlVi\nZBDhQb6ktRKwNpo0/v7NQe5YlkbvLsHcN8n5sd0txJ8TNkWXyquNfLwzj5kp3d1+5j86JIBX5g4n\nt7SaBz/Y5XCLRKviyjq+Si/g+mGxBPp2TLma1rL4WrMho5jc0hq3LQLodIpL+0fx/eEiKi1Boad3\nWAMM5xZdendLFu9vzeaeib24Jtn8ehHk58PzNyaTU1rNs18e8Mi1VNU1OFX53ltJwOpm1l+Gts4m\nvrsli28PnOS+S3s71It08oCuRAT58sE2cwpPabURpXD4vKCVuVAInKo2svbASW5esoVgPx8+umcs\nKXGO74KO7BnO76cnsfZgIS+uPXfCt8uSCvVzL7gE5om7UmfvsOaV1XDjq5s5fLKC1+cPZ+bQGADu\nv7Q380bH89r3x1yarK47WMjgmBC39r+dOjgag17xyc48Mi3BZqITK6NKKRIigjy2w1pebeTOZWn8\n7etDTB3SndW/HM/yhaMoOl3Hja9uPudsmfUMlyuTNms6nSfaOjgiISIIkwY5pW0vBlh3mz2aEuxz\nJiXY2XRgWwG+el6cNZTiyjqe+CS9xSwEk0njzR+PMyQ2hNR2zm77+ui4e0Ii2zNL2XqspM37aprG\n6r35VNQ2eLSyMliCi5mDmTKwG8+s3s/HO53rGWtL0zT+77/7myZSD07u44YrbZ/tbo8nd1ht3xfd\ns8N6Zmw9+8V+6owmnpk5yKHFli6d/PnP3WNIjg3lgRW7eH/ruem4ViWVdcx5YytFFXW8c/tIBsW4\n/wxir6hgFv0ikU9355+VpWBs0Fyu7mvwcEqwUorh8WHsyDo3pbKkso75b25l8bqjzEqN46N7xjrd\nxg3MAWt5jbGpnsQH27OpMTZ6rBDaiIRwnr5mIOsPFfH8/5zLqvjP9hyMjRpzR5//YktW1iyHluqk\ntOT9rVlEBPly5UD3naG/bEBXKmob2HDYPL7P9xnWLcdKePrzn7i0fxceveLsyocje4Zz5/hE3t+a\nzfpDhW69jt05ZUz91wYWrzvq1uftSBKwupl1xbiuhR3WRpPGH/+7nyc/TWdC3yhud/DFztdHx/XD\nY1l7oJDCilpKq+sJCTA4XclPr1OEBhhYvSefO5el0bdrJz66Z6xLqRjzRsdz/bBYXlx7hDXNipfs\nzC6zVBZ2X8B0oVJKnbUSl1FYwQ2vbKKooo7lC0edtbutlOLpawZy1SDzZPXzPe2vzDdXVl3PzuxS\nl6sDNxcW5MvEfl34bE8+Ry1BX89I51LWPNXaZm9uGVNf2sAPR4r444yB/Gt2CsF+PoxICGfFXaOp\nNTZy02ub+Sn/TE8/o2U30pXdAYOPdcLWwSnBkTaVgttgDVg9ucNq/X7WGU0upQPbGhwbwq8u78sX\n+wr4ZFfeOZ///nARx4qqWHhJT7uCi9kjexAZ7Mu/12W0ep+MwkrmLd3G/e/vYmD3zlw3zPP9DfU6\nxT9npzAmMYLHPtzLdwcdKw5lS9M0/vLVQd7elMnCS3ry6yv7nbczbuN6RTTtvHhy8uij1zUFre5o\na2PdYd14pJhPd+ezaGIvp9JzQwIMLF84iol9o/jdJ/taPDNdVl3P3KXbyC2tZumtIzy6yHvvpN7E\nRwTy5KfpTe9HDSaTS0E+eLZKsFVqfBiZJdVnpTTvzC5l2ksb2Z5ZynPXD+a5G4a4PM6iLXOWE+W1\nNDSaWLY5i9GJ4R4tHjlnVDy3jOrBK+uP8l8H3/MbTRrvb81mTGKE29tTOcKRHVZzsaVCbkiNdeuu\n/CV9IvH10bHa0rLM0+/HtlWCc0urufe9nfSICOSfs1OaWqnZevjyvvTtGszjH+2lvNq1FHAwF4f7\n19ojXP/KJoyNGqMTPVOPoCNIwOpm/oYzKW+2TtcaWfjOdt788Ti3jUvgzQWpTr2IzhoRR4NJ4+Od\neeazU06eX7UKD/LlWHEVE/pGseLO0UR1cm2yqpTi2WsHMTgmhIf/s7spiAHzDqvsrp5hPZy/J6eM\nG1/djLFR44O7RzOy57kvMHqd4v/NSmFkz3AeWbmbjQ6csQP44UgxJg0m9net/2pLrhsaQ1FFHe9u\nyQJw+qxQfGQgOaeqnU6Bak7TNN7bmsUNr2xG02DVorHMH5Nw1sR8UEwIKxeNwVevY/brW0iz9PVr\nSgl2w0TXG1KCof2CVsWV1pRgz/dhPXm61uV0YFuLftGLEQlh/OGzn87pf7l0o7mS51WD7NvJ9Tfo\nuWN8IhuOFLPHkrpqVV3fwHNfH+SqF39gT24Zf5wxkM/vv8Sl3RtH+Bv0vD5/OEnRnbnn3Z1296Fs\n7oU1h3n9h2PMGx3Pk1MHnNeCLD56XVMVYk+f77amBbujPZWx0UStsZGnPksnPiKQeyf2cvo5A3z1\nvD4/lRkp3fn7N4f485cHmoLWilojC97cxtHCSl6fl8roxAinv449/A16npkxiOPFVbz2vbk3q9HF\nll7g+TOsAKmWc6w7sk6haRrLNmcy67XN+OgVH98zllkj3JNa2q2z+ff7RHkta/afJK+s5ry0mXp6\n+kBS48N47MM97M8/bddjrO97eWU1HdLKxlZ4kPn3z57CSyst9VludtPPzCrQ14dLekc2vZZ7+v3Y\nz3KGtbq+gbuW7cDYaGLJ/FQ6+7ecCelv0PPCTSmUVNbz+8/TXfraOaeqmf36Fl5Yc5hpQ6L58sHx\njPLw68f5JAGrm/m1UEwnu6Sa61/exMYjxTx77SD+MH2g07uivaKCGZkQzn+253Cqqt7lRtBTB0dz\n27gElsxPJciF3m62/A16Xp03HIOPjruWpVFRa6Swopbc0hqGSsGlJgEGPTuyyrhlyRaC/Hz4cNGY\nNgtw+Rv0LJmfSq+oYO5enkZ6Xnmr921u/cFCwgINJMe6//s/qX8XOvn7kJZVSkxogNNvCD0jgjA2\nai2eKXVUdX0DD6/cwxOfpDO2dwSrH7ik1TT3XlHBrLpnLFHBfsxdupX1hwqbdlNcmujqrH1YO/Zl\nNizQQEiAgcx2dq9LzsMOq0GvUAq+2FfgcjqwLb1O8cJNKWjAIyv3NC16HDxxmo0Zxcwfk+DQqv3c\n0fGEBBiadlk1TeOrfQVc9o/veWX9UWakxLDu0YnMH5PQ4qq5J3XyN/D2bSOICQ3g9re3c6DAvoms\n1Utrj/DSdxnMHhHH/10zsEOqh946NoEJfaOcqijuiFDLgq4rOza+NinBr35/lOPFVfxp5iCXJ74G\nvY7/d1MKC8bEs2TDcR77cC+na43c/vZ2fso/zctzhjGhr3szYlozoW8U05O7s3h9BseLq6hv0Fyu\nfOvplGAwLzj6+uj44UgxD/1nN7//7CfG94li9f3j3ZpCbd1hLSiv5a0fM4kLD+CyAc73CLWXr4+O\nl+cOIzTAlzuXpbUb+KXnlTPrtS38/rOfSIkL5fIkz19jW8KDzO8l7V13o0ljxbYcxvWOcLiHuz0m\nDzizUO/pgDXAoKemvpHHVu3lwInT/Ovmoe1mYgyKCeGXk/vw2e58vtjbfnvI5jRN46MduVz14gYO\nnajgxdkpvDh7qNPHBb2VXa9ISqkpSqlDSqkMpdRvWvi8Ukr9y/L5vUqpYfY+9mJjnZxac9i3Hith\nxuKNFFbUsez2kcwZ5fqK16wRcRwvriItq9TpCsFWD1/Rjz9MH+jWsuwAMaEB/PuWoWSWVPPIyj3s\nzDKfX5WCS2f4GXQcKDhNTFgAH90z1q4X6pAAA2/fNpLQQF9ufWubXWc+TSaN9YeL+EXfKI9Mrv0N\neqYNMQcerqSTx1smr+0FVm2pa2hk7YGTzFz8I5/uzuORy/vy5oIR7S7sxIQGsHLRGBIjg7lzWVpT\n2rUrE12lFAa96vAdVnNrm6CmM8atKa6sw9+gI9CD5wqVUvj56Cgor3VLOrCtuPBA/jhjINsyT/Ha\nD+ZzO29tzMTfoGPOKMdW7YP9fLh1bAJr9p/k6/QCFry1nXve20nnAAOrFo3h+RuTPRrYtyci2I9l\nC0cS5OvD/De3kW1H26L8shr++tVB/rHmMNcNjeHZawe7VAXWFb27dGLZ7SPdtkjaGmtKojsWno4U\nVvDyuqNck9yd8X3cE0jqdObjHg9O7sOHO3IZ/9w6dmSV8uLsoVx2noONp6YOwE+v46lP0zE2mlzq\nwQpn2gF5soWdn4+e5NgQ3t+azX/35PPYlf14Y35qq1WbnWU9xvTdwUK2ZZ5iwXlcqOrSyZ/X5g2n\nqLKO+97b2WIRncKKWh5btYfp/97I0aJK/nztYD66Z6zb53WOCgkwoJS5vc7RospWOx1sOFJkroY9\n0jM7wpNtjlh5OqvD36CnpKqeL/YV8Jsp/ZnUz76stnsn9iI5NoQnP91HYYX9/ejLq43cv2IXj6za\nQ1J0Z756aDwzUmKcvXyv1u5PTimlBxYDVwFJwM1KqaRmd7sK6GP5cxfwigOPvaj427S1WZmWw9yl\nWwkL9OXT+8YxtnekW77G1YOj6eTvQ32DqSnlwhuN7RXJ764ewP/2n+SZ1Qcw6BUDu3vuzMeFJjEy\niNT4MFbePYaune0/19stxJ93bh9Jg0ljwZvbWm3NYrU3r5xTVfVM8kA6sNVMywukKwFrTzvPWjZX\nXd/Al/sKeGDFLoY/8y0L30njVJWR5beP4oHJfeyelEcG+7HirtEkx4byd0tvT1f6N1of39FnWAF6\nRgS2mxJcUllPZLCfx3fcrDsu7koHtnXt0BimDonmhf8dZv2hQj7Zncf1w2Kbdtoccdu4BIJ89Sx6\ndye7skr5w/QkVj9wCSMSvONMUGxYIMsXjsTYaGLu0q0tTnKOF1eZd4T/vZGxf/2OV78/ysyU7vzt\nhiHnfWe4I4QGuuEMq+X3929fH8LPoOPJaQPccm1WSil+dXlfnp6ehLHRxPM3JjN1iHsyDxzRpbM/\nj03px8aMYr4/XOSGHVbPpwQDXDmwG106+bHs9lHcN6m3RxZh/A16QgMNfLGvgEBfPTemxrn9a7Ql\nOS6Uv1w7mM3HSs6qKFtrbGTxugwm/X09n+7O487xiax7bCK3jOrhFb/fep1iSEwIX+wtYPI/vmfY\nM2tY+PZ2Fq/LYPPRkqYiVu9vzSYy2NdjO8LdQvwZbNlx93RfdGvLwhkp3blrgv0twnz0Ov5xUwrV\n9Y389qN9drUx3HS0mCkv/sA36Sd47Mp+rLhrtEv9hr2dPcubI4EMTdOOASilPgBmALZ10GcAyzTz\nd3iLUipUKRUNJNjx2IuK9cV5+ZYsck7VcEnvSBbfMsytK34BvnpmpsSwfEuWyynBnnb7uAT25Zbx\n6e58kuNCO3y3yZu8Ni8VncKpSXvvLsEsXTCCOW9s4ZqXNjIkNpTYsADiwgOb/o4JDSDIz4d1BwtR\nCia4aVegJSMSwrllVA+X+r116WRubZNpx27R6VpzZeuv00/w/eEiao0mwoN8mTYkmimDujG2V6RT\nK/shAQaWLRzJond38sPhIgJ8XZtsBfnpCfbvmJYCthIig/hsTz6//XgvdQ0m8x+jibqGxqbbxwor\nSTwPBTqsPxd3pQPbUkrx7MxB7MgsZeE7aTSaNG6/xLmzZqGBvvx+ehJ7c8t5cHIfujiwqHS+9Ona\niTdvHcGcJVtZ8OZ2PrhrNPllNXyVfoJv0k9w6GQFAMmxIfx6Sj+mDOxGopv6eF4IwtyYElxeY+SZ\nGQPdWmXd1q3jejKvA1LMbc0ZFc+HO3LZm1tOjIvnsn2bUoI9G7AuvKSn3QXVXNGtsz9l1UZuGB7b\nIamW1w+PJT2/nLd+zCQpujNBfj78+csD5JbWcHlSV564eoBH0mld9cm94zhWXMmOrFJ2ZJWyM7uM\ntQfNFXH1OkVSdGf2F5zmzvGJHt2Nv2xAV/bllRPk59k56MR+UZw4XcufHKwgDuZ53eNT+vPH1ftZ\nlZbLTSPiMJk0TlbUknOqhpxT1eSUVjf9e3vWKXpGBPHxvWMZ4oHjXt7GnplUDJBjczsXGGXHfWLs\nfOxFxUevw0enyDlVw7zR8fx+epJH0jJmjYhj+ZYsojowLc0eSin+ct0Qiis9u8N3IXJ1YjI8Poyl\nC0awZMMxjhRWsO5QIXXNem2GB/mae9/GhXp0cUOnU/z52sEuP0dCRBCf78nnSGFlq/errW9kV04p\nxkaNrp39mJUax5RB0YxICHP6bLitQF8f3pifylfpBYzt5VpWxKtzh3vFiufYXpG8uyWbbw8U4uej\ns/zR42cw/zs0wMCYXhFMS/Z8g3k/H53b04FthQb68vyNycxdupVJ/aKcquRqNWtED2aNcOPFecCw\nHmG8Om84d7yzndF/XkuNsRGlzItIf5iexBUDu7kcfFyo3JISbAm8kmNDuMUNR3ra0tG7Ynqd4tmZ\ng5mxeKPLKcEBvj7olGdbF4FzC77OiA7x5+CJChaMTTgvX68lT1w9gEMnKnjsw70A9O/WiffuGMU4\nN2XveYJOp+jdpRO9u3RqKoJVVl3PruwySwBbSlxYgMPHNhx154SeJHXv7LEFJ6vUhHBSXcjCsR5F\n+cPnP/HK90fJK62h3iYNXCno2smfuPAA7p7Qi19O7t1hfXbPN9XetrNS6gZgiqZpd1huzwNGaZp2\nv819VgN/1TRto+X2WuBxzDusbT7W5jnuwpxOTI8ePYZnZWW5/r/rIE99mk7/6E5uOa/alnUHCxna\nI9SpdDdx8dE0jaLKOnJLa8gtNa/A5ZbWkF9mbsQ9ZZD7ept5yjubMltsTWJLp8zB+pRB0QyNC+2w\nc3jCOYvXZRAbFuDxczabjhbTt2unDj1rej59nV7AxzvzmNS/C5cN6OpyxfeLwYGC07z9YyZ/uc75\n87q1xkYeXrmbByf3pV+3Tm6+Qu+0dONxFDidnQDmHem9uWVuO+/b0b7cV8DhkxU8dFnfDr2OU1X1\nPPHJPsb1jmT2iDi3LNIK75JXVsNvPtpL5wADcWGBxIUHWP4OpHuov8fTms8npdQOTdNS7bqvHQHr\nGOBpTdOutNz+LYCmaX+xuc9rwHpN01ZYbh8CJmIOWNt8bEtSU1O1tLQ0e65fCCGEEEIIIcQFxJGA\n1Z6lme1AH6VUT6WULzAb+LzZfT4H5luqBY8GyjVNK7DzsUIIIYQQQgghxDnaTXzWNK1BKXU/8A2g\nB97UNO0npdQiy+dfBb4ErgYygGrgtrYe65H/iRBCCCGEEEKIi0q7KcEdQVKChRBCCCGEEOLi5O6U\nYCGEEEIIIYQQ4ryTgFUIIYQQQgghhFeSgFUIIYQQQgghhFeSgFUIIYQQQgghhFeSgFUIIYQQQggh\nhFeSgFUIIYQQQgghhFeSgFUIIYQQQgghhFeSgFUIIYQQQgghhFeSgFUIIYQQQgghhFeSgFUIIYQQ\nQgghhFeSgFUIIYQQQgghhFeSgFUIIYQQQgghhFeSgFUIIYQQQgghhFeSgFUIIYQQQgghhFeSgFUI\nIYQQQgghhFdSmqZ19DWcQylVBGR14CVEAsUd+PWFcISMV3GhkTErLjQyZsWFRMaruBDEa5oWZc8d\nvTJg7WhKqTRN01I7+jqEsIeMV3GhkTErLjQyZsWFRMaruNhISrAQQgghhBBCCK8kAasQQgghhBBC\nCK8kAWvLXu/oCxDCATJexYVGxqy40MiYFRcSGa/ioiJnWIUQQgghhBBCeCXZYRVCCCGEEEII4ZUk\nYLWhlJqilDqklMpQSv2mo69HiOaUUnFKqXVKqf1KqZ+UUg9aPh6ulFqjlDpi+Tuso69VCCullF4p\ntUsptdpyW8ar8FpKqVCl1IdKqYNKqQNKqTEyZoU3U0r9yjInSFdKrVBK+cuYFRcTCVgtlFJ6YDFw\nFZAE3KyUSurYqxLiHA3AI5qmJQGjgfss4/Q3wFpN0/oAay23hfAWDwIHbG7LeBXe7EXga03T+gPJ\nmMeujFnhlZRSMcAvgVRN0wYBemA2MmbFRUQC1jNGAhmaph3TNK0e+ACY0cHXJMRZNE0r0DRtp+Xf\nFZgnUjGYx+o7lru9A8zsmCsU4mxKqVhgKvCGzYdlvAqvpJQKASYASwE0TavXNK0MGbPCu/kAAUop\nHyAQyEfGrLiISMB6RgyQY3M71/IxIbySUioBGApsBbpqmlZg+dQJoGsHXZYQzf0T+DVgsvmYjFfh\nrXoCRcBbljT2N5RSQciYFV5K07Q84HkgGygAyjVN+x8yZsVFRAJWIS5ASqlg4CPgIU3TTtt+TjOX\n/pby36LDKaWmAYWapu1o7T4yXoWX8QGGAa9omjYUqKJZKqWMWeFNLGdTZ2BebOkOBCml5treR8as\nuNBJwHpGHhBnczvW8jEhvIpSyoA5WH1P07SPLR8+qZSKtnw+GijsqOsTwsY44BqlVCbmYxaXKqXe\nRcar8F65QK6maVsttz/EHMDKmBXe6jLguKZpRZqmGYGPgbHImBUXEQlYz9gO9FFK9VRK+WI+sP55\nB1+TEGdRSinMZ6sOaJr2gs2nPgcWWP69APjsfF+bEM1pmvZbTdNiNU1LwPya+p2maXOR8Sq8lKZp\nJ4AcpVQ/y4cmA/uRMSu8VzYwWikVaJkjTMZc30LGrLhoKHOWgABQSl2N+byVHnhT07RnO/iShDiL\nUuoSYAOwjzNnAn+H+RzrSqAHkAXcpGnaqQ65SCFaoJSaCDyqado0pVQEMl6Fl1JKpWAuEuYLHANu\nw7zAL2NWeCWl1P8BszB3EtgF3AEEI2NWXCQkYBVCCCGEEEII4ZUkJVgIIYQQQgghhFeSgFUIIYQQ\nQgghhFeSgFUIIYQQQgghhFeSgFUIIYQQQgghhFeSgFUIIYQQQgghhFeSgFUIIYQQQgghhFeSgFUI\nIYQQQgghhFeSgFUIIYQQQgghhFf6///nybxooG2QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116e55198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4 / 20\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.2266, -1.5942, -0.1541, -0.8899, -1.8308,  0.2310, -1.0186,  0.0474,\n",
      "         -0.1291,  0.6669, -0.0990,  0.3174,  0.3886, -0.4885,  0.6059, -0.4315,\n",
      "         -0.8666,  0.6080, -1.3493,  0.8139]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(681490707193528320., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 4 [0/32 (0%)]\tLoss: 681490707193528320.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.1046,  1.3039, -1.9842,  0.1336,  0.1296,  0.9234, -0.1266,  0.7457,\n",
      "         -0.0844,  1.2281, -0.7472, -1.1577, -1.8503, -0.9693,  1.0586,  0.8572,\n",
      "          0.2972, -0.7891, -0.4077, -0.9456]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(5961383270766608384., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 4 [1/32 (3%)]\tLoss: 5961383270766608384.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.8394, -0.6031, -0.3794, -0.7629, -0.2041, -0.0040, -0.6758,  1.3131,\n",
      "         -0.4572, -0.0775,  2.4777,  0.3197,  1.0438,  0.4706,  1.8112,  0.7097,\n",
      "         -1.1310,  0.4103,  1.0798, -0.6805]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(673125930893312., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 4 [2/32 (6%)]\tLoss: 673125930893312.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.6483,  1.0252,  0.6751, -0.9360, -0.5511,  0.8949, -0.2503, -0.0268,\n",
      "          0.4151, -0.4319,  0.5684,  0.6592,  0.5134, -0.5145,  0.7546,  0.5538,\n",
      "         -1.3924, -1.2108, -1.5772, -0.0126]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(10166051525068062720., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 4 [3/32 (9%)]\tLoss: 10166051525068062720.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-3.4116,  0.3097, -0.9521, -0.3601, -0.1367,  0.4302, -2.7053, -0.3564,\n",
      "         -0.6674, -0.7547, -2.0762,  0.8825,  0.8879, -1.0081, -0.9868,  0.5208,\n",
      "         -1.8237,  0.6878, -0.2283,  1.3413]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(129342009107569180672., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [4/32 (12%)]\tLoss: 129342009107569180672.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.0552, -0.7533,  1.6851, -1.9903,  1.1741, -0.9913, -0.9880,  2.1400,\n",
      "         -0.1001, -0.4537,  2.4071, -0.4098, -0.2991, -1.4207,  0.1579,  1.9779,\n",
      "         -1.1010,  0.6516, -0.1848,  0.6668]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(4799747036998008832., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 4 [5/32 (16%)]\tLoss: 4799747036998008832.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.6220,  0.3231, -0.3872,  1.0517, -0.8052,  0.5412, -1.1826, -0.1115,\n",
      "         -1.2770, -0.5754, -0.1722, -2.1444, -1.6262,  1.3246,  0.9250,  0.9841,\n",
      "          0.0372, -0.1531, -0.7823, -1.6365]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(1026129259665555456., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 4 [6/32 (19%)]\tLoss: 1026129259665555456.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.8676, -0.2177,  1.2232, -0.3461,  1.9785,  2.1864, -1.7063, -1.2166,\n",
      "          0.5798,  1.1778,  1.8070,  0.6288,  0.5367,  0.4416, -0.8231,  0.9254,\n",
      "         -1.2830,  0.0916,  1.5236,  1.0718]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(9442163477708800., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 4 [7/32 (22%)]\tLoss: 9442163477708800.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.0632,  1.2234, -0.1779,  0.9873, -0.9142, -0.3874, -1.2944,  0.3854,\n",
      "          0.5294,  0.8002, -1.5248,  0.3676, -1.2754,  0.3368,  0.4620, -0.9711,\n",
      "         -0.4686,  0.7865, -2.2486, -0.3679]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(11413887271431045120., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 4 [8/32 (25%)]\tLoss: 11413887271431045120.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.9345, -0.7487,  0.7814, -0.5784, -0.7281,  0.3927, -0.2947, -0.3152,\n",
      "          0.1193,  1.1422, -1.2355,  0.8380,  1.7692, -1.8345, -1.1366, -0.5497,\n",
      "          1.1984, -1.5372,  0.0284, -0.7259]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(20418227795939819520., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [9/32 (28%)]\tLoss: 20418227795939819520.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.5912, -1.4036, -1.0526, -0.5394,  0.5053,  1.4775, -0.7651, -0.1414,\n",
      "          1.0974, -1.0419, -0.1885,  0.0024,  0.7029,  0.6877, -0.4336, -0.1557,\n",
      "          0.2183, -0.6669, -1.2329,  1.1469]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(171314803644563456., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 4 [10/32 (31%)]\tLoss: 171314803644563456.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.3164, -1.2263, -0.1977, -0.8083,  0.7889,  1.4378,  1.0199,  1.2445,\n",
      "          2.2274,  0.6727, -1.6966,  0.9764, -1.5061, -0.1632, -0.2355,  0.5709,\n",
      "         -0.8506,  0.4495, -0.0610, -0.7730]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(431491278052524032., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 4 [11/32 (34%)]\tLoss: 431491278052524032.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.9035, -0.3221, -1.4659,  0.3750,  1.0276,  0.0834,  0.5455, -1.2519,\n",
      "          0.0916,  0.5986, -0.8216,  1.0409, -0.1546,  0.6716, -0.2048,  1.5740,\n",
      "          0.3720,  0.9914,  0.0751,  2.3653]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(1386609781829009408., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 4 [12/32 (38%)]\tLoss: 1386609781829009408.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.7257,  1.3059,  2.5764, -0.4949,  0.0786,  0.7964, -0.4937,  0.0257,\n",
      "         -1.2397, -0.5760,  1.7019, -0.8722,  0.9883, -0.9714, -1.2992,  0.4352,\n",
      "          0.8632,  0.0539,  0.2919, -0.7491]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(56907392532476854272., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 4 [13/32 (41%)]\tLoss: 56907392532476854272.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.5259, -0.8640, -0.4857, -1.2175, -0.5768, -0.3966,  0.5489, -0.1641,\n",
      "          0.7707, -1.0957,  0.4268,  1.0912,  0.3246, -0.7441, -0.4191,  1.6295,\n",
      "         -1.3118, -0.4360,  1.3987, -0.0211]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(201689698293369536512., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [14/32 (44%)]\tLoss: 201689698293369536512.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-1.5023, -0.8046, -0.5587,  0.3246, -0.2644,  0.1655,  1.9527,  2.2979,\n",
      "          0.3190, -0.0558,  1.2838, -0.6920,  1.4589, -0.6589,  0.0655, -1.7643,\n",
      "          0.1564,  0.8710, -0.3905,  0.0410]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(144827926697352364032., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 4 [15/32 (47%)]\tLoss: 144827926697352364032.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.4738, -0.1281,  0.1043, -1.0483,  0.9483, -0.2132,  0.6649, -0.1783,\n",
      "          0.2704, -0.3344,  1.8263, -0.4650, -0.1707, -1.0233,  0.2335,  0.5444,\n",
      "         -0.6830, -0.7524,  0.3275,  0.2725]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(119241552247190978560., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 4 [16/32 (50%)]\tLoss: 119241552247190978560.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 1.6086,  1.9923,  0.8082, -0.5514, -0.4815, -0.1911, -0.0232,  0.5049,\n",
      "         -1.6826, -1.0450,  0.9174, -0.4224, -0.3078,  0.1296,  0.7005, -0.7701,\n",
      "          0.4174, -1.3581,  0.1674, -0.2872]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(1425600938051108864., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 4 [17/32 (53%)]\tLoss: 1425600938051108864.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.7893, -1.2863, -0.1153, -0.3183,  1.3153,  1.3441,  0.5254,  0.0711,\n",
      "         -0.1555, -0.6653,  1.7686, -2.7827,  1.1620,  0.7034,  0.2474,  0.5883,\n",
      "         -1.0183,  0.3735,  0.8776, -0.3520]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(2520466579443941376., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n",
      "Train Epoch: 4 [18/32 (56%)]\tLoss: 2520466579443941376.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[ 0.5854,  0.1732, -1.0130, -1.2401, -0.0456, -0.4325, -0.0359, -0.1521,\n",
      "         -0.0534, -0.7683,  1.2925,  2.2624,  0.4428, -0.7507, -0.1451, -0.5217,\n",
      "          0.9860,  0.8732,  0.9025,  0.3637]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(6439071595012030464., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [19/32 (59%)]\tLoss: 6439071595012030464.000000\n",
      "Starting Encoding\n",
      "----------------------------\n",
      "Encode - Forward Pass Finished\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "----------------------------\n",
      "logvar (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (after encoding) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "Reparametrization...\n",
      "logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "mu (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<DropoutBackward>)\n",
      "0.5*logvar (in reparametrization) = \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], grad_fn=<MulBackward>)\n",
      "std (in reparametrization) = \n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]], grad_fn=<ExpBackward>)\n",
      "eps (in reparametrization) = \n",
      "tensor([[-0.1003,  0.7517, -0.4364,  0.1266,  1.3024, -0.1955,  0.3335, -1.1141,\n",
      "         -1.9340, -0.8481,  0.1129,  1.2946,  0.5629,  1.8669, -0.9879,  1.1685,\n",
      "         -0.2688, -0.3595, -1.4230,  1.2693]])\n",
      "----------------------------\n",
      "Starting Decoding\n",
      "Last ReLU output shape = torch.Size([1, 1, 128, 128, 128])\n",
      "--------------------------------------\n",
      "Calculating Loss...\n",
      "MSE Loss = tensor(92985538641920., grad_fn=<MseLossBackward>)\n",
      "KLD Loss = tensor(-0., grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    loss_history = []\n",
    "    #for epoch in range(1, args.epochs + 1):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(\"Epoch = \" + str(epoch) + \" / \" + str(epochs))\n",
    "        \n",
    "        train(epoch)\n",
    "        \n",
    "        # Plotting Training Losses\n",
    "        plt.figure(figsize=(16,8))\n",
    "        plt.plot(loss_history)\n",
    "        plt.show()\n",
    "        \n",
    "#         test(epoch)\n",
    "        \n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             sample = torch.randn(64, 20).to(device)\n",
    "#             sample = model.decode(sample).cpu()\n",
    "#             save_image(sample.view(64, 1, 28, 28),\n",
    "#                        'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
