{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code taken from: https://github.com/pytorch/examples/tree/master/vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modify to accept hard coded arguments\n",
    "batch_size = 4\n",
    "epochs = 2\n",
    "no_cuda = False\n",
    "log_interval = 10\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "# kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HydrogenDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, h5_file, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h5_file (string): name of the h5 file with 32 sampled cubes.\n",
    "            root_dir (string): Directory with the .h5 file.\n",
    "        \"\"\"\n",
    "        file_size = os.path.getsize(root_dir + h5_file) / 1e6 # in MBs\n",
    "        print(\"The file size is \" + str(int(file_size)) + \" MBs\")\n",
    "        \n",
    "        # self.subcubes = h5py.File('../data/sample_32.h5', 'r')\n",
    "        self.subcubes = h5py.File(root_dir + h5_file, 'r')['sample32']\n",
    "        self.h5_file = h5_file\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        # Function called when len(self) is executed\n",
    "        \n",
    "        #print(len(self.subcubes))\n",
    "        return len(self.subcubes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        This can be implemented in such a way that the whole h5 file read \n",
    "        using h5py.File() and get_sample() function is called to return\n",
    "        a random subcube. This won't increase memory usage because the\n",
    "        subcubes will be read in the same way and only the batch will\n",
    "        be read into memory.\n",
    "        \n",
    "        Here we have implemented it so that it can be used with data\n",
    "        generated by get_sample() function.\n",
    "        \n",
    "        The output of this function is one subcube with the dimensions\n",
    "        specified by get_sample() implementation.\n",
    "        \"\"\"\n",
    "        \n",
    "        sample = self.subcubes[idx]\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file size is 268 MBs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.HydrogenDataset at 0x11c64a588>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_subcubes = HydrogenDataset(h5_file=\"sample_32.h5\",\n",
    "                                    root_dir = \"../data/\")\n",
    "sampled_subcubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "train_loader = DataLoader(\n",
    "        dataset=sampled_subcubes,\n",
    "        #batch_size=args.batch_size, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, \n",
    "        **kwargs)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "        dataset=sampled_subcubes,\n",
    "        #batch_size=args.batch_size, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, \n",
    "        **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The Encoding Layers\n",
    "        nn.Conv3d \n",
    "        nn.MaxPool3d \n",
    "        \n",
    "        out_channels is the number of different filters we convolute \n",
    "        over the whole sampled subcube.\n",
    "        \n",
    "        So the first convolutional layer's in_channel should be 0 (?)\n",
    "        \n",
    "        In addition, the next layer's in_channel should be equal to\n",
    "        the previous layer's out_channels (all examples show that\n",
    "        this is the case)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.encode_group1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=1, \n",
    "                      out_channels=8, \n",
    "                      kernel_size=(4,4,4),\n",
    "                      stride = (2,2,2),\n",
    "                      padding=(1,1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), \n",
    "                         stride=(2, 2, 2)))\n",
    "        #init.xavier_normal(self.group1.state_dict()['weight'])\n",
    "        \n",
    "        self.encode_group2 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 8, \n",
    "                      out_channels = 16, \n",
    "                      kernel_size=(4,4,4),\n",
    "                      stride = (2,2,2),\n",
    "                      padding=(1,1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), \n",
    "                         stride=(2, 2, 2)))\n",
    "        \n",
    "        self.encode_group3 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels = 16, \n",
    "                      out_channels = 32, \n",
    "                      kernel_size=(4,4,4),\n",
    "                      stride = (2,2,2),\n",
    "                      padding=(1,1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), \n",
    "                         stride=(2, 2, 2)))\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Fully Connected Layers after 3D Convolutional Layers\n",
    "        First FC layer's input should be equal to \n",
    "        last convolutional layer's output\n",
    "        8192 = 8^3 * 16 \n",
    "            8^3 = (output of 2nd convolutional layer)\n",
    "            16 = number of out_channels\n",
    "        \"\"\"\n",
    "        \n",
    "        self.encode_fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=512, \n",
    "                      out_features=2048), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        #init.xavier_normal(self.fc1.state_dict()['weight'])\n",
    "        \n",
    "        self.encode_fc2 = nn.Sequential(\n",
    "            nn.Linear(in_features = 2048,\n",
    "                      out_features = 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        #init.xavier_normal(self.fc2.state_dict()['weight'])\n",
    "        \n",
    "        \"\"\"\n",
    "        The last fully connected layer's output is the dimensions\n",
    "        of the embeddings?\n",
    "        \"\"\"\n",
    "        self.encode_fc3 = nn.Sequential(\n",
    "            nn.Linear(in_features=2048,\n",
    "                      out_features=32))\n",
    "        \n",
    "        \n",
    "        # Grouping convolutional and fully-connected layers in Encoding\n",
    "        self._encode_conv = nn.Sequential(\n",
    "            self.encode_group1,\n",
    "            self.encode_group2\n",
    "        )\n",
    "\n",
    "        self._encode_fc = nn.Sequential(\n",
    "            self.encode_fc1,\n",
    "            self.encode_fc2\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        The Decoding Layers\n",
    "        nn.Conv3d -> nn.ConvTranspose3d\n",
    "        nn.MaxPool3d -> nn.MaxUnpool3d\n",
    "        \"\"\"\n",
    "        \n",
    "        self.decode_fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=32,\n",
    "                      out_features=2048))\n",
    "        \n",
    "        self.decode_fc2 = nn.Sequential(\n",
    "            nn.Linear(in_features=2048, \n",
    "                      out_features=2048), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        #init.xavier_normal(self.fc1.state_dict()['weight'])\n",
    "        \n",
    "        self.decode_fc3 = nn.Sequential(\n",
    "            nn.Linear(in_features = 2048,\n",
    "                      out_features = 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        #init.xavier_normal(self.fc2.state_dict()['weight'])\n",
    "        \n",
    "        \n",
    "        self.decode_group1 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(in_channels=32, \n",
    "                      out_channels=16, \n",
    "                      kernel_size=(4,4,4),\n",
    "                      stride = (2,2,2),\n",
    "                      padding=(1,1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxUnpool3d(kernel_size=(2, 2, 2), \n",
    "                         stride=(2, 2, 2)))\n",
    "        #init.xavier_normal(self.group1.state_dict()['weight'])\n",
    "        \n",
    "        self.decode_group2 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(in_channels = 16, \n",
    "                      out_channels = 8, \n",
    "                      kernel_size=(4,4,4),\n",
    "                      stride = (2,2,2),\n",
    "                      padding=(1,1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxUnpool3d(kernel_size=(2, 2, 2), \n",
    "                         stride=(2, 2, 2)))\n",
    "        \n",
    "        self.decode_group3 = nn.Sequential(\n",
    "            nn.ConvTranspose3d(in_channels = 8, \n",
    "                      out_channels = 1, \n",
    "                      kernel_size=(4,4,4),\n",
    "                      stride = (2,2,2),\n",
    "                      padding=(1,1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxUnpool3d(kernel_size=(2, 2, 2), \n",
    "                         stride=(2, 2, 2)))\n",
    "\n",
    "        \n",
    "        # Grouping convolutional and fully-connected layers in Decoding\n",
    "        self._decode_fc = nn.Sequential(\n",
    "            self.decode_fc1,\n",
    "            self.decode_fc2,\n",
    "            self.decode_fc3\n",
    "        )\n",
    "        \n",
    "        self._decode_conv = nn.Sequential(\n",
    "            self.decode_group1,\n",
    "            self.decode_group2,\n",
    "            self.decode_group3\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # Encoding part of VAE\n",
    "    def encode(self, x):\n",
    "#         h1 = F.relu(self.fc1(x))\n",
    "#         return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "        out = self._encode_conv(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self._encode_fc(out)\n",
    "        return self.encode_fc3(out)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Reparametrization Trick\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    \n",
    "    # Decoding part of VAE\n",
    "    def decode(self, z):\n",
    "#         h3 = F.relu(self.fc3(z))\n",
    "#         return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "        out = self._decode_fc(z)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self._decode_conv(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, \n",
    "                                 x.view(-1, 784), \n",
    "                                 reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, tensor([[[[9.6290e-02, 1.9568e-02, 3.3024e-01,  ..., 4.3493e-01,\n",
       "            3.6002e-03, 2.3939e-01],\n",
       "           [3.7194e-02, 1.7360e-01, 3.9472e-01,  ..., 3.5033e-01,\n",
       "            8.0818e-01, 8.6389e-01],\n",
       "           [0.0000e+00, 3.6711e-02, 8.3470e-02,  ..., 4.3923e-01,\n",
       "            8.6236e-01, 1.1797e+00],\n",
       "           ...,\n",
       "           [1.9543e-02, 2.8558e-01, 3.0122e-01,  ..., 0.0000e+00,\n",
       "            1.5595e-02, 6.1377e-02],\n",
       "           [0.0000e+00, 3.4958e-02, 6.2547e-02,  ..., 4.3017e-02,\n",
       "            1.5451e-02, 6.0810e-02],\n",
       "           [0.0000e+00, 0.0000e+00, 1.4033e-02,  ..., 5.6892e-03,\n",
       "            3.9870e-02, 3.3510e-02]],\n",
       " \n",
       "          [[5.3820e-01, 0.0000e+00, 0.0000e+00,  ..., 1.5452e-01,\n",
       "            9.0802e-02, 5.8512e-01],\n",
       "           [3.7022e-01, 7.5920e-02, 1.7262e-01,  ..., 1.2536e-01,\n",
       "            1.8182e-01, 4.5449e-01],\n",
       "           [0.0000e+00, 1.6054e-02, 3.6503e-02,  ..., 5.3587e-01,\n",
       "            1.0942e-02, 9.3789e-01],\n",
       "           ...,\n",
       "           [1.1353e-02, 4.0257e-02, 1.1858e-02,  ..., 0.0000e+00,\n",
       "            6.7755e-02, 2.6666e-01],\n",
       "           [3.9616e-02, 3.4054e-02, 2.2480e-02,  ..., 1.9893e-01,\n",
       "            6.7129e-02, 2.6420e-01],\n",
       "           [7.2201e-02, 6.2064e-02, 0.0000e+00,  ..., 2.6309e-02,\n",
       "            0.0000e+00, 1.5786e-01]],\n",
       " \n",
       "          [[0.0000e+00, 1.1224e-01, 2.4865e-01,  ..., 8.4394e-02,\n",
       "            5.2883e-01, 2.5974e+00],\n",
       "           [0.0000e+00, 3.7233e-01, 7.8672e-02,  ..., 5.8903e-01,\n",
       "            9.1805e-01, 1.3201e+00],\n",
       "           [8.5700e-02, 1.0760e-01, 9.5361e-03,  ..., 1.9560e+00,\n",
       "            1.8855e+00, 1.9310e+00],\n",
       "           ...,\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8304e-02,\n",
       "            6.2042e-02, 0.0000e+00],\n",
       "           [3.5009e-02, 3.0094e-02, 0.0000e+00,  ..., 8.7069e-02,\n",
       "            2.9512e-01, 0.0000e+00],\n",
       "           [6.3805e-02, 5.4847e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 3.5221e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.6832e-02, 1.7807e-02, 5.4858e-02,  ..., 0.0000e+00,\n",
       "            1.5978e-03, 7.0684e-02],\n",
       "           [3.0537e-02, 4.8105e-02, 6.5102e-02,  ..., 0.0000e+00,\n",
       "            1.2485e-01, 3.2837e-02],\n",
       "           [3.5956e-02, 5.2470e-02, 2.8241e-02,  ..., 0.0000e+00,\n",
       "            1.3579e-01, 1.4087e-02],\n",
       "           ...,\n",
       "           [9.2477e-03, 1.0346e-02, 4.8554e-03,  ..., 3.7206e-01,\n",
       "            4.3769e-01, 0.0000e+00],\n",
       "           [6.4468e-03, 6.8029e-03, 1.7274e-02,  ..., 4.7698e-02,\n",
       "            2.4197e-01, 8.4926e-01],\n",
       "           [3.3065e-03, 2.9680e-03, 0.0000e+00,  ..., 3.6230e-01,\n",
       "            5.2278e-01, 8.5162e-01]],\n",
       " \n",
       "          [[6.6047e-03, 3.9800e-02, 5.5515e-02,  ..., 1.3407e-01,\n",
       "            9.2970e-02, 1.1335e-01],\n",
       "           [3.1609e-02, 4.2126e-02, 2.0249e-02,  ..., 1.0992e-01,\n",
       "            7.3511e-02, 0.0000e+00],\n",
       "           [6.0209e-02, 6.1093e-02, 2.0551e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [7.4880e-03, 2.6710e-02, 5.9703e-04,  ..., 3.2568e-01,\n",
       "            5.1051e-01, 8.1137e-01],\n",
       "           [4.8051e-03, 1.1635e-02, 1.3373e-02,  ..., 4.4110e-01,\n",
       "            5.5341e-01, 2.1378e-01],\n",
       "           [1.7705e-03, 1.5747e-05, 1.1570e-02,  ..., 3.3152e-01,\n",
       "            2.6712e-01, 8.0540e-01]],\n",
       " \n",
       "          [[2.3992e-02, 4.8222e-02, 1.8884e-02,  ..., 1.0623e-01,\n",
       "            7.1043e-02, 5.6417e-02],\n",
       "           [1.0123e-03, 4.8776e-02, 3.9480e-02,  ..., 8.7096e-02,\n",
       "            5.8247e-02, 3.7120e-01],\n",
       "           [1.9661e-02, 2.0818e-02, 2.7648e-02,  ..., 5.8888e-02,\n",
       "            4.9370e-01, 0.0000e+00],\n",
       "           ...,\n",
       "           [3.5096e-04, 2.2217e-03, 5.6456e-03,  ..., 2.5337e-01,\n",
       "            6.5050e-02, 3.5163e-01],\n",
       "           [5.2732e-02, 4.6433e-03, 7.6205e-03,  ..., 2.4880e-01,\n",
       "            1.0898e-01, 9.6160e-01],\n",
       "           [1.2962e-02, 9.0570e-04, 5.8577e-03,  ..., 2.0571e-01,\n",
       "            8.4569e-01, 6.9096e-01]]],\n",
       " \n",
       " \n",
       "         [[[2.0290e+00, 3.7475e+00, 5.0344e+00,  ..., 7.9352e-01,\n",
       "            5.1604e-01, 4.0242e-01],\n",
       "           [1.6556e+00, 2.1601e+00, 1.9566e+00,  ..., 4.1573e-02,\n",
       "            1.0867e-01, 2.0917e-01],\n",
       "           [8.1559e-01, 1.0287e+00, 1.1963e+00,  ..., 1.0584e-01,\n",
       "            1.3194e-01, 4.6197e-01],\n",
       "           ...,\n",
       "           [2.6852e+00, 2.8419e+00, 2.3734e+00,  ..., 2.1651e-02,\n",
       "            4.9322e-02, 1.5709e-01],\n",
       "           [4.0422e-01, 4.6763e+00, 2.4006e+00,  ..., 6.8501e-03,\n",
       "            2.6730e-02, 9.5781e-02],\n",
       "           [1.8960e-01, 2.3213e+00, 3.0230e+00,  ..., 1.5277e-02,\n",
       "            3.6303e-02, 2.6170e-03]],\n",
       " \n",
       "          [[8.0200e-01, 9.8078e-01, 1.5339e+00,  ..., 6.1556e-01,\n",
       "            6.0774e-01, 4.2624e-01],\n",
       "           [1.0131e+00, 7.5930e-01, 8.5208e-01,  ..., 3.3921e-01,\n",
       "            2.7321e-01, 1.1470e-01],\n",
       "           [6.8800e-01, 7.0556e-01, 5.4804e-01,  ..., 1.9059e-01,\n",
       "            1.4170e-02, 1.7512e-01],\n",
       "           ...,\n",
       "           [3.7328e+00, 2.0459e+00, 2.2607e+00,  ..., 9.6505e-02,\n",
       "            6.6118e-02, 1.2407e-01],\n",
       "           [1.9321e+00, 3.2093e+00, 2.3741e+00,  ..., 3.1625e-02,\n",
       "            4.3901e-02, 2.7913e-02],\n",
       "           [2.1772e+00, 4.2005e+00, 2.1098e+00,  ..., 5.9163e-03,\n",
       "            1.1261e-02, 2.4404e-02]],\n",
       " \n",
       "          [[5.6279e-01, 7.3014e-01, 8.1630e-01,  ..., 2.0441e-01,\n",
       "            6.0652e-01, 7.1211e-01],\n",
       "           [6.7983e-01, 6.4100e-01, 6.2487e-01,  ..., 2.2049e-01,\n",
       "            2.2856e-01, 4.5915e-01],\n",
       "           [4.9510e-01, 5.3015e-01, 6.0103e-01,  ..., 1.1900e-01,\n",
       "            2.4566e-01, 2.2376e-01],\n",
       "           ...,\n",
       "           [3.7079e+00, 1.6070e+00, 2.1055e+00,  ..., 2.1986e-02,\n",
       "            6.8591e-02, 9.0425e-02],\n",
       "           [3.0854e+00, 2.8593e+00, 1.7203e+00,  ..., 5.2479e-02,\n",
       "            1.6002e-02, 4.6413e-02],\n",
       "           [3.5888e+00, 1.3874e+00, 2.6608e+00,  ..., 9.1116e-03,\n",
       "            1.1325e-02, 1.2339e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.7419e-03, 3.7238e-03, 0.0000e+00,  ..., 2.8103e-01,\n",
       "            3.2623e-01, 1.7985e-01],\n",
       "           [1.5347e-04, 1.0174e-03, 2.5425e-02,  ..., 8.5334e-02,\n",
       "            4.8155e-01, 1.1724e-01],\n",
       "           [0.0000e+00, 1.2652e-05, 1.4596e-02,  ..., 4.0499e-01,\n",
       "            1.6544e-01, 5.4178e-01],\n",
       "           ...,\n",
       "           [1.8655e-02, 7.2816e-02, 2.7251e-02,  ..., 1.2489e+01,\n",
       "            6.2620e+00, 6.4894e-01],\n",
       "           [9.1456e-02, 2.4060e-01, 1.6526e-01,  ..., 2.6711e+01,\n",
       "            1.3373e+01, 5.6903e+00],\n",
       "           [7.0662e-02, 0.0000e+00, 0.0000e+00,  ..., 3.5344e+01,\n",
       "            2.6600e+01, 7.9246e+00]],\n",
       " \n",
       "          [[1.0933e-02, 1.4520e-02, 1.8823e-02,  ..., 1.8607e-01,\n",
       "            2.2040e-01, 6.8933e-02],\n",
       "           [1.1474e-03, 5.2411e-03, 3.8152e-03,  ..., 1.2666e-02,\n",
       "            2.0777e-01, 4.6560e-01],\n",
       "           [0.0000e+00, 1.5929e-06, 3.4551e-02,  ..., 3.7996e-02,\n",
       "            7.8738e-02, 3.0696e-01],\n",
       "           ...,\n",
       "           [8.2785e-02, 1.4707e-01, 2.4654e-02,  ..., 5.7717e+00,\n",
       "            2.3931e+00, 1.3632e+00],\n",
       "           [8.7485e-02, 2.0440e-02, 2.7290e-02,  ..., 1.3038e+01,\n",
       "            7.1470e+00, 1.5604e+00],\n",
       "           [5.8892e-02, 4.9745e-03, 0.0000e+00,  ..., 2.4098e+01,\n",
       "            1.1886e+01, 2.9101e+00]],\n",
       " \n",
       "          [[8.7922e-03, 9.7459e-03, 1.6582e-02,  ..., 0.0000e+00,\n",
       "            5.4289e-01, 4.1005e-01],\n",
       "           [1.0521e-02, 9.4954e-03, 2.3993e-03,  ..., 1.7135e-01,\n",
       "            2.3828e-01, 4.6733e-01],\n",
       "           [2.7669e-03, 8.2345e-04, 5.6054e-07,  ..., 1.0290e+00,\n",
       "            3.6549e-01, 2.6039e-01],\n",
       "           ...,\n",
       "           [0.0000e+00, 0.0000e+00, 7.1680e-02,  ..., 1.5901e+00,\n",
       "            1.8186e+00, 1.4089e+00],\n",
       "           [1.6971e-01, 2.1099e-01, 7.9345e-02,  ..., 5.2311e+00,\n",
       "            1.5155e+00, 3.5013e+00],\n",
       "           [4.1304e-02, 5.1351e-02, 8.1108e-02,  ..., 1.0837e+01,\n",
       "            3.7419e+00, 3.2221e+00]]],\n",
       " \n",
       " \n",
       "         [[[1.0511e+00, 4.1948e-02, 5.0808e-02,  ..., 0.0000e+00,\n",
       "            2.1613e-01, 2.0115e-02],\n",
       "           [3.3228e-01, 1.6979e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            2.8801e-01, 2.6805e-02],\n",
       "           [3.1974e-02, 8.4078e-02, 1.2519e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [3.2367e-01, 5.1718e-01, 6.0274e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.2415e-01, 5.7836e-01, 1.4106e-01,  ..., 2.0314e-03,\n",
       "            1.0750e-02, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3811e-02,\n",
       "            2.8476e-01, 0.0000e+00]],\n",
       " \n",
       "          [[3.3003e-01, 1.4737e-03, 0.0000e+00,  ..., 9.0498e-02,\n",
       "            7.4303e-02, 0.0000e+00],\n",
       "           [1.7917e-02, 2.6748e-04, 0.0000e+00,  ..., 4.3261e-02,\n",
       "            3.5519e-02, 0.0000e+00],\n",
       "           [0.0000e+00, 1.4840e-01, 4.0081e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 4.8020e-02],\n",
       "           ...,\n",
       "           [1.0360e+00, 9.6803e-02, 2.8798e-01,  ..., 0.0000e+00,\n",
       "            3.7844e-02, 1.8811e-02],\n",
       "           [6.9131e-02, 9.6945e-02, 1.9069e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.5695e-02, 1.4470e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[3.3426e-01, 1.8050e-02, 1.7112e-01,  ..., 1.4004e-01,\n",
       "            1.1498e-01, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 3.0548e-01,  ..., 6.6945e-02,\n",
       "            5.4965e-02, 0.0000e+00],\n",
       "           [1.6195e-01, 8.6195e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 2.9427e-01],\n",
       "           ...,\n",
       "           [5.2934e-01, 1.7513e+00, 1.0199e+00,  ..., 0.0000e+00,\n",
       "            1.6406e-01, 8.1548e-02],\n",
       "           [8.5893e-01, 1.5497e-01, 9.0950e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [8.2985e-01, 1.7250e-01, 1.4321e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.0000e+00, 0.0000e+00, 4.1476e-02,  ..., 2.9684e-01,\n",
       "            5.2696e-01, 7.0169e-01],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.3100e-01,\n",
       "            7.6307e-01, 4.8143e-01],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.1070e-01,\n",
       "            6.4548e-01, 7.4076e-01],\n",
       "           ...,\n",
       "           [7.8214e-03, 4.7574e-03, 6.1880e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 3.4607e-04, 4.5013e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.2856e-02, 3.2920e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[0.0000e+00, 0.0000e+00, 1.1917e-01,  ..., 2.6694e-01,\n",
       "            2.7419e-01, 7.8285e-01],\n",
       "           [1.4579e-02, 0.0000e+00, 0.0000e+00,  ..., 6.3908e-01,\n",
       "            3.7749e-01, 1.2191e+00],\n",
       "           [6.0018e-03, 0.0000e+00, 0.0000e+00,  ..., 9.4395e-01,\n",
       "            1.8881e-01, 8.7105e-01],\n",
       "           ...,\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.4298e-01, 8.9051e-02, 3.9961e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 1.7643e-01]],\n",
       " \n",
       "          [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.6616e-01,\n",
       "            6.8415e-01, 3.3433e-01],\n",
       "           [2.2356e-01, 0.0000e+00, 0.0000e+00,  ..., 8.9102e-01,\n",
       "            8.1475e-01, 9.3411e-01],\n",
       "           [9.2038e-02, 0.0000e+00, 0.0000e+00,  ..., 5.2080e-01,\n",
       "            6.9889e-01, 1.6080e+00],\n",
       "           ...,\n",
       "           [7.9997e-02, 0.0000e+00, 3.9880e-02,  ..., 0.0000e+00,\n",
       "            5.8638e-01, 1.0236e-01],\n",
       "           [0.0000e+00, 0.0000e+00, 4.7298e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.3464e-02, 0.0000e+00, 8.6972e-02,  ..., 4.4669e-01,\n",
       "            1.5334e-01, 5.2011e-01]]],\n",
       " \n",
       " \n",
       "         [[[2.5777e-02, 1.5318e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.8789e-02, 1.7107e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.1373e-02, 1.2041e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.9370e-02, 1.3379e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [6.2863e-02, 4.3421e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3050e-01,\n",
       "            1.4632e-01, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.9442e-02,\n",
       "            1.8690e-02, 0.0000e+00],\n",
       "           ...,\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2293e-02,\n",
       "            7.5209e-03, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.6655e-02,\n",
       "            2.2426e-02, 0.0000e+00],\n",
       "           [0.0000e+00, 1.7815e-01, 1.6405e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            3.1833e-02, 1.0214e-01],\n",
       "           [0.0000e+00, 1.9527e-01, 1.3865e-01,  ..., 4.9262e-02,\n",
       "            3.1271e-02, 0.0000e+00],\n",
       "           [0.0000e+00, 4.6288e-02, 3.2867e-02,  ..., 8.8663e-03,\n",
       "            1.7011e-01, 0.0000e+00],\n",
       "           ...,\n",
       "           [5.6810e-02, 1.5820e-01, 0.0000e+00,  ..., 3.6951e-02,\n",
       "            2.2607e-02, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1018e-01,\n",
       "            6.7408e-02, 0.0000e+00],\n",
       "           [0.0000e+00, 1.3815e-01, 1.2721e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.0569e-03, 5.2063e-02, 3.0506e-01,  ..., 6.7645e-01,\n",
       "            1.8828e-01, 2.1164e-02],\n",
       "           [8.2479e-02, 3.9768e-02, 2.3302e-01,  ..., 2.5030e-01,\n",
       "            6.5981e-02, 0.0000e+00],\n",
       "           [0.0000e+00, 3.3815e-02, 6.4895e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [0.0000e+00, 2.9956e-02, 9.7879e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 3.0337e-01, 3.7686e+00,  ..., 2.7266e-02,\n",
       "            0.0000e+00, 2.0699e-01],\n",
       "           [1.7207e-02, 1.8936e+00, 1.1812e+01,  ..., 3.9239e-02,\n",
       "            0.0000e+00, 2.0455e-02]],\n",
       " \n",
       "          [[0.0000e+00, 3.2799e-03, 1.9219e-02,  ..., 2.3035e-01,\n",
       "            6.0724e-02, 0.0000e+00],\n",
       "           [0.0000e+00, 2.5053e-03, 1.4680e-02,  ..., 8.5235e-02,\n",
       "            2.2469e-02, 4.3759e-02],\n",
       "           [0.0000e+00, 7.5611e-02, 1.4511e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 2.1873e-01],\n",
       "           ...,\n",
       "           [0.0000e+00, 0.0000e+00, 1.4131e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 6.8896e-02, 3.8969e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 7.1627e-03],\n",
       "           [5.4792e-01, 1.1697e+00, 4.3286e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 7.0784e-04]],\n",
       " \n",
       "          [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0475e-01,\n",
       "            1.8391e-02, 4.0633e-02],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 1.0127e-02],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 5.0619e-02],\n",
       "           ...,\n",
       "           [0.0000e+00, 0.0000e+00, 8.5549e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 4.8081e-02, 2.6743e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.8182e-02, 2.6003e-01, 8.6805e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]]]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(train_loader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        print(batch_idx)\n",
    "        print(data)\n",
    "        \n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        # if batch_idx % args.log_interval == 0:\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      #recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
    "                                        recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-5c689a6e5971>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#for epoch in range(1, args.epochs + 1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-4ae8db777c02>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #for epoch in range(1, args.epochs + 1):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(64, 20).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28),\n",
    "                       'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
