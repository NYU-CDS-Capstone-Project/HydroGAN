{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code taken from: https://github.com/pytorch/examples/tree/master/vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model imports\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import pickle as pkl\n",
    "import random\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "## plot imports\n",
    "import itertools\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import colors\n",
    "import h5py\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0: Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    \"\"\"Function for dividing/truncating cmaps\"\"\"\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "\n",
    "def visualize_cube(cube=None,      ## array name\n",
    "             edge_dim=None,        ## edge dimension (128 for 128 x 128 x 128 cube)\n",
    "             start_cube_index_x=0,\n",
    "             start_cube_index_y=0,\n",
    "             start_cube_index_z=0,\n",
    "             fig_size=None,\n",
    "             stdev_to_white=1,\n",
    "             norm_multiply=600,\n",
    "             color_map=\"Blues\",\n",
    "             lognormal=False):\n",
    "    \n",
    "    cube_size = edge_dim\n",
    "    edge = np.array([*range(cube_size)])\n",
    "    \n",
    "    end_x = start_cube_index_x + cube_size\n",
    "    end_y = start_cube_index_y + cube_size\n",
    "    end_z = start_cube_index_z + cube_size\n",
    "    \n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    data_value = cube[start_cube_index_x:end_x,\n",
    "                      start_cube_index_y:end_y,\n",
    "                      start_cube_index_z:end_z]\n",
    "    \n",
    "    x,y,z = edge,edge,edge\n",
    "    product = [*itertools.product(x,y,z)]\n",
    "    \n",
    "    X = np.array([product[k][0] for k in [*range(len(product))]])\n",
    "    Y = np.array([product[k][1] for k in [*range(len(product))]])\n",
    "    Z = np.array([product[k][2] for k in [*range(len(product))]])\n",
    "    \n",
    "    ## map data to 1d array that corresponds to the axis values in the product array\n",
    "    data_1dim = np.array([data_value[X[i]][Y[i]][Z[i]] for i in [*range(len(product))]])\n",
    "    \n",
    "    \n",
    "    initial_mean = np.mean(data_1dim) - stdev_to_white*np.std(data_1dim)\n",
    "    mask = data_1dim > initial_mean\n",
    "    mask = mask.astype(np.int)\n",
    "    \n",
    "    data_1dim = np.multiply(mask,data_1dim)\n",
    "    ## mask X,Y,Z to match the dimensions of the data\n",
    "    X, Y, Z, data_1dim = [axis[np.where(data_1dim>0)] for axis in [X,Y,Z,data_1dim]]\n",
    "\n",
    "    if lognormal == False:\n",
    "        s = norm_multiply*data_1dim/np.linalg.norm(data_1dim)\n",
    "    else:\n",
    "        s = np.log(norm_multiply*data_1dim/np.linalg.norm(data_1dim))\n",
    "    \n",
    "    cmap=plt.get_cmap(color_map)\n",
    "    new_cmap = truncate_colormap(cmap, 0.99, 1,n=10)\n",
    "    \n",
    "    ## IGNORE BELOW 3D PLOT FORMATTING \n",
    "    \n",
    "    ## plot cube\n",
    "    \n",
    "    cube_definition = [(start_cube_index_x, start_cube_index_x, start_cube_index_x),\n",
    "                      (start_cube_index_x, start_cube_index_x+edge_dim, start_cube_index_x),\n",
    "                      (start_cube_index_x+edge_dim, start_cube_index_x, start_cube_index_x),\n",
    "                      (start_cube_index_x, start_cube_index_x, start_cube_index_x+edge_dim)]\n",
    "    \n",
    "    cube_definition_array = [\n",
    "        np.array(list(item))\n",
    "        for item in cube_definition\n",
    "    ]\n",
    "    \n",
    "    points = []\n",
    "    points += cube_definition_array\n",
    "    vectors = [\n",
    "        cube_definition_array[1] - cube_definition_array[0],\n",
    "        cube_definition_array[2] - cube_definition_array[0],\n",
    "        cube_definition_array[3] - cube_definition_array[0]\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    points += [cube_definition_array[0] + vectors[0] + vectors[1]]\n",
    "    points += [cube_definition_array[0] + vectors[0] + vectors[2]]\n",
    "    points += [cube_definition_array[0] + vectors[1] + vectors[2]]\n",
    "    points += [cube_definition_array[0] + vectors[0] + vectors[1] + vectors[2]]\n",
    "\n",
    "    points = np.array(points)\n",
    "\n",
    "    edges = [\n",
    "        [points[0], points[3], points[5], points[1]],\n",
    "        [points[1], points[5], points[7], points[4]],\n",
    "        [points[4], points[2], points[6], points[7]],\n",
    "        [points[2], points[6], points[3], points[0]],\n",
    "        [points[0], points[2], points[4], points[1]],\n",
    "        [points[3], points[6], points[7], points[5]]\n",
    "    ]\n",
    "    \n",
    "#     ax.fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    faces = Poly3DCollection(edges, linewidths=1, edgecolors='k',)\n",
    "    faces.set_facecolor((0,0,1,0)) ## set transparent facecolor to the cube\n",
    "    \n",
    "    ax.add_collection3d(faces)\n",
    "    \n",
    "    ax.scatter(points[:,0], points[:,1], points[:,2], s=0)\n",
    "\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    ax.xaxis.pane.fill = False\n",
    "    ax.yaxis.pane.fill = False\n",
    "    ax.zaxis.pane.fill = False\n",
    "    \n",
    "    ax.xaxis.pane.set_edgecolor('w')\n",
    "    ax.yaxis.pane.set_edgecolor('w')\n",
    "    ax.zaxis.pane.set_edgecolor('w')\n",
    "    \n",
    "    ax.xaxis.set_major_locator(MultipleLocator(edge_dim))\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(edge_dim))\n",
    "    ax.zaxis.set_major_locator(MultipleLocator(edge_dim))\n",
    "    \n",
    "    ax.grid(False)\n",
    "    \n",
    "    ax.set_xlim3d(0,edge_dim)\n",
    "    ax.set_ylim3d(0,edge_dim)\n",
    "    ax.set_zlim3d(0,edge_dim)\n",
    "#     ax.get_frame_on()\n",
    "    \n",
    "    ax.xaxis._axinfo['tick']['inward_factor'] = 0\n",
    "    ax.xaxis._axinfo['tick']['outward_factor'] = 0\n",
    "    ax.yaxis._axinfo['tick']['inward_factor'] = 0\n",
    "    ax.yaxis._axinfo['tick']['outward_factor'] = 0\n",
    "    ax.zaxis._axinfo['tick']['inward_factor'] = 0\n",
    "    ax.zaxis._axinfo['tick']['outward_factor'] = 0\n",
    "    \n",
    "    ax.scatter(X, Y, Z,       ## axis vals\n",
    "               c=data_1dim,   ## data, mapped to 1-dim\n",
    "               cmap=new_cmap,\n",
    "               s=s,           ## sizes - dims multiplied by each data point's magnitude\n",
    "               alpha=0.7,\n",
    "               edgecolors=\"face\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube_mass_histogram(cube=None,      ## array name\n",
    "             edge_dim=128,        ## edge dimension (128 for 128 x 128 x 128 cube)\n",
    "             start_cube_index_x=0,\n",
    "             start_cube_index_y=0,\n",
    "             start_cube_index_z=0,\n",
    "             fig_size=None,\n",
    "             stdev_to_white=1,\n",
    "             norm_multiply=1,\n",
    "             color_map=\"Blues\",\n",
    "             lognormal=False,\n",
    "             hist_color=\"b\",\n",
    "                       hist_bins=300):\n",
    "    \n",
    "    ## adjust cube and get 1d array of in-cube masses\n",
    "    cube_size = edge_dim\n",
    "    edge = np.array([*range(cube_size)])\n",
    "    \n",
    "    end_x = start_cube_index_x + cube_size\n",
    "    end_y = start_cube_index_y + cube_size\n",
    "    end_z = start_cube_index_z + cube_size\n",
    "    \n",
    "    data_value = cube[start_cube_index_x:end_x,\n",
    "                      start_cube_index_y:end_y,\n",
    "                      start_cube_index_z:end_z]\n",
    "    \n",
    "    x,y,z = edge,edge,edge\n",
    "    product = [*itertools.product(x,y,z)]\n",
    "    \n",
    "    X = np.array([product[k][0] for k in [*range(len(product))]])\n",
    "    Y = np.array([product[k][1] for k in [*range(len(product))]])\n",
    "    Z = np.array([product[k][2] for k in [*range(len(product))]])\n",
    "    \n",
    "    ## map data to 1d array that corresponds to the axis values in the product array\n",
    "    data_1dim = np.array([data_value[X[i]][Y[i]][Z[i]] for i in [*range(len(product))]])\n",
    "    \n",
    "    ## start plotting\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.hist(data_1dim,bins=hist_bins,\n",
    "            alpha=0.4)\n",
    "    plt.xlabel(\"Masses\")\n",
    "    plt.ylabel(\"Number of Samples That Correspond to the Mass\")\n",
    "    plt.title(\"Output Mass Histogram\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify to accept hard coded arguments\n",
    "batch_size = 8\n",
    "epochs = 20\n",
    "no_cuda = False\n",
    "log_interval = 1\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "# cuda = False\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "# kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_test(s_test, s_train):\n",
    "    #2048/16=128\n",
    "    m=8\n",
    "    x=random.randint(0,m)*s_train\n",
    "    y=random.randint(0,m)*s_train\n",
    "    z=random.randint(0,m)*s_train\n",
    "    #print(x,y,z)\n",
    "    return {'x':[x,x+s_test], 'y':[y,y+s_test], 'z':[z,z+s_test]}\n",
    "\n",
    "def check_coords(test_coords, train_coords):\n",
    "    valid=True\n",
    "    for i in ['x','y','z']:\n",
    "        r=(max(test_coords[i][0], \n",
    "               train_coords[i][0]), \n",
    "           min(test_coords[i][1],\n",
    "               train_coords[i][1]))\n",
    "        if r[0]<=r[1]:\n",
    "            valid=False\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(s_sample, nsamples, redshift, test_coords):\n",
    "    #n is size of minibatch, get valid samples (not intersecting with test_coords)\n",
    "    sample_list=[]\n",
    "    m=2048-128\n",
    "    for n in range(nsamples):\n",
    "        #print(\"Sample No = \" + str(n + 1) + \" / \" + str(nsamples))\n",
    "        sample_valid=False\n",
    "        while sample_valid==False:\n",
    "            x = random.randint(0,m)\n",
    "            y = random.randint(0,m)\n",
    "            z = random.randint(0,m)\n",
    "            sample_coords = {'x':[x,x+s_sample], \n",
    "                             'y':[y,y+s_sample], \n",
    "                             'z':[z,z+s_sample]}\n",
    "            \n",
    "            sample_valid = check_coords(test_coords, \n",
    "                                        sample_coords)\n",
    "        \n",
    "        sample_list.append(sample_coords)\n",
    "    \n",
    "    print(\"Sampling finished.\")\n",
    "        \n",
    "    #Load cube and get samples and convert them to np.arrays\n",
    "    sample_array=[]\n",
    "    datapath=''\n",
    "    f = h5py.File(datapath+'fields_z='+redshift+'.hdf5', 'r')\n",
    "    f=f['delta_HI']\n",
    "    \n",
    "    # getting the max of the whole cube\n",
    "    #print(f.shape)\n",
    "    max_list = []\n",
    "    for i in range(f.shape[0]):\n",
    "        #print(np.max(f[i:i+1,:,:]))\n",
    "        max_list.append(np.max(f[i:i+1,:,:]))\n",
    "    max_cube = max(max_list)\n",
    "    #f.close()\n",
    "    \n",
    "    print(\"Getting max value finished.\")\n",
    "    \n",
    "    counter = 0\n",
    "    for c in sample_list:\n",
    "        print(\"Counter = \" + str(counter + 1) + \" / \" + str(len(sample_list)))\n",
    "        a = f[c['x'][0]:c['x'][1],\n",
    "              c['y'][0]:c['y'][1],\n",
    "              c['z'][0]:c['z'][1]]\n",
    "        \n",
    "        # a = np.array(a)\n",
    "        a = np.array(a) / max_cube\n",
    "        sample_array.append(a)\n",
    "    \n",
    "        counter = counter + 1\n",
    "        \n",
    "    f=0\n",
    "    return sample_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_coords=define_test(1024,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HydrogenDataset2(Dataset):\n",
    "    \"\"\"Hydrogen Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, h5_file, root_dir, s_test, s_train,\n",
    "                 s_sample, nsamples, redshift):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h5_file (string): name of the h5 file with 32 sampled cubes.\n",
    "            root_dir (string): Directory with the .h5 file.\n",
    "        \"\"\"\n",
    "        file_size = os.path.getsize(root_dir + h5_file) / 1e6 # in MBs\n",
    "        print(\"The whole file size is \" + str(int(file_size)) + \" MBs\")\n",
    "        \n",
    "        # self.subcubes = h5py.File('../data/sample_32.h5', 'r')\n",
    "        self.subcubes = h5py.File(root_dir + h5_file, 'r')[\"delta_HI\"]\n",
    "        self.h5_file = h5_file\n",
    "        self.root_dir = root_dir\n",
    "        self.s_test = s_test\n",
    "        self.s_train = s_train\n",
    "        self.t_coords = define_test(self.s_test,\n",
    "                                    self.s_train)\n",
    "        self.s_sample = s_sample\n",
    "        self.nsamples = nsamples\n",
    "        self.redshift = redshift\n",
    "        \n",
    "        self.samples = get_samples(s_sample = self.s_sample,\n",
    "                             nsamples = self.nsamples,\n",
    "                             redshift = self.redshift,\n",
    "                             test_coords = self.t_coords)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Function called when len(self) is executed\n",
    "        \n",
    "        #print(len(self.subcubes))\n",
    "#         return len(self.nsamples)\n",
    "        return self.nsamples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        This can be implemented in such a way that the whole h5 file read \n",
    "        using h5py.File() and get_sample() function is called to return\n",
    "        a random subcube. This won't increase memory usage because the\n",
    "        subcubes will be read in the same way and only the batch will\n",
    "        be read into memory.\n",
    "        \n",
    "        Here we have implemented it so that it can be used with data\n",
    "        generated by get_sample() function.\n",
    "        \n",
    "        The output of this function is one subcube with the dimensions\n",
    "        specified by get_sample() implementation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # default version -> error in training because of dimensions\n",
    "#         sample = self.subcubes[idx]\n",
    "        \n",
    "        # reshaped version to add another dimension\n",
    "#         sample = self.subcubes[idx].reshape((1,128,128,128))\n",
    "\n",
    "        # On prince using get_samples()\n",
    "#         print(\"nsamples = \" + str(self.nsamples))\n",
    "#         sample = get_samples(s_sample = self.s_sample,\n",
    "#                              nsamples = self.nsamples,\n",
    "#                              redshift = self.redshift,\n",
    "#                              test_coords = self.t_coords)\n",
    "    \n",
    "        sample = self.samples[idx].reshape((1,128,128,128))\n",
    "        \n",
    "        # added division by 1e6 for exploding variance\n",
    "        # and resulting in inf during reparametrization trick part\n",
    "        sample = sample/1e6\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whole file size is 68719 MBs\n",
      "Sampling finished.\n",
      "Getting max value finished.\n",
      "Counter = 1 / 256\n",
      "Counter = 2 / 256\n",
      "Counter = 3 / 256\n",
      "Counter = 4 / 256\n",
      "Counter = 5 / 256\n",
      "Counter = 6 / 256\n",
      "Counter = 7 / 256\n",
      "Counter = 8 / 256\n",
      "Counter = 9 / 256\n",
      "Counter = 10 / 256\n",
      "Counter = 11 / 256\n",
      "Counter = 12 / 256\n",
      "Counter = 13 / 256\n",
      "Counter = 14 / 256\n",
      "Counter = 15 / 256\n",
      "Counter = 16 / 256\n",
      "Counter = 17 / 256\n",
      "Counter = 18 / 256\n",
      "Counter = 19 / 256\n",
      "Counter = 20 / 256\n",
      "Counter = 21 / 256\n",
      "Counter = 22 / 256\n",
      "Counter = 23 / 256\n",
      "Counter = 24 / 256\n",
      "Counter = 25 / 256\n",
      "Counter = 26 / 256\n",
      "Counter = 27 / 256\n",
      "Counter = 28 / 256\n",
      "Counter = 29 / 256\n",
      "Counter = 30 / 256\n",
      "Counter = 31 / 256\n",
      "Counter = 32 / 256\n",
      "Counter = 33 / 256\n",
      "Counter = 34 / 256\n",
      "Counter = 35 / 256\n",
      "Counter = 36 / 256\n",
      "Counter = 37 / 256\n",
      "Counter = 38 / 256\n",
      "Counter = 39 / 256\n",
      "Counter = 40 / 256\n",
      "Counter = 41 / 256\n",
      "Counter = 42 / 256\n",
      "Counter = 43 / 256\n",
      "Counter = 44 / 256\n",
      "Counter = 45 / 256\n",
      "Counter = 46 / 256\n",
      "Counter = 47 / 256\n",
      "Counter = 48 / 256\n",
      "Counter = 49 / 256\n",
      "Counter = 50 / 256\n",
      "Counter = 51 / 256\n",
      "Counter = 52 / 256\n",
      "Counter = 53 / 256\n",
      "Counter = 54 / 256\n",
      "Counter = 55 / 256\n",
      "Counter = 56 / 256\n",
      "Counter = 57 / 256\n",
      "Counter = 58 / 256\n",
      "Counter = 59 / 256\n",
      "Counter = 60 / 256\n",
      "Counter = 61 / 256\n",
      "Counter = 62 / 256\n",
      "Counter = 63 / 256\n",
      "Counter = 64 / 256\n",
      "Counter = 65 / 256\n",
      "Counter = 66 / 256\n",
      "Counter = 67 / 256\n",
      "Counter = 68 / 256\n",
      "Counter = 69 / 256\n",
      "Counter = 70 / 256\n",
      "Counter = 71 / 256\n",
      "Counter = 72 / 256\n",
      "Counter = 73 / 256\n",
      "Counter = 74 / 256\n",
      "Counter = 75 / 256\n",
      "Counter = 76 / 256\n",
      "Counter = 77 / 256\n",
      "Counter = 78 / 256\n",
      "Counter = 79 / 256\n",
      "Counter = 80 / 256\n",
      "Counter = 81 / 256\n",
      "Counter = 82 / 256\n",
      "Counter = 83 / 256\n",
      "Counter = 84 / 256\n",
      "Counter = 85 / 256\n",
      "Counter = 86 / 256\n",
      "Counter = 87 / 256\n",
      "Counter = 88 / 256\n",
      "Counter = 89 / 256\n",
      "Counter = 90 / 256\n",
      "Counter = 91 / 256\n",
      "Counter = 92 / 256\n",
      "Counter = 93 / 256\n",
      "Counter = 94 / 256\n",
      "Counter = 95 / 256\n",
      "Counter = 96 / 256\n",
      "Counter = 97 / 256\n",
      "Counter = 98 / 256\n",
      "Counter = 99 / 256\n",
      "Counter = 100 / 256\n",
      "Counter = 101 / 256\n",
      "Counter = 102 / 256\n",
      "Counter = 103 / 256\n",
      "Counter = 104 / 256\n",
      "Counter = 105 / 256\n",
      "Counter = 106 / 256\n",
      "Counter = 107 / 256\n",
      "Counter = 108 / 256\n",
      "Counter = 109 / 256\n",
      "Counter = 110 / 256\n",
      "Counter = 111 / 256\n",
      "Counter = 112 / 256\n",
      "Counter = 113 / 256\n",
      "Counter = 114 / 256\n",
      "Counter = 115 / 256\n",
      "Counter = 116 / 256\n",
      "Counter = 117 / 256\n",
      "Counter = 118 / 256\n",
      "Counter = 119 / 256\n",
      "Counter = 120 / 256\n",
      "Counter = 121 / 256\n",
      "Counter = 122 / 256\n",
      "Counter = 123 / 256\n",
      "Counter = 124 / 256\n",
      "Counter = 125 / 256\n",
      "Counter = 126 / 256\n",
      "Counter = 127 / 256\n",
      "Counter = 128 / 256\n",
      "Counter = 129 / 256\n",
      "Counter = 130 / 256\n",
      "Counter = 131 / 256\n",
      "Counter = 132 / 256\n",
      "Counter = 133 / 256\n",
      "Counter = 134 / 256\n",
      "Counter = 135 / 256\n",
      "Counter = 136 / 256\n",
      "Counter = 137 / 256\n",
      "Counter = 138 / 256\n",
      "Counter = 139 / 256\n",
      "Counter = 140 / 256\n",
      "Counter = 141 / 256\n",
      "Counter = 142 / 256\n",
      "Counter = 143 / 256\n",
      "Counter = 144 / 256\n",
      "Counter = 145 / 256\n",
      "Counter = 146 / 256\n",
      "Counter = 147 / 256\n",
      "Counter = 148 / 256\n",
      "Counter = 149 / 256\n",
      "Counter = 150 / 256\n",
      "Counter = 151 / 256\n",
      "Counter = 152 / 256\n",
      "Counter = 153 / 256\n",
      "Counter = 154 / 256\n",
      "Counter = 155 / 256\n",
      "Counter = 156 / 256\n",
      "Counter = 157 / 256\n",
      "Counter = 158 / 256\n",
      "Counter = 159 / 256\n",
      "Counter = 160 / 256\n",
      "Counter = 161 / 256\n",
      "Counter = 162 / 256\n",
      "Counter = 163 / 256\n",
      "Counter = 164 / 256\n",
      "Counter = 165 / 256\n",
      "Counter = 166 / 256\n",
      "Counter = 167 / 256\n",
      "Counter = 168 / 256\n",
      "Counter = 169 / 256\n",
      "Counter = 170 / 256\n",
      "Counter = 171 / 256\n",
      "Counter = 172 / 256\n",
      "Counter = 173 / 256\n",
      "Counter = 174 / 256\n",
      "Counter = 175 / 256\n",
      "Counter = 176 / 256\n",
      "Counter = 177 / 256\n",
      "Counter = 178 / 256\n",
      "Counter = 179 / 256\n",
      "Counter = 180 / 256\n",
      "Counter = 181 / 256\n",
      "Counter = 182 / 256\n",
      "Counter = 183 / 256\n",
      "Counter = 184 / 256\n",
      "Counter = 185 / 256\n",
      "Counter = 186 / 256\n",
      "Counter = 187 / 256\n",
      "Counter = 188 / 256\n",
      "Counter = 189 / 256\n",
      "Counter = 190 / 256\n",
      "Counter = 191 / 256\n",
      "Counter = 192 / 256\n",
      "Counter = 193 / 256\n",
      "Counter = 194 / 256\n",
      "Counter = 195 / 256\n",
      "Counter = 196 / 256\n",
      "Counter = 197 / 256\n",
      "Counter = 198 / 256\n",
      "Counter = 199 / 256\n",
      "Counter = 200 / 256\n",
      "Counter = 201 / 256\n",
      "Counter = 202 / 256\n",
      "Counter = 203 / 256\n",
      "Counter = 204 / 256\n",
      "Counter = 205 / 256\n",
      "Counter = 206 / 256\n",
      "Counter = 207 / 256\n",
      "Counter = 208 / 256\n",
      "Counter = 209 / 256\n",
      "Counter = 210 / 256\n",
      "Counter = 211 / 256\n",
      "Counter = 212 / 256\n",
      "Counter = 213 / 256\n",
      "Counter = 214 / 256\n",
      "Counter = 215 / 256\n",
      "Counter = 216 / 256\n",
      "Counter = 217 / 256\n",
      "Counter = 218 / 256\n",
      "Counter = 219 / 256\n",
      "Counter = 220 / 256\n",
      "Counter = 221 / 256\n",
      "Counter = 222 / 256\n",
      "Counter = 223 / 256\n",
      "Counter = 224 / 256\n",
      "Counter = 225 / 256\n",
      "Counter = 226 / 256\n",
      "Counter = 227 / 256\n",
      "Counter = 228 / 256\n",
      "Counter = 229 / 256\n",
      "Counter = 230 / 256\n",
      "Counter = 231 / 256\n",
      "Counter = 232 / 256\n",
      "Counter = 233 / 256\n",
      "Counter = 234 / 256\n",
      "Counter = 235 / 256\n",
      "Counter = 236 / 256\n",
      "Counter = 237 / 256\n",
      "Counter = 238 / 256\n",
      "Counter = 239 / 256\n",
      "Counter = 240 / 256\n",
      "Counter = 241 / 256\n",
      "Counter = 242 / 256\n",
      "Counter = 243 / 256\n",
      "Counter = 244 / 256\n",
      "Counter = 245 / 256\n",
      "Counter = 246 / 256\n",
      "Counter = 247 / 256\n",
      "Counter = 248 / 256\n",
      "Counter = 249 / 256\n",
      "Counter = 250 / 256\n",
      "Counter = 251 / 256\n",
      "Counter = 252 / 256\n",
      "Counter = 253 / 256\n",
      "Counter = 254 / 256\n",
      "Counter = 255 / 256\n",
      "Counter = 256 / 256\n"
     ]
    }
   ],
   "source": [
    "sampled_subcubes = HydrogenDataset2(h5_file=\"fields_z=1.0.hdf5\",\n",
    "                                    root_dir = \"\",\n",
    "                                    s_test = 1024, \n",
    "                                    s_train = 128,\n",
    "                                    s_sample = 128, \n",
    "                                    nsamples = 256, \n",
    "                                    redshift = \"1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.HydrogenDataset2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sampled_subcubes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Model and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "train_loader = DataLoader(\n",
    "        dataset=sampled_subcubes,\n",
    "        #batch_size=args.batch_size, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, \n",
    "        **kwargs)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "        dataset=sampled_subcubes,\n",
    "        #batch_size=args.batch_size, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, \n",
    "        **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The Encoding Layers\n",
    "        nn.Conv3d \n",
    "        nn.MaxPool3d \n",
    "        \n",
    "        out_channels is the number of different filters we convolute \n",
    "        over the whole sampled subcube.\n",
    "        \n",
    "        So the first convolutional layer's in_channel should be 0 (?)\n",
    "        \n",
    "        In addition, the next layer's in_channel should be equal to\n",
    "        the previous layer's out_channels (all examples show that\n",
    "        this is the case)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convolutional Layer 1\n",
    "        self.encode_conv1 = nn.Conv3d(in_channels=1, \n",
    "                                      out_channels=8, \n",
    "                                      kernel_size=(4,4,4), # == 4\n",
    "                                      stride = (2,2,2), # == 2\n",
    "                                      padding=(1,1,1)) # == 1\n",
    "        nn.init.xavier_uniform_(self.encode_conv1.weight) #Xaviers Initialisation\n",
    "        \n",
    "        self.encode_relu1 = nn.ReLU()\n",
    "        self.encode_maxpool1 = nn.MaxPool3d(kernel_size=(2, 2, 2), \n",
    "                                             stride=(2, 2, 2),\n",
    "                                            return_indices = True)\n",
    "        \n",
    "        # Convolutional Layer 2\n",
    "        self.encode_conv2 = nn.Conv3d(in_channels=8, \n",
    "                                      out_channels=16, \n",
    "                                      kernel_size=(4,4,4), # == 4 \n",
    "                                      stride = (2,2,2),\n",
    "                                      padding=(1,1,1))\n",
    "        nn.init.xavier_uniform_(self.encode_conv2.weight) #Xaviers Initialisation\n",
    "        \n",
    "        self.encode_relu2 = nn.ReLU()\n",
    "        self.encode_maxpool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), \n",
    "                                             stride=(2, 2, 2),\n",
    "                                            return_indices = True)\n",
    "\n",
    "        # Convolutional Layer 3\n",
    "        self.encode_conv3 = nn.Conv3d(in_channels=16, \n",
    "                                      out_channels=32, \n",
    "                                      kernel_size=(4,4,4), # == 4 \n",
    "                                      stride = (2,2,2),\n",
    "                                      padding=(1,1,1))\n",
    "        nn.init.xavier_uniform_(self.encode_conv3.weight) #Xaviers Initialisation\n",
    "        \n",
    "        self.encode_relu3 = nn.ReLU()\n",
    "#         self.encode_maxpool3 = nn.MaxPool3d(kernel_size=(2, 2, 2), \n",
    "#                                              stride=(2, 2, 2),\n",
    "#                                             return_indices = True)\n",
    "\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Fully Connected Layers after 3D Convolutional Layers\n",
    "        First FC layer's input should be equal to \n",
    "        last convolutional layer's output\n",
    "        8192 = 8^3 * 16 \n",
    "            8^3 = (output of 2nd convolutional layer)\n",
    "            16 = number of out_channels\n",
    "        \"\"\"\n",
    "        \n",
    "#         self.encode_fc1 = nn.Sequential(\n",
    "#             nn.Linear(in_features=2048, \n",
    "#                       out_features=5096), \n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5))\n",
    "        \n",
    "        self.encode_fc1_linear = nn.Linear(in_features=2048, \n",
    "                                           out_features=128)\n",
    "        self.encode_fc1_relu = nn.ReLU()\n",
    "        self.encode_fc1_dropout = nn.Dropout(0.5)\n",
    "        nn.init.xavier_uniform_(self.encode_fc1_linear.weight) #Xaviers Initialisation\n",
    "\n",
    "        \n",
    "#         self.encode_fc2 = nn.Sequential(\n",
    "#             nn.Linear(in_features = 5096,\n",
    "#                       out_features = 5096),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5))\n",
    "\n",
    "        self.encode_fc2_linear = nn.Linear(in_features=128, \n",
    "                                           out_features=128)\n",
    "        self.encode_fc2_relu = nn.ReLU()\n",
    "        self.encode_fc2_dropout = nn.Dropout(0.5)\n",
    "        nn.init.xavier_uniform_(self.encode_fc2_linear.weight) #Xaviers Initialisation\n",
    "        \n",
    "        \"\"\"\n",
    "        The last fully connected layer's output is the dimensions\n",
    "        of the embeddings?\n",
    "        \n",
    "        PyTorch VAE example uses output of 20 dimensions for mu &\n",
    "        logvariance\n",
    "        \"\"\"\n",
    "#         self.encode_fc31 = nn.Sequential(\n",
    "#             nn.Linear(in_features=5096,\n",
    "#                       out_features=20))\n",
    "        \n",
    "        self.encode_fc31_linear = nn.Linear(in_features=128, \n",
    "                                           out_features=64) ### burasi\n",
    "        self.encode_fc31_relu = nn.ReLU()\n",
    "        self.encode_fc31_dropout = nn.Dropout(0.5)\n",
    "        nn.init.xavier_uniform_(self.encode_fc31_linear.weight) #Xaviers Initialisation\n",
    "\n",
    "        \n",
    "#         self.encode_fc32 = nn.Sequential(\n",
    "#             nn.Linear(in_features=5096,\n",
    "#                       out_features=20))\n",
    "        ## out features daki 10 degisebilir - yukaridaki ile ayni olmali \n",
    "        self.encode_fc32_linear = nn.Linear(in_features=128, \n",
    "                                           out_features=64)\n",
    "        self.encode_fc32_relu = nn.ReLU()\n",
    "        self.encode_fc32_dropout = nn.Dropout(0.5)\n",
    "        nn.init.xavier_uniform_(self.encode_fc32_linear.weight) #Xaviers Initialisation\n",
    "\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        The Decoding Layers\n",
    "        nn.Conv3d -> nn.ConvTranspose3d\n",
    "        nn.MaxPool3d -> nn.MaxUnpool3d\n",
    "        \"\"\"\n",
    "        ## buradaki 10 degisebilir - latent gibi \n",
    "#         self.decode_fc1 = nn.Sequential(\n",
    "#             nn.Linear(in_features=10,\n",
    "#                       out_features=128))\n",
    "\n",
    "        self.decode_fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=64,\n",
    "                      out_features=128))\n",
    "        \n",
    "        self.decode_fc2 = nn.Sequential(\n",
    "            nn.Linear(in_features=128, \n",
    "                      out_features=128), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        #init.xavier_normal(self.fc1.state_dict()['weight'])\n",
    "        \n",
    "        self.decode_fc3 = nn.Sequential(\n",
    "            nn.Linear(in_features = 128,\n",
    "                      out_features = 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        #init.xavier_normal(self.fc2.state_dict()['weight'])\n",
    "        \n",
    "        \n",
    "        self.decode_conv1 = nn.ConvTranspose3d(in_channels=32, \n",
    "                                              out_channels=16, \n",
    "                                              kernel_size=(4,4,4),\n",
    "                                              stride = (2,2,2),\n",
    "                                              padding=(1,1,1))\n",
    "        self.decode_relu1 = nn.ReLU()\n",
    "        self.decode_maxunpool1 = nn.MaxUnpool3d(kernel_size=(2, 2, 2), \n",
    "                                                     stride=(2, 2, 2))\n",
    "        #init.xavier_normal(self.group1.state_dict()['weight'])\n",
    "        \n",
    "        self.decode_conv2 = nn.ConvTranspose3d(in_channels=16, \n",
    "                                              out_channels=8, \n",
    "                                              kernel_size=(4,4,4),\n",
    "                                              stride = (2,2,2),\n",
    "                                              padding=(1,1,1))\n",
    "        self.decode_relu2 = nn.ReLU()\n",
    "        self.decode_maxunpool2 = nn.MaxUnpool3d(kernel_size=(2, 2, 2), \n",
    "                                                     stride=(2, 2, 2))\n",
    "        \n",
    "        self.decode_conv3 = nn.ConvTranspose3d(in_channels=8, \n",
    "                                              out_channels=1, \n",
    "                                              kernel_size=(4,4,4),\n",
    "                                              stride = (2,2,2),\n",
    "                                              padding=(1,1,1))\n",
    "        self.decode_relu3 = nn.ReLU()\n",
    "        self.decode_maxunpool3 = nn.MaxUnpool3d(kernel_size=(2, 2, 2), \n",
    "                                                     stride=(2, 2, 2))\n",
    "        \n",
    "        \n",
    "    # Encoding part of VAE\n",
    "    def encode(self, x):\n",
    "#         h1 = F.relu(self.fc1(x))\n",
    "#         return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "        print(\"Starting Encoding\")\n",
    "#         print(\"----------------------------\")\n",
    "        \n",
    "        out = self.encode_conv1(x)\n",
    "#         print(\"First Conv output shape = \" + str(out.shape))\n",
    "        #print(out.shape)\n",
    "        out = self.encode_relu1(out)\n",
    "#         print(\"First ReLU Layer output shape = \" + str(out.shape))\n",
    "        size1 = out.size()\n",
    "        out, ind1 = self.encode_maxpool1(out)\n",
    "#         print(\"First MaxPooling output shape = \" + str(out.shape))\n",
    "#         print(\"Ind1 shape = \" + str(ind1.shape))\n",
    "#         #print(\"Size1 = \" + str(size1))\n",
    "#         print(\"----------------------------\")\n",
    "        \n",
    "        out = self.encode_conv2(out)\n",
    "#         print(\"Second Conv output shape = \" + str(out.shape))\n",
    "        out = self.encode_relu2(out)\n",
    "#         print(\"Second ReLU Layer output shape = \" + str(out.shape))\n",
    "        size2 = out.size()\n",
    "        out, ind2 = self.encode_maxpool2(out)\n",
    "#         print(\"Second MaxPooling output shape = \" + str(out.shape))\n",
    "#         print(\"Ind2 shape = \" + str(ind2.shape))\n",
    "        #print(\"Size2 = \" + str(size2))\n",
    "#          print(\"----------------------------\")\n",
    "        \n",
    "        out = self.encode_conv3(out)\n",
    "#         print(\"Last Conv output shape = \" + str(out.shape))\n",
    "        out = self.encode_relu3(out)\n",
    "#         print(\"Last ReLU output shape = \" + str(out.shape))\n",
    "        size3 = out.size()\n",
    "#         out, ind3 = self.encode_maxpool3(out)\n",
    "#         print(\"Last Conv Layer output shape = \" + str(out.shape))\n",
    "#         print(\"Ind3 shape = \" + str(ind3.shape))\n",
    "        #print(\"Size3 = \" + str(size3))\n",
    "#         print(\"----------------------------\")\n",
    "\n",
    "        \"\"\"\n",
    "        From here on, the convolutional layers' output is flattened\n",
    "        into a rank 1 tensor of size x & put into a fully connected \n",
    "        network to output ??????\n",
    "        \n",
    "        https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "        PyTorch's own example uses just 2 fully-connected layers\n",
    "        to output mu and logvar predictions, below we use 3.\n",
    "        \"\"\"\n",
    "        #out = out.view(out.size(0), -1)\n",
    "        \n",
    "        \n",
    "        # batch_size = 1 - WORKS\n",
    "#         out = out.view(1, -1)\n",
    "        # batch_size != 1\n",
    "        out = out.view(batch_size, -1)\n",
    "        print(out.shape)\n",
    "        \n",
    "#         print(\"Last Conv Layer output shape after reshaping \\n \\\n",
    "#                 (Input to first FC layer) = \" + str(out.shape))\n",
    "        \n",
    "#         out = self.encode_fc1(out)\n",
    "    \n",
    "        out = self.encode_fc1_linear(out)\n",
    "        out = self.encode_fc1_relu(out)\n",
    "        out = self.encode_fc1_dropout(out)\n",
    "        \n",
    "#         out = self.encode_fc2(out)\n",
    "\n",
    "        out = self.encode_fc2_linear(out)\n",
    "        out = self.encode_fc2_relu(out)\n",
    "        out = self.encode_fc2_dropout(out)\n",
    "        \n",
    "        \n",
    "#         out_mu = self.encode_fc31(out)\n",
    "        \n",
    "        out_mu = self.encode_fc31_linear(out)\n",
    "        out_mu = self.encode_fc31_relu(out_mu)\n",
    "        out_mu = self.encode_fc31_dropout(out_mu)\n",
    "        \n",
    "#         out_logvar = self.encode_fc32(out)\n",
    "\n",
    "        out_logvar = self.encode_fc32_linear(out)\n",
    "        out_logvar = self.encode_fc32_relu(out_logvar)\n",
    "        out_logvar = self.encode_fc32_dropout(out_logvar)\n",
    "        \n",
    "        print(\"Encode - Forward Pass Finished\")\n",
    "        print(out_mu.shape)\n",
    "        print(out_logvar.shape)\n",
    "#         print(\"----------------------------\")\n",
    "        \n",
    "#         return out_mu, out_logvar, [ind1,ind2,ind3], [size1,size2,size3]\n",
    "        return out_mu, out_logvar, [ind1,ind2], [size1,size2]\n",
    "    \n",
    "\n",
    "    # Reparametrization Trick\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        torch.exp = returns a new tensor with the exponential of \n",
    "                    the elements of input\n",
    "        rand_like = returns a tensor with the same size as input\n",
    "                    that is filled with random numbers from a normal\n",
    "                    distribution with mean 0 and variance 1\n",
    "        \n",
    "        \"\"\"\n",
    "#         print(\"Reparametrization...\")\n",
    "#         print(\"logvar (in reparametrization) = \\n\" + str(logvar))\n",
    "#         print(\"logvar (in reparametrization) = \\n\" + str(logvar*1e6))\n",
    "#         print(\"mu * 1e6 (in reparametrization) = \\n\" + str(mu))\n",
    "#         print(\"mu * 1e6 (in reparametrization) = \\n\" + str(mu*1e6))\n",
    "        \n",
    "        std = torch.exp(0.5*logvar)\n",
    "#         std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "#         print(\"0.5*logvar (in reparametrization) = \\n\" + str(0.5*logvar))\n",
    "#         print(\"std (in reparametrization) = \\n\" + str(std))\n",
    "#         print(\"eps (in reparametrization) = \\n\" + str(eps))\n",
    "         \n",
    "#         print(\"eps.mul(std).add_(mu) \\n \\\n",
    "#                 [This is the output from reparameterize()] =\\\n",
    "#                     \\n\" + str(eps.mul(std).add_(mu)))\n",
    "        \n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    \n",
    "    # Decoding part of VAE\n",
    "    def decode(self, z, indices_list, size_list):\n",
    "#         h3 = F.relu(self.fc3(z))\n",
    "#         return torch.sigmoid(self.fc4(h3))\n",
    "#         print(\"----------------------------\")\n",
    "        print(\"Starting Decoding\")\n",
    "#         print(\"z shape = \" + str(z.shape))\n",
    "        \n",
    "        ### buradaki butun outlarin sumlarina bak \n",
    "        out = self.decode_fc1(z)\n",
    "#         print(\"1st FC output shape = \" + str(out.shape))\n",
    "        out = self.decode_fc2(out)\n",
    "#         print(\"2nd FC output shape = \" + str(out.shape))\n",
    "        out = self.decode_fc3(out)\n",
    "#         print(\"Last FC output shape = \" + str(out.shape))\n",
    "        \n",
    "        # batch_size = 1 - WORKS\n",
    "#         out = out.view(1, 32, 4, 4, 4)\n",
    "        # batch_size != 1 \n",
    "        out = out.view(batch_size, 32, 4, 4, 4)        \n",
    "        \n",
    "        \n",
    "#         print(\"First Deconv input shape = \" + str(out.shape))\n",
    "#         print(\"After last convolution (encoding stage) output shape = \" +\\\n",
    "#                   str(indices_list[1].shape))\n",
    "        out = self.decode_conv1(out)\n",
    "#         print(\"First Deconv output shape = \" + str(out.shape))\n",
    "        out = self.decode_relu1(out)\n",
    "#         print(\"First ReLU output shape = \" + str(out.shape))\n",
    "        # maxunpooling needs indices\n",
    "\n",
    "#         out = self.decode_maxunpool1(out,\n",
    "#                              indices = indices_list[1])\n",
    "        out = self.decode_maxunpool1(out,\n",
    "                                     indices = indices_list[1],\n",
    "                                     output_size = size_list[1])\n",
    "#         print(\"2nd MaxUnpool ouput shape = \" + str(out.shape))\n",
    "        \n",
    "        out = self.decode_conv2(out)\n",
    "#         print(\"2nd Deconv output shape = \" + str(out.shape))\n",
    "        out = self.decode_relu2(out)\n",
    "#         print(\"2nd ReLU output shape = \" + str(out.shape))\n",
    "        out = self.decode_maxunpool1(out,\n",
    "                     indices = indices_list[0])\n",
    "#         out = self.decode_maxunpool2(out,\n",
    "#                                      indices= indices_list[1],\n",
    "#                                      output_size = size_list[1])\n",
    "        \n",
    "        out = self.decode_conv3(out)\n",
    "        out = self.decode_relu3(out)\n",
    "#         print(\"Last ReLU output shape = \" + str(out.shape))\n",
    "#         out = self.decode_maxunpool1(out,\n",
    "#                              indices = indices_list[0])\n",
    "        # there is no last maxunpool in https://github.com/pgtgrly/Convolution-Deconvolution-Network-Pytorch/blob/master/Neural_Network_Class.py\n",
    "#         out = self.decode_maxunpool2(out,\n",
    "#                                      indices= indices_list[0],\n",
    "#                                      output_size = size_list[0])\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "    # Forward Pass\n",
    "    def forward(self, x):\n",
    "#         mu, logvar = self.encode(x.view(-1, 784))\n",
    "        mu, logvar, indices_list, size_list = self.encode(x)\n",
    "#         print(\"logvar (after encoding) = \\n\" + str(logvar))\n",
    "#         print(\"mu (after encoding) = \\n\" + str(mu))\n",
    "        \n",
    "        z = self.reparameterize(mu, logvar)\n",
    "#         print(\"z = \")\n",
    "#         print(z)\n",
    "        reconstructed_x = self.decode(z, indices_list, size_list)\n",
    "    \n",
    "        return reconstructed_x , mu, logvar\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=0.005,\n",
    "                      weight_decay=0.001) ## change learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "#     print(\"--------------------------------------\")\n",
    "#     print(\"Calculating Loss...\")\n",
    "#     print(\"recon_x shape = \" + str(recon_x.shape))\n",
    "    \n",
    "#     BCE = F.binary_cross_entropy(recon_x, \n",
    "#                                  x.view(-1, 1, 128, 128, 128), \n",
    "#                                  reduction='sum')\n",
    "#     print(\"BCE Loss = \" + str(BCE))\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    https://towardsdatascience.com/teaching-a-variational-autoencoder-vae-to-draw-mnist-characters-978675c95776\n",
    "    This article is about reconstructing MNIST dataset (2D images with 1 channels)\n",
    "    And it uses squared difference for the reconstruction loss, thus\n",
    "    it is safe to say that for 3D reconstruction we might use the same thing too.\n",
    "    \n",
    "    But the MSE loss seems to be around 1e20 magnitude, thus suggesting some\n",
    "    bug might exist in it.\n",
    "    The multplication by 1e12 is because of the 1e6 division when loading\n",
    "    the dataset\n",
    "    \"\"\"\n",
    "    MSE = F.mse_loss(recon_x, \n",
    "                     x.view(-1, 1, 128, 128, 128), \n",
    "                     reduction='sum') * 1e12 \n",
    "    \n",
    "    print(\"MSE Loss = \" + str(MSE))\n",
    "\n",
    "    \"\"\"\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    \n",
    "    # https://vxlabs.com/2017/12/08/variational-autoencoder-in-pytorch-commented-and-annotated/\n",
    "    # normalises the KLD loss by batch_size\n",
    "    \"\"\"\n",
    "#     print(\"logvar (KLD Loss) = \\n\" + str(logvar))\n",
    "#     print(\"mu (KLD Loss) = \\n\" + str(mu))\n",
    "#     print(\"logvar.exp() (KLD Loss) = \\n\" + str(logvar.exp()))\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    KLD = KLD / float(batch_size)\n",
    "    print(\"KLD Loss = \" + str(KLD))\n",
    "\n",
    "    \n",
    "#     return MSE\n",
    "#     return BCE\n",
    "#     return BCE + KLD\n",
    "    return MSE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "#     for batch_idx, (data, _) in enumerate(train_loader):\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "#         print(batch_idx)\n",
    "#         print(data)\n",
    "        \n",
    "        #print(\"Batch size = \" + str(data.shape))\n",
    "        \n",
    "        data = data.to(device)\n",
    "        print(\"Data transfer to device completed.\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        \n",
    "#         print(\"Reconstructed Input = \\n \" + str(recon_batch))\n",
    "#         print(\"Real Input = \\n \" + str(data))\n",
    "#         print(\"Reconstructed Input Shape = \\n \" + str(recon_batch.shape))\n",
    "#         print(\"Real Input Shape = \\n \" + str(data.shape))\n",
    "\n",
    "#         print(\"logvar = \\n\" + str(logvar))\n",
    "#         print(\"mu = \\n\" + str(mu))\n",
    "\n",
    "        # Plotting Input Cube\n",
    "        print(\"data shape = \" + str(data.view(-1,128,128,128).shape))\n",
    "        print(\"data shape = \" + str(data[0].cpu().view(128,128,128).numpy().shape))\n",
    "\n",
    "        if epoch % (epochs / 4) == 0 and batch_idx == 0:\n",
    "            visualize_cube(cube=data[0].cpu().view(128,128,128).detach().numpy(),\n",
    "                           edge_dim = 128,\n",
    "                           start_cube_index_x = 0,\n",
    "                          start_cube_index_y = 0,\n",
    "                          start_cube_index_z = 0,\n",
    "                          fig_size = (20,20),\n",
    "                          stdev_to_white = 1,\n",
    "                          norm_multiply = 1000,\n",
    "                          color_map = \"Blues\",\n",
    "                          lognormal = False)\n",
    "            \n",
    "            cube_mass_histogram(data[0].cpu().view(128,128,128).detach().numpy())\n",
    "            print (\"Output mass sum (original): \"\\\n",
    "                   + str(np.sum(data[0].cpu().view(128,128,128).detach().numpy())))\n",
    "        \n",
    "        # Plotting Reconstructed Cube\n",
    "        print(\"reconstructed cube = \" + \\\n",
    "              str(recon_batch.view(-1,128,128,128).shape))\n",
    "        print(\"reconstructed cube = \" + \\\n",
    "              str(recon_batch[0].view(128,128,128).shape))\n",
    "\n",
    "\n",
    "        if epoch % (epochs / 4) == 0 and batch_idx == 0:\n",
    "            visualize_cube(cube=recon_batch[0].cpu().view(128,128,128).detach().numpy(),\n",
    "                           edge_dim = 128,\n",
    "                           start_cube_index_x = 0,\n",
    "                          start_cube_index_y = 0,\n",
    "                          start_cube_index_z = 0,\n",
    "                          fig_size = (20,20),\n",
    "                          stdev_to_white = 10,\n",
    "                          norm_multiply = 1000,\n",
    "                          color_map = \"Blues\",\n",
    "                          lognormal = False)\n",
    "            \n",
    "            cube_mass_histogram(cube=recon_batch[0].cpu().view(128,128,128).detach().numpy())\n",
    "            print (\"Output mass sum (reconstructed batch): \"\\\n",
    "                   + str(np.sum(recon_batch[0].cpu().view(128,128,128).detach().numpy())))\n",
    "        \n",
    "        \n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        loss_history.append(loss.item())\n",
    "        \n",
    "        optimizer.step()\n",
    "        # if batch_idx % args.log_interval == 0:\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.12f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.12f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "#         for i, (data, _) in enumerate(test_loader):\n",
    "        for k, data in enumerate(test_loader):\n",
    "            print(\"k = \" + str(k))\n",
    "        \n",
    "            data = data.to(device)\n",
    "            \n",
    "            # Plotting Input Cube\n",
    "            print(\"data shape = \" + str(data.view(-1,128,128,128).shape))\n",
    "            print(\"data shape = \" + str(data[0].view(128,128,128).shape))\n",
    "  \n",
    "            if epoch % (epochs / 4) == 0 and k == 0:\n",
    "                visualize_cube(cube=data[0].view(128,128,128),\n",
    "                               edge_dim = 128,\n",
    "                               start_cube_index_x = 0,\n",
    "                              start_cube_index_y = 0,\n",
    "                              start_cube_index_z = 0,\n",
    "                              fig_size = (20,20),\n",
    "                              stdev_to_white = 1,\n",
    "                              norm_multiply = 1000,\n",
    "                              color_map = \"Blues\",\n",
    "                              lognormal = False)\n",
    "            \n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            \n",
    "            # Plotting Reconstructed Cube\n",
    "            print(\"reconstructed cube = \" + \\\n",
    "                  str(recon_batch.view(-1,128,128,128).shape))\n",
    "            print(\"reconstructed cube = \" + \\\n",
    "                  str(recon_batch[0].view(128,128,128).shape))\n",
    "            \n",
    "            if epoch % (epochs / 4) == 0 and k == 0:\n",
    "                visualize_cube(cube=recon_batch[0].view(128,128,128),\n",
    "                               edge_dim = 128,\n",
    "                               start_cube_index_x = 0,\n",
    "                              start_cube_index_y = 0,\n",
    "                              start_cube_index_z = 0,\n",
    "                              fig_size = (20,20),\n",
    "                              stdev_to_white = 1,\n",
    "                              norm_multiply = 1000,\n",
    "                              color_map = \"Blues\",\n",
    "                              lognormal = False)\n",
    "            \n",
    "            \n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "\n",
    "#             if k == 0:\n",
    "#                 n = min(data.size(0), 8)\n",
    "#                 comparison = torch.cat([data[:n], \n",
    "#                                         recon_batch.view(batch_size, 1, 128, 128, 128)[:n]])\n",
    "                                      #recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
    "#                                         recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "#                 save_image(comparison.cpu(),\n",
    "#                          'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.12f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    loss_history = []\n",
    "    #for epoch in range(1, args.epochs + 1):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(\"Epoch = \" + str(epoch) + \" / \" + str(epochs))\n",
    "        \n",
    "        train(epoch)\n",
    "        \n",
    "        # Plotting Training Losses\n",
    "        plt.figure(figsize=(16,16))\n",
    "        plt.ylim(-0.0001,0.2)\n",
    "        plt.plot(loss_history)\n",
    "        plt.show()\n",
    "        \n",
    "#         test(epoch)\n",
    "        \n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             sample = torch.randn(64, 20).to(device)\n",
    "#             sample = model.decode(sample).cpu()\n",
    "#             save_image(sample.view(64, 1, 28, 28),\n",
    "#                        'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
