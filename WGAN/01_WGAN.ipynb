{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/martinarjovsky/WassersteinGAN\n",
    "\n",
    "https://vincentherrmann.github.io/blog/wasserstein/\n",
    "\n",
    "https://github.com/martinarjovsky/WassersteinGAN/issues/2\n",
    "\n",
    "https://github.com/caogang/wgan-gp/blob/master/gan_cifar10.py\n",
    "\n",
    "# To do:\n",
    "\n",
    "* Clip and then intorudce logs (should go to -17,9)\n",
    "* Overfit small dataset\n",
    "* Add confidence intervals to (mean and std)\n",
    "* Upload redshift 5\n",
    "* Fix sns denisty plots\n",
    "* Check for overfitting\n",
    "* Play around with optimizers\n",
    "* Clean code, integrate transformations\n",
    "* Revise and change architecture (less convs, polling, FC), add convolution after ReLU in generator\n",
    "* Mean and standard errors of power spectrums\n",
    "* Increase channels\n",
    "* Bigger cubes with no padding\n",
    "* Use other standarizations of data\n",
    "* Gradient penalty instead of weight clamping\n",
    "* Locally connected layers\n",
    "* Weight initialization\n",
    "* Mini batch discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#import argparse\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "#import torch\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method('spawn')\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable, grad\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import h5py\n",
    "\n",
    "import itertools\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import colors\n",
    "import matplotlib as mpl\n",
    "import timeit\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "ngpu = 1   \n",
    "\n",
    "from power_spectrum_utils import power_spectrum_np\n",
    "from wgan_utils import check_coords #, define_test\n",
    "#from data_utils import get_max_cube, get_min_cube, get_mean_cube, get_stddev_cube\n",
    "from plot_utils import plot_loss, plot_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mean_5=get_mean_cube('../../../../../','5.0')\n",
    "#std_5=get_stddev_cube('../../../../../','5.0',mean_5)\n",
    "#\n",
    "mean_5=14592.24\n",
    "std_5=922711.56\n",
    "max_5=4376932000\n",
    "mask_value=10**2\n",
    "\n",
    "#Bootstrap\n",
    "mean_l5 = 2.7784111\n",
    "std_l5 = 1.5777067\n",
    "max_l5 =22.199614"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get_ipython().run_line_magic('run', '../mmd_gan/utils/power_spectrum_utils.py')\n",
    "#get_ipython().run_line_magic('run', '../mmd_gan/utils/plot_utils.py')\n",
    "\n",
    "def moving_average(data_set, periods=3):\n",
    "    weights = np.ones(periods) / periods\n",
    "    return np.convolve(data_set, weights, mode='valid')\n",
    "\n",
    "\n",
    "\n",
    "def plot_power_spec(real_cube,        # should be inverse_transformed\n",
    "                    generated_cube,   # should be inverse_transformed\n",
    "                    raw_cube_mean,\n",
    "                    s_size,\n",
    "                    threads=1, \n",
    "                    MAS=\"CIC\", \n",
    "                    axis=0, \n",
    "                    BoxSize=75.0/2048*64):\n",
    "    \"\"\"Takes as input;\n",
    "    - Real cube: (batch_size x 1 x n x n x n) torch cuda FloatTensor,\n",
    "    - Generated copy: (batch_size x 1 x n x n x n) torch cuda FloatTensor,\n",
    "    - constant assignments: threads, MAS, axis, BoxSize.\n",
    "    \n",
    "    Returns;\n",
    "    - Power spectrum plots of both cubes\n",
    "    in the same figure.\n",
    "    \"\"\"\n",
    "    real_cube = real_cube.reshape(-1,\n",
    "                                  1,\n",
    "                                  real_cube.shape[2],\n",
    "                                  real_cube.shape[2],\n",
    "                                  real_cube.shape[2])\n",
    "    generated_cube = generated_cube.reshape(-1,\n",
    "                                            1,\n",
    "                                            generated_cube.shape[2],\n",
    "                                            generated_cube.shape[2],\n",
    "                                            generated_cube.shape[2])\n",
    "    \n",
    "    #print(\"number of samples of real and generated cubes = \" + str(real_cube.shape[0]))\n",
    "\n",
    "    plt.figure(figsize=(14,8))\n",
    "    \n",
    "    for cube_no in range(real_cube.shape[0]):\n",
    "        \n",
    "        delta_real_cube = real_cube[cube_no][0]\n",
    "        delta_gen_cube = generated_cube[cube_no][0]\n",
    "        \n",
    "        k_real ,Pk_real = power_spectrum_np(cube = delta_real_cube, mean_raw_cube = raw_cube_mean, SubBoxSize=s_sample)\n",
    "        k_gen,  Pk_gen  = power_spectrum_np(cube = delta_gen_cube,  mean_raw_cube = raw_cube_mean, SubBoxSize=s_sample)\n",
    "    \n",
    "        plt.plot(np.log(k_real), np.log(Pk_real), color=\"b\", alpha = 0.8, label=\"Real\", linewidth = 2)\n",
    "        plt.plot(np.log(k_gen), np.log(Pk_gen), color=\"r\", alpha = 0.8, label=\"Generated\", linewidth = 2)\n",
    "        \n",
    "        plt.rcParams[\"font.size\"] = 16\n",
    "        plt.title(\"Power Spectrum Comparison - (Blue: Real, Red: Generated)\")\n",
    "        plt.xlabel('logk')\n",
    "        plt.ylabel('log(Pk.k3D)')\n",
    "        #plt.xlim(0, 3)\n",
    "#         plt.legend()\n",
    "    \n",
    "    #if save_plot:\n",
    "     #   plt.savefig(redshift_fig_folder + 'powerspectrum_' + str(t) + '.png', \n",
    "      #              bbox_inches='tight')\n",
    "   \n",
    "    plt.show()\n",
    "    #return np.log10(k_real), np.log10(k_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HydrogenDataset(Dataset):\n",
    "    \"\"\"Hydrogen Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, h5_file, s_test, s_sample, nsamples,transform, rotate):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h5_file (string): name of the h5 file with 32 sampled cubes.\n",
    "            root_dir (string): Directory with the .h5 file.\n",
    "        \"\"\"\n",
    "        #file_size = os.path.getsize(root_dir + h5_file) / 1e6 # in MBs\n",
    "        #print(\"The whole file size is \" + str(int(file_size)) + \" MBs\")\n",
    "        # self.subcubes = h5py.File('../data/sample_32.h5', 'r')\n",
    "        self.s_test = s_test\n",
    "        self.s_sample = s_sample\n",
    "        self.t_coords = {'x': [0, 1023], 'y': [0, 1023], 'z': [0, 1023]} # define_test(self.s_test,self.s_sample)\n",
    "        self.nsamples = nsamples\n",
    "        self.redshift = redshift\n",
    "        self.transform=transform\n",
    "        self.rotate=rotate\n",
    "        self.file=h5_file\n",
    "\n",
    "    def __len__(self):\n",
    "        # Function called when len(self) is executed\n",
    "        return self.nsamples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        This can be implemented in such a way that the whole h5 file read \n",
    "        using h5py.File() and get_sample() function is called to return\n",
    "        a random subcube. This won't increase memory usage because the\n",
    "        subcubes will be read in the same way and only the batch will\n",
    "        be read into memory.\n",
    "        \n",
    "        Here we have implemented it so that it can be used with data\n",
    "        generated by get_sample() function.\n",
    "        \n",
    "        The output of this function is one subcube with the dimensions\n",
    "        specified by get_sample() implementation.\n",
    "        \"\"\"\n",
    "        \n",
    "        sample = get_samples(file = self.file,\n",
    "                            s_sample = self.s_sample,\n",
    "                             nsamples = 1,\n",
    "                             test_coords = self.t_coords)\n",
    "        \n",
    "        if self.rotate==True:\n",
    "            t=random.randint(1,4)\n",
    "            if t==4:\n",
    "                sample=sample\n",
    "            else:\n",
    "                axis=random.choice([(0,1), (0,2), (1,0), (1,2), (2,0), (2,1) ])\n",
    "                sample=np.rot90(sample, t, axis)\n",
    "        \n",
    "        #sample=np.array(sample)\n",
    "        if self.transform=='max':\n",
    "            #print(sample.shape)\n",
    "            sample = sample / self.max_cube\n",
    "        if self.transform=='normalize':\n",
    "            #sample = (sample-mean_5)/std_5\n",
    "            sample = (sample-mean_5)/std_5\n",
    "            #sample[sample <10**2]==0\n",
    "            #sample[sample > 3*std_5]==3*std_5\n",
    "            #sample=np.log(sample+1)\n",
    "        if self.transform=='clip_log':\n",
    "            sample[sample <10**2]==0\n",
    "            sample[sample > 3*std_5]==3*std_5\n",
    "            sample=np.log(sample+1)\n",
    "        \n",
    "        if self.transform=='log_max':\n",
    "            #sample[sample==0]=0.000000001\n",
    "            sample=np.log(sample+1) / max_l5\n",
    "            \n",
    "        if self.transform=='log':\n",
    "            sample= np.log(sample+1)\n",
    "        #print(np.array(sample).shape)\n",
    "    \n",
    "        sample=sample.reshape((1, self.s_sample, self.s_sample, self.s_sample))\n",
    "\n",
    "        return torch.tensor(sample)\n",
    "\n",
    "def get_samples(file, s_sample, nsamples, test_coords):\n",
    "    #n is size of minibatch, get valid samples (not intersecting with test_coords)\n",
    "    sample_list=[]\n",
    "    m=2048-s_sample\n",
    "    for n in range(nsamples):\n",
    "        #print(\"Sample No = \" + str(n + 1) + \" / \" + str(nsamples))\n",
    "        sample_valid=False\n",
    "        while sample_valid==False:\n",
    "            x = random.randint(0,m)\n",
    "            y = random.randint(0,m)\n",
    "            z = random.randint(0,m)\n",
    "            sample_coords = {'x':[x,x+s_sample], \n",
    "                             'y':[y,y+s_sample], \n",
    "                             'z':[z,z+s_sample]}\n",
    "            \n",
    "            sample_valid = check_coords(test_coords, sample_coords)\n",
    "        \n",
    "        sample_list.append(sample_coords)\n",
    "    \n",
    "    #Load cube and get samples and convert them to np.arrays\n",
    "    sample_array=[]\n",
    "    #f file has to be opened outisde the function\n",
    "    for c in sample_list:\n",
    "        a = f[c['x'][0]:c['x'][1],\n",
    "              c['y'][0]:c['y'][1],\n",
    "              c['z'][0]:c['z'][1]]\n",
    "        \n",
    "        sample_array.append(np.array(a))\n",
    "    \n",
    "    return np.array(sample_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize2d(real, fake, log=False, save=''):\n",
    "    #min_=-.5\n",
    "    min_= 0\n",
    "    max_= .3\n",
    "    #max_=(max_l5-mean_l5) / std_l5\n",
    "    \n",
    "    cols=8\n",
    "    rows=2\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=2, ncols=cols, figsize=(16,4))\n",
    "    \n",
    "    for ax, row in zip(axes[:,0], ['Generated', 'Real']):\n",
    "        ax.set_ylabel(row, rotation=90, fontsize=16)\n",
    "    \n",
    "    mf, mr= 0,0\n",
    "    for ax in axes.flat:\n",
    "        #Plot only half of the mini-batch\n",
    "        if mf<cols:\n",
    "            if log==True:\n",
    "                im = ax.imshow(np.log(fake[mf][0].mean(axis=2)), aspect='equal', #cmap='Blues',\n",
    "                       interpolation=None, vmin=min_, vmax=max_)\n",
    "            else:\n",
    "                im = ax.imshow((fake[mf][0].mean(axis=2)), aspect='equal', #cmap='Blues',\n",
    "                       interpolation=None, vmin=min_, vmax=max_)\n",
    "            mf+=1\n",
    "        else:\n",
    "            if log==True:\n",
    "                im = ax.imshow(np.log(real[mr][0].mean(axis=2)), aspect='equal', #cmap='Blues',\n",
    "                       interpolation=None, vmin=min_, vmax=max_)\n",
    "            else:\n",
    "                im = ax.imshow((real[mr][0].mean(axis=2)), aspect='equal', #cmap='Blues',\n",
    "                       interpolation=None, vmin=min_, vmax=max_)\n",
    "                \n",
    "            mr+=1\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    #fig.subplots_adjust(right=.8)\n",
    "    #cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "    #fig.colorbar(ax, cax=cbar_ax)\n",
    "    fig.tight_layout() \n",
    "    if save!='':\n",
    "        fig.savefig(save+'samples.png', dpi=1000)\n",
    "           \n",
    "    plt.show()  \n",
    "    \n",
    "def hist_plot(noise, real, log_plot, redshift_fig_folder) :\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        real(): real data\n",
    "        epoch(integer): epoch number\n",
    "        file_name(string): name of the file\n",
    "        hd (integer) : if 0 it's a histogram, if 1 it's a pdf\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize = (20,10))\n",
    "    plot_min = min(float(noise.min()), float(real.min()))\n",
    "    plot_max = max(float(noise.max()), float(real.max()))\n",
    "    plt.xlim(plot_min,plot_max)\n",
    "    \n",
    "    bins = np.linspace(plot_min,plot_max,400)\n",
    "    \n",
    "    real_label = \"Real Subcube - Only Nonzero\"\n",
    "    noise_label = \"Noise Subcube - Only Nonzero\"\n",
    "    \n",
    "    for m in range(noise.shape[0]):\n",
    "        if m<=8:\n",
    "            plt.hist(real[m][0].flatten(), bins = bins, color = \"b\" , log = log_plot, alpha = 0.3, label = real_label, normed=True)\n",
    "            plt.hist(noise[m][0].flatten(), bins = bins, color = \"r\" , log = log_plot, alpha= 0.3, label = noise_label, normed=True)\n",
    "\n",
    "    #plt.legend()\n",
    "    #plt.savefig(redshift_fig_folder + file_name, bbox_inches='tight')   \n",
    "    plt.title('Blue: Real/ Red: Generated', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "'''\n",
    "\n",
    "#https://github.com/caogang/wgan-gp/blob/master/gan_cifar10.py\n",
    "def calc_gradient_penalty(real_data, fake_data, lambda_):\n",
    "    # print \"real_data: \", real_data.size(), fake_data.size()\n",
    "    BATCH_SIZE=real_data.size()[0]\n",
    "    \n",
    "    alpha = torch.rand(BATCH_SIZE, 1, 1)\n",
    "    alpha = alpha.expand(BATCH_SIZE, real_data.nelement()//BATCH_SIZE).contiguous().view(BATCH_SIZE, 1,\n",
    "                                                                                         s_sample, s_sample,s_sample)\n",
    "    alpha = alpha.cuda() if cuda else alpha\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "\n",
    "    if cuda:\n",
    "        interpolates = interpolates.cuda()\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=torch.ones(disc_interpolates.size()).cuda() if cuda else torch.ones(\n",
    "                                  disc_interpolates.size()),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    \n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_\n",
    "    return gradient_penalty\n",
    "\n",
    "'''\n",
    "\n",
    "##https://github.com/EmilienDupont/wgan-gp/blob/master/training.py\n",
    "def _gradient_penalty(real_data, generated_data, gp_weight):\n",
    "        batch_size = real_data.size()[0]\n",
    "\n",
    "        # Calculate interpolation\n",
    "        alpha = torch.rand(batch_size, 1, 1, 1, 1)\n",
    "        alpha = alpha.expand_as(real_data)\n",
    "        if cuda:\n",
    "            alpha = alpha.cuda()\n",
    "        interpolated = alpha * real_data.data + (1 - alpha) * generated_data.data\n",
    "        interpolated = Variable(interpolated, requires_grad=True)\n",
    "        if cuda:\n",
    "            interpolated = interpolated.cuda()\n",
    "\n",
    "        # Calculate probability of interpolated examples\n",
    "        prob_interpolated = netD(interpolated)\n",
    "\n",
    "        # Calculate gradients of probabilities with respect to examples\n",
    "        gradients = grad(outputs=prob_interpolated, inputs=interpolated,\n",
    "                               grad_outputs=torch.ones(prob_interpolated.size()).cuda() if cuda else torch.ones(\n",
    "                               prob_interpolated.size()),\n",
    "                               create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "        # Gradients have shape (batch_size, num_channels, img_width, img_height),\n",
    "        # so flatten to easily take norm per example in batch\n",
    "        gradients = gradients.view(batch_size, -1)\n",
    "        #self.losses['gradient_norm'].append(gradients.norm(2, dim=1).mean().data[0])\n",
    "\n",
    "        # Derivatives of the gradient close to 0 can cause problems because of\n",
    "        # the square root, so manually calculate norm and add epsilon\n",
    "        gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
    "\n",
    "        # Return gradient penalty\n",
    "        return gp_weight * ((gradients_norm - 1) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DCGAN_G(nn.Module):\n",
    "    def __init__(self, isize, nz, nc, ngf, ngpu, n_extra_layers):\n",
    "        super(DCGAN_G, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, tisize = ngf//2, 4\n",
    "        while tisize != isize:\n",
    "            cngf = cngf * 2\n",
    "            tisize = tisize * 2\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        #main.add_module('initial_fc'.format(nz, nz),\n",
    "        #               nn.ConvTranspose2d(nz, nz, 1, 1, 0),)\n",
    "                   \n",
    "       #     nn.LeakyReLU(0.2, inplace=True),\n",
    "        # input is Z, going into a convolution\n",
    "        main.add_module('initial_{0}-{1}_convt'.format(nz, cngf),\n",
    "                        nn.ConvTranspose3d(nz, cngf, 4, 1, 0, bias=False))\n",
    "        main.add_module('initial_{0}_batchnorm'.format(cngf),\n",
    "                        nn.BatchNorm3d(cngf))\n",
    "        main.add_module('initial_{0}_relu'.format(cngf),\n",
    "                        nn.ReLU(True))\n",
    "\n",
    "        csize, cndf = 4, cngf\n",
    "        while csize < isize//2:\n",
    "            main.add_module('pyramid_{0}-{1}_convt'.format(cngf, cngf//2),\n",
    "                            nn.ConvTranspose3d(cngf, cngf//2, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid_{0}_batchnorm'.format(cngf//2),\n",
    "                            nn.BatchNorm3d(cngf//2))\n",
    "            main.add_module('pyramid_{0}_relu'.format(cngf//2),\n",
    "                            nn.ReLU(True))\n",
    "            cngf = cngf // 2\n",
    "            csize = csize * 2\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}_{1}_conv'.format(t, cngf),\n",
    "                            nn.Conv3d(cngf, cngf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}_{1}_batchnorm'.format(t, cngf),\n",
    "                            nn.BatchNorm3d(cngf))\n",
    "            main.add_module('extra-layers-{0}_{1}_relu'.format(t, cngf),\n",
    "                            nn.ReLU(True))\n",
    "\n",
    "        main.add_module('final_{0}-{1}_convt'.format(cngf, nc),\n",
    "                        nn.ConvTranspose3d(cngf, nc, 4, 2, 1, bias=False))\n",
    "        main.add_module('final_{0}_tanh'.format(nc),\n",
    "                        nn.Sigmoid())\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else: \n",
    "            output = self.main(input)\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DCGAN_D(nn.Module):\n",
    "    def __init__(self, isize, nz, nc, ndf, ngpu, n_extra_layers):\n",
    "        super(DCGAN_D, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial_conv_{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv3d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial_relu_{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        \n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}_{1}_conv'.format(t, cndf),\n",
    "                            nn.Conv3d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            #main.add_module('extra-layers-{0}_{1}_batchnorm'.format(t, cndf),\n",
    "            #                nn.BatchNorm3d(cndf))\n",
    "            main.add_module('extra-layers-{0}_{1}_relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid_{0}-{1}_conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv3d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "           # main.add_module('pyramid_{0}_batchnorm'.format(out_feat),\n",
    "            #                nn.BatchNorm3d(out_feat))\n",
    "            main.add_module('pyramid_{0}_relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "        main.add_module('final_{0}-{1}_conv'.format(cndf, 1),\n",
    "                        nn.Conv3d(cndf, 1, 4, 1, 0, bias=False))\n",
    "        self.main = main\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else: \n",
    "            output = self.main(input)\n",
    "            \n",
    "        output = output.mean(0)\n",
    "        return output.view(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCGAN_G(\n",
      "  (main): Sequential(\n",
      "    (initial_100-1024_convt): ConvTranspose3d(100, 1024, kernel_size=(4, 4, 4), stride=(1, 1, 1), bias=False)\n",
      "    (initial_1024_batchnorm): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (initial_1024_relu): ReLU(inplace)\n",
      "    (pyramid_1024-512_convt): ConvTranspose3d(1024, 512, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "    (pyramid_512_batchnorm): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (pyramid_512_relu): ReLU(inplace)\n",
      "    (pyramid_512-256_convt): ConvTranspose3d(512, 256, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "    (pyramid_256_batchnorm): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (pyramid_256_relu): ReLU(inplace)\n",
      "    (pyramid_256-128_convt): ConvTranspose3d(256, 128, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "    (pyramid_128_batchnorm): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (pyramid_128_relu): ReLU(inplace)\n",
      "    (final_128-1_convt): ConvTranspose3d(128, 1, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "    (final_1_tanh): Sigmoid()\n",
      "  )\n",
      ")\n",
      "DCGAN_D(\n",
      "  (main): Sequential(\n",
      "    (initial_conv_1-128): Conv3d(1, 128, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "    (initial_relu_128): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (pyramid_128-256_conv): Conv3d(128, 256, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "    (pyramid_256_relu): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (pyramid_256-512_conv): Conv3d(256, 512, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "    (pyramid_512_relu): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (pyramid_512-1024_conv): Conv3d(512, 1024, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "    (pyramid_1024_relu): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (final_1024-1_conv): Conv3d(1024, 1, kernel_size=(4, 4, 4), stride=(1, 1, 1), bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#New training scripts\n",
    "redshift ='5.0'\n",
    "cuda = True\n",
    "n_samples = 64000\n",
    "s_sample = 64\n",
    "batchSize = 8\n",
    "n_extra_layers_G = 0\n",
    "n_extra_layers_D = 0\n",
    "\n",
    "nz = 100\n",
    "nc = 1\n",
    "ngf = 128\n",
    "ndf = ngf\n",
    "\n",
    "niter = 30 #epochs\n",
    "Diters_init = 5\n",
    "\n",
    "manualSeed = 1   # 'manual seed'\n",
    "\n",
    "#.0002\n",
    "#Default: 0.00005\n",
    "lrD = 0.00004\n",
    "lrG = 0.00004\n",
    "\n",
    "#Clamp weights or use GP\n",
    "#clamp_lower = -.012  #Default .01\n",
    "#clamp_upper = .012\n",
    "lambda_ = 10\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "experiment='first_iteration/'\n",
    "\n",
    "DCGAN=True\n",
    "#DC\n",
    "if DCGAN == True:\n",
    "    netG = DCGAN_G(s_sample, nz, nc, ngf, ngpu, n_extra_layers_G)\n",
    "    netD = DCGAN_D(s_sample, nz, nc, ngf, ngpu, n_extra_layers_D)\n",
    "    #netG = DCGAN_G(ngpu, 4, 1, False)\n",
    "    #netD = DCGAN_D(ngpu, 4, 1, False)\n",
    "#MLP\n",
    "else:\n",
    "    netG = MLP_G(s_sample, nz, nc, ngf, ngpu)\n",
    "    netD = MLP_D(s_sample, nz, nc, ndf, ngpu)\n",
    "\n",
    "#netG_arg = \"first_iteration/saved/DCGAN/netG_epoch_63.pth\"        # \"path to netG (to continue training) \"\n",
    "#netD_arg = \"first_iteration/saved//DCGAN/netD_epoch_63.pth\"  \n",
    "netG_arg = ''\n",
    "netD_arg = ''\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "\n",
    "if netG_arg != '':\n",
    "    netG.load_state_dict(torch.load(netG_arg))\n",
    "\n",
    "if netD_arg != '':\n",
    "    netD.load_state_dict(torch.load(netD_arg))\n",
    "\n",
    "\n",
    "datapath='../../../../../'\n",
    "f = h5py.File(datapath+'fields_z='+redshift+'.hdf5', 'r')\n",
    "f=f['delta_HI']\n",
    "    \n",
    "dataset = HydrogenDataset( h5_file=0,\n",
    "                            s_test = 1024, \n",
    "                            s_sample = s_sample, \n",
    "                            nsamples = n_samples, \n",
    "                            transform='log_max',\n",
    "                            rotate=True)\n",
    "\n",
    "workers=0\n",
    "dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                         batch_size = batchSize,\n",
    "                                         shuffle=False, \n",
    "                                         #num_workers=int(opt.workers)\n",
    "                                         num_workers=int(workers),\n",
    "                                        drop_last = True)\n",
    "\n",
    "print(netG)\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/30][20/8000][1] Loss_D: -6234.222656 Loss_G: 10253.004883 Loss_D_real: 2140.118164 Loss_D_fake 8374.340820\n",
      "[0/30][40/8000][2] Loss_D: -71655.343750 Loss_G: 99240.726562 Loss_D_real: 24794.707031 Loss_D_fake 96450.046875\n",
      "[0/30][60/8000][3] Loss_D: -78862.484375 Loss_G: 108524.773438 Loss_D_real: 30429.722656 Loss_D_fake 109292.203125\n",
      "[0/30][80/8000][4] Loss_D: -78524.609375 Loss_G: 111972.343750 Loss_D_real: 31087.251953 Loss_D_fake 109611.859375\n",
      "[0/30][100/8000][5] Loss_D: -83420.539062 Loss_G: 113527.445312 Loss_D_real: 31173.689453 Loss_D_fake 114594.226562\n",
      "[0/30][120/8000][6] Loss_D: -80328.515625 Loss_G: 112349.984375 Loss_D_real: 30418.375000 Loss_D_fake 110746.890625\n",
      "[0/30][140/8000][7] Loss_D: -84146.914062 Loss_G: 114361.125000 Loss_D_real: 32731.302734 Loss_D_fake 116878.218750\n",
      "[0/30][160/8000][8] Loss_D: -76770.890625 Loss_G: 107718.812500 Loss_D_real: 34148.093750 Loss_D_fake 110918.984375\n",
      "[0/30][180/8000][9] Loss_D: -82542.734375 Loss_G: 116253.875000 Loss_D_real: 28336.136719 Loss_D_fake 110878.875000\n",
      "[0/30][200/8000][10] Loss_D: -77473.093750 Loss_G: 108482.500000 Loss_D_real: 32341.003906 Loss_D_fake 109814.101562\n",
      "[0/30][220/8000][11] Loss_D: -75898.781250 Loss_G: 108766.625000 Loss_D_real: 29927.367188 Loss_D_fake 105826.148438\n",
      "[0/30][240/8000][12] Loss_D: -74120.460938 Loss_G: 101537.015625 Loss_D_real: 29959.025391 Loss_D_fake 104079.484375\n",
      "[0/30][260/8000][13] Loss_D: -72134.640625 Loss_G: 103526.695312 Loss_D_real: 28317.757812 Loss_D_fake 100452.398438\n",
      "[0/30][280/8000][14] Loss_D: -69964.593750 Loss_G: 101199.273438 Loss_D_real: 28896.337891 Loss_D_fake 98860.929688\n",
      "[0/30][300/8000][15] Loss_D: -74989.859375 Loss_G: 101518.593750 Loss_D_real: 28326.185547 Loss_D_fake 103316.046875\n",
      "[0/30][320/8000][16] Loss_D: -68696.640625 Loss_G: 97161.453125 Loss_D_real: 28120.527344 Loss_D_fake 96817.171875\n",
      "[0/30][340/8000][17] Loss_D: -57759.757812 Loss_G: 87705.242188 Loss_D_real: 29073.328125 Loss_D_fake 86833.085938\n",
      "[0/30][360/8000][18] Loss_D: -63189.210938 Loss_G: 90511.710938 Loss_D_real: 29768.775391 Loss_D_fake 92957.984375\n",
      "[0/30][380/8000][19] Loss_D: -58085.750000 Loss_G: 84153.625000 Loss_D_real: 28135.623047 Loss_D_fake 86221.375000\n",
      "[0/30][400/8000][20] Loss_D: -56203.976562 Loss_G: 81795.343750 Loss_D_real: 28514.271484 Loss_D_fake 84718.250000\n",
      "[0/30][420/8000][21] Loss_D: -53497.765625 Loss_G: 80081.804688 Loss_D_real: 26376.849609 Loss_D_fake 79874.617188\n",
      "[0/30][440/8000][22] Loss_D: -55758.804688 Loss_G: 80452.226562 Loss_D_real: 25847.710938 Loss_D_fake 81606.515625\n",
      "[0/30][460/8000][23] Loss_D: -50285.464844 Loss_G: 76192.867188 Loss_D_real: 23426.691406 Loss_D_fake 73712.156250\n",
      "[0/30][480/8000][24] Loss_D: -48303.281250 Loss_G: 72696.835938 Loss_D_real: 23237.365234 Loss_D_fake 71540.648438\n",
      "[0/30][500/8000][25] Loss_D: -46646.171875 Loss_G: 69125.359375 Loss_D_real: 25578.462891 Loss_D_fake 72224.632812\n",
      "[0/30][505/8000][26] Loss_D: -46213.656250 Loss_G: 70424.906250 Loss_D_real: 22527.626953 Loss_D_fake 68741.281250\n",
      "[0/30][510/8000][27] Loss_D: -43463.460938 Loss_G: 68360.218750 Loss_D_real: 20659.751953 Loss_D_fake 64123.214844\n",
      "[0/30][515/8000][28] Loss_D: -41888.117188 Loss_G: 63234.816406 Loss_D_real: 22565.082031 Loss_D_fake 64453.199219\n",
      "[0/30][520/8000][29] Loss_D: -43142.511719 Loss_G: 65691.492188 Loss_D_real: 21080.148438 Loss_D_fake 64222.660156\n",
      "[0/30][525/8000][30] Loss_D: -38699.664062 Loss_G: 60887.531250 Loss_D_real: 19105.888672 Loss_D_fake 57805.550781\n",
      "[0/30][530/8000][31] Loss_D: -37361.742188 Loss_G: 58389.582031 Loss_D_real: 21298.015625 Loss_D_fake 58659.757812\n",
      "[0/30][535/8000][32] Loss_D: -35150.773438 Loss_G: 55061.367188 Loss_D_real: 21044.109375 Loss_D_fake 56194.882812\n",
      "[0/30][540/8000][33] Loss_D: -34395.281250 Loss_G: 53503.855469 Loss_D_real: 20455.031250 Loss_D_fake 54850.312500\n",
      "[0/30][545/8000][34] Loss_D: -34416.289062 Loss_G: 54731.035156 Loss_D_real: 18908.912109 Loss_D_fake 53325.203125\n",
      "[0/30][550/8000][35] Loss_D: -31894.304688 Loss_G: 51965.558594 Loss_D_real: 18142.667969 Loss_D_fake 50036.972656\n",
      "[0/30][555/8000][36] Loss_D: -28340.294922 Loss_G: 45468.351562 Loss_D_real: 20347.623047 Loss_D_fake 48687.917969\n",
      "[0/30][560/8000][37] Loss_D: -29366.238281 Loss_G: 46885.769531 Loss_D_real: 22556.675781 Loss_D_fake 51922.914062\n",
      "[0/30][565/8000][38] Loss_D: -29380.720703 Loss_G: 47287.046875 Loss_D_real: 18385.322266 Loss_D_fake 47766.042969\n",
      "[0/30][570/8000][39] Loss_D: -26375.425781 Loss_G: 43426.125000 Loss_D_real: 17947.031250 Loss_D_fake 44322.457031\n",
      "[0/30][575/8000][40] Loss_D: -26452.656250 Loss_G: 43860.242188 Loss_D_real: 15095.989258 Loss_D_fake 41548.644531\n",
      "[0/30][580/8000][41] Loss_D: -26493.382812 Loss_G: 42656.492188 Loss_D_real: 17089.640625 Loss_D_fake 43583.023438\n",
      "[0/30][585/8000][42] Loss_D: -24615.472656 Loss_G: 40732.242188 Loss_D_real: 16599.281250 Loss_D_fake 41214.753906\n",
      "[0/30][590/8000][43] Loss_D: -22675.646484 Loss_G: 38262.714844 Loss_D_real: 15854.173828 Loss_D_fake 38529.820312\n",
      "[0/30][595/8000][44] Loss_D: -21927.882812 Loss_G: 36971.914062 Loss_D_real: 15660.468750 Loss_D_fake 37588.351562\n",
      "[0/30][600/8000][45] Loss_D: -24757.097656 Loss_G: 40820.597656 Loss_D_real: 13468.180664 Loss_D_fake 38225.277344\n",
      "[0/30][605/8000][46] Loss_D: -19833.550781 Loss_G: 34024.128906 Loss_D_real: 14086.405273 Loss_D_fake 33919.957031\n",
      "[0/30][610/8000][47] Loss_D: -19823.113281 Loss_G: 33494.058594 Loss_D_real: 15451.632812 Loss_D_fake 35274.746094\n",
      "[0/30][615/8000][48] Loss_D: -20704.873047 Loss_G: 34824.753906 Loss_D_real: 12980.576172 Loss_D_fake 33685.449219\n",
      "[0/30][620/8000][49] Loss_D: -17594.453125 Loss_G: 31438.722656 Loss_D_real: 12218.023438 Loss_D_fake 29812.476562\n",
      "[0/30][625/8000][50] Loss_D: -17345.042969 Loss_G: 29627.718750 Loss_D_real: 13603.014648 Loss_D_fake 30948.056641\n",
      "[0/30][630/8000][51] Loss_D: -17555.265625 Loss_G: 31029.812500 Loss_D_real: 12524.022461 Loss_D_fake 30079.289062\n",
      "[0/30][635/8000][52] Loss_D: -16061.690430 Loss_G: 27575.232422 Loss_D_real: 14119.973633 Loss_D_fake 30181.664062\n",
      "[0/30][640/8000][53] Loss_D: -16874.507812 Loss_G: 28817.019531 Loss_D_real: 13525.298828 Loss_D_fake 30399.806641\n",
      "[0/30][645/8000][54] Loss_D: -14874.725586 Loss_G: 26774.134766 Loss_D_real: 12060.948242 Loss_D_fake 26935.673828\n",
      "[0/30][650/8000][55] Loss_D: -14885.130859 Loss_G: 27125.476562 Loss_D_real: 10479.642578 Loss_D_fake 25364.773438\n",
      "[0/30][655/8000][56] Loss_D: -14021.783203 Loss_G: 25641.755859 Loss_D_real: 11330.587891 Loss_D_fake 25352.371094\n",
      "[0/30][660/8000][57] Loss_D: -12687.349609 Loss_G: 23032.111328 Loss_D_real: 11723.554688 Loss_D_fake 24410.904297\n",
      "[0/30][665/8000][58] Loss_D: -12480.877930 Loss_G: 22717.716797 Loss_D_real: 11368.790039 Loss_D_fake 23849.667969\n",
      "[0/30][670/8000][59] Loss_D: -12021.030273 Loss_G: 22353.408203 Loss_D_real: 10195.471680 Loss_D_fake 22216.501953\n",
      "[0/30][675/8000][60] Loss_D: -11335.567383 Loss_G: 21187.039062 Loss_D_real: 10385.963867 Loss_D_fake 21721.531250\n",
      "[0/30][680/8000][61] Loss_D: -10561.660156 Loss_G: 20110.798828 Loss_D_real: 10142.671875 Loss_D_fake 20704.332031\n",
      "[0/30][685/8000][62] Loss_D: -11463.389648 Loss_G: 21018.986328 Loss_D_real: 9763.438477 Loss_D_fake 21226.828125\n",
      "[0/30][690/8000][63] Loss_D: -10138.188477 Loss_G: 19101.466797 Loss_D_real: 9903.221680 Loss_D_fake 20041.410156\n",
      "[0/30][695/8000][64] Loss_D: -10818.940430 Loss_G: 20645.966797 Loss_D_real: 8157.125977 Loss_D_fake 18976.066406\n",
      "[0/30][700/8000][65] Loss_D: -10212.555664 Loss_G: 19337.500000 Loss_D_real: 8456.583008 Loss_D_fake 18669.138672\n",
      "[0/30][705/8000][66] Loss_D: -10265.523438 Loss_G: 18760.472656 Loss_D_real: 9658.691406 Loss_D_fake 19924.214844\n",
      "[0/30][710/8000][67] Loss_D: -9130.036133 Loss_G: 18077.660156 Loss_D_real: 7628.612305 Loss_D_fake 16758.648438\n",
      "[0/30][715/8000][68] Loss_D: -8541.458984 Loss_G: 16655.041016 Loss_D_real: 8812.304688 Loss_D_fake 17353.763672\n",
      "[0/30][720/8000][69] Loss_D: -8893.925781 Loss_G: 17284.785156 Loss_D_real: 7751.025879 Loss_D_fake 16644.951172\n",
      "[0/30][725/8000][70] Loss_D: -7064.555664 Loss_G: 14315.863281 Loss_D_real: 7271.020508 Loss_D_fake 14335.576172\n",
      "[0/30][730/8000][71] Loss_D: -7787.442871 Loss_G: 15399.792969 Loss_D_real: 7417.457520 Loss_D_fake 15204.900391\n",
      "[0/30][735/8000][72] Loss_D: -7668.419922 Loss_G: 14687.015625 Loss_D_real: 7647.371094 Loss_D_fake 15315.791016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/30][740/8000][73] Loss_D: -6768.514160 Loss_G: 13646.152344 Loss_D_real: 7517.694824 Loss_D_fake 14286.208984\n",
      "[0/30][745/8000][74] Loss_D: -7029.872559 Loss_G: 13904.264648 Loss_D_real: 7255.649902 Loss_D_fake 14285.522461\n",
      "[0/30][750/8000][75] Loss_D: -7034.600098 Loss_G: 14415.845703 Loss_D_real: 6315.691895 Loss_D_fake 13350.291992\n",
      "[0/30][755/8000][76] Loss_D: -6602.337402 Loss_G: 13444.418945 Loss_D_real: 6377.232910 Loss_D_fake 12979.570312\n",
      "[0/30][760/8000][77] Loss_D: -6503.788574 Loss_G: 12674.130859 Loss_D_real: 6724.982910 Loss_D_fake 13228.771484\n",
      "[0/30][765/8000][78] Loss_D: -6061.922852 Loss_G: 12199.586914 Loss_D_real: 6113.784180 Loss_D_fake 12175.707031\n",
      "[0/30][770/8000][79] Loss_D: -5697.094727 Loss_G: 11576.278320 Loss_D_real: 6249.809570 Loss_D_fake 11946.904297\n",
      "[0/30][775/8000][80] Loss_D: -5470.783203 Loss_G: 11347.610352 Loss_D_real: 5337.315430 Loss_D_fake 10808.098633\n",
      "[0/30][780/8000][81] Loss_D: -5309.761230 Loss_G: 10913.257812 Loss_D_real: 5593.914551 Loss_D_fake 10903.675781\n",
      "[0/30][785/8000][82] Loss_D: -4613.173340 Loss_G: 9764.830078 Loss_D_real: 5080.679199 Loss_D_fake 9693.852539\n",
      "[0/30][790/8000][83] Loss_D: -4298.407715 Loss_G: 9128.652344 Loss_D_real: 5007.032715 Loss_D_fake 9305.440430\n",
      "[0/30][795/8000][84] Loss_D: -4290.361816 Loss_G: 8922.963867 Loss_D_real: 5802.802246 Loss_D_fake 10093.164062\n",
      "[0/30][800/8000][85] Loss_D: -5141.026367 Loss_G: 10418.110352 Loss_D_real: 3939.630127 Loss_D_fake 9080.656250\n",
      "[0/30][805/8000][86] Loss_D: -3785.514160 Loss_G: 8241.624023 Loss_D_real: 4359.339844 Loss_D_fake 8144.854004\n",
      "[0/30][810/8000][87] Loss_D: -3618.373047 Loss_G: 7737.355469 Loss_D_real: 4733.009766 Loss_D_fake 8351.382812\n",
      "[0/30][815/8000][88] Loss_D: -3606.812988 Loss_G: 7850.179688 Loss_D_real: 3887.919922 Loss_D_fake 7494.732910\n",
      "[0/30][820/8000][89] Loss_D: -4005.133789 Loss_G: 8777.995117 Loss_D_real: 4596.309570 Loss_D_fake 8601.443359\n",
      "[0/30][825/8000][90] Loss_D: -4126.318359 Loss_G: 8769.251953 Loss_D_real: 4675.810547 Loss_D_fake 8802.128906\n",
      "[0/30][830/8000][91] Loss_D: -3684.900391 Loss_G: 7770.872559 Loss_D_real: 3838.060059 Loss_D_fake 7522.960449\n",
      "[0/30][835/8000][92] Loss_D: -3412.440918 Loss_G: 6917.191406 Loss_D_real: 3898.004883 Loss_D_fake 7310.445801\n",
      "[0/30][840/8000][93] Loss_D: -3255.986572 Loss_G: 6862.727539 Loss_D_real: 2604.447998 Loss_D_fake 5860.434570\n",
      "[0/30][845/8000][94] Loss_D: -3085.145996 Loss_G: 6350.086426 Loss_D_real: 3110.950684 Loss_D_fake 6196.096680\n",
      "[0/30][850/8000][95] Loss_D: -3154.067871 Loss_G: 6585.266113 Loss_D_real: 2831.174316 Loss_D_fake 5985.242188\n",
      "[0/30][855/8000][96] Loss_D: -2811.957275 Loss_G: 5786.666016 Loss_D_real: 3086.826416 Loss_D_fake 5898.783691\n",
      "[0/30][860/8000][97] Loss_D: -2944.982422 Loss_G: 5981.492188 Loss_D_real: 2888.575684 Loss_D_fake 5833.558105\n",
      "[0/30][865/8000][98] Loss_D: -3044.074219 Loss_G: 6315.135742 Loss_D_real: 2480.077637 Loss_D_fake 5524.151855\n",
      "[0/30][870/8000][99] Loss_D: -2616.692139 Loss_G: 5183.722656 Loss_D_real: 2545.087646 Loss_D_fake 5161.779785\n",
      "[0/30][875/8000][100] Loss_D: -2680.923340 Loss_G: 4955.097168 Loss_D_real: 2916.811035 Loss_D_fake 5597.734375\n",
      "[0/30][880/8000][101] Loss_D: -2828.108887 Loss_G: 5546.007324 Loss_D_real: 2277.465332 Loss_D_fake 5105.574219\n",
      "[0/30][885/8000][102] Loss_D: -2449.172363 Loss_G: 4568.518555 Loss_D_real: 1936.987671 Loss_D_fake 4386.160156\n",
      "[0/30][890/8000][103] Loss_D: -2332.184326 Loss_G: 4175.319824 Loss_D_real: 1481.170166 Loss_D_fake 3813.354492\n",
      "[0/30][895/8000][104] Loss_D: -2617.309570 Loss_G: 4651.026855 Loss_D_real: 1860.156616 Loss_D_fake 4477.466309\n",
      "[0/30][900/8000][105] Loss_D: -2353.295898 Loss_G: 3922.087158 Loss_D_real: 1566.743652 Loss_D_fake 3920.039551\n",
      "[0/30][905/8000][106] Loss_D: -2191.103516 Loss_G: 3791.292480 Loss_D_real: 1400.559448 Loss_D_fake 3591.662842\n",
      "[0/30][910/8000][107] Loss_D: -2438.000977 Loss_G: 4209.732422 Loss_D_real: 1629.713379 Loss_D_fake 4067.714355\n",
      "[0/30][915/8000][108] Loss_D: -2361.625977 Loss_G: 3772.650391 Loss_D_real: 1911.268921 Loss_D_fake 4272.895020\n",
      "[0/30][920/8000][109] Loss_D: -2170.965576 Loss_G: 3388.977051 Loss_D_real: 1053.244385 Loss_D_fake 3224.209961\n",
      "[0/30][925/8000][110] Loss_D: -2036.476074 Loss_G: 2827.474854 Loss_D_real: 1155.274170 Loss_D_fake 3191.750244\n",
      "[0/30][930/8000][111] Loss_D: -2095.630615 Loss_G: 3354.646484 Loss_D_real: 600.461121 Loss_D_fake 2696.091797\n",
      "[0/30][935/8000][112] Loss_D: -1921.909668 Loss_G: 3084.562012 Loss_D_real: 329.720245 Loss_D_fake 2251.629883\n",
      "[0/30][940/8000][113] Loss_D: -1909.780151 Loss_G: 2376.764648 Loss_D_real: 546.159790 Loss_D_fake 2455.939941\n",
      "[0/30][945/8000][114] Loss_D: -2157.395996 Loss_G: 2695.693115 Loss_D_real: 744.874817 Loss_D_fake 2902.270752\n",
      "[0/30][950/8000][115] Loss_D: -2038.189453 Loss_G: 2449.656006 Loss_D_real: 314.539856 Loss_D_fake 2352.729248\n",
      "[0/30][955/8000][116] Loss_D: -1831.317505 Loss_G: 1726.241455 Loss_D_real: 64.557198 Loss_D_fake 1895.874756\n",
      "[0/30][960/8000][117] Loss_D: -1906.465088 Loss_G: 1912.674072 Loss_D_real: 100.739487 Loss_D_fake 2007.204590\n",
      "[0/30][965/8000][118] Loss_D: -1927.119263 Loss_G: 1847.359619 Loss_D_real: 499.828217 Loss_D_fake 2426.947510\n",
      "[0/30][970/8000][119] Loss_D: -1886.815430 Loss_G: 1776.069946 Loss_D_real: 187.761505 Loss_D_fake 2074.576904\n",
      "[0/30][975/8000][120] Loss_D: -2002.942383 Loss_G: 1890.950439 Loss_D_real: 307.772217 Loss_D_fake 2310.714600\n",
      "[0/30][980/8000][121] Loss_D: -1835.427979 Loss_G: 1395.404053 Loss_D_real: -304.314911 Loss_D_fake 1531.113037\n",
      "[0/30][985/8000][122] Loss_D: -2078.669434 Loss_G: 2885.641113 Loss_D_real: 116.439919 Loss_D_fake 2195.109375\n",
      "[0/30][990/8000][123] Loss_D: -2004.759521 Loss_G: 1483.927246 Loss_D_real: -100.821243 Loss_D_fake 1903.938232\n",
      "[0/30][995/8000][124] Loss_D: -1844.114014 Loss_G: 2308.091797 Loss_D_real: -208.650940 Loss_D_fake 1635.463135\n",
      "[0/30][1000/8000][125] Loss_D: -1670.154785 Loss_G: 1806.251099 Loss_D_real: -621.585876 Loss_D_fake 1048.568848\n",
      "[0/30][1005/8000][126] Loss_D: -1751.844238 Loss_G: 1302.766968 Loss_D_real: -974.829285 Loss_D_fake 777.015015\n"
     ]
    }
   ],
   "source": [
    "# setup optimizer\n",
    "if lambda_ is None:\n",
    "    optimizerD = optim.RMSprop(netD.parameters(), lr = lrD)\n",
    "    optimizerG = optim.RMSprop(netG.parameters(), lr = lrG)\n",
    "else:\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=lrD, betas=(0.5, 0.9))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=lrG, betas=(0.5, 0.9))\n",
    "\n",
    "wass_loss=[]\n",
    "errG_l=[]\n",
    "errD_real_l=[]\n",
    "errD_fake_l=[]\n",
    "\n",
    "input = torch.FloatTensor(batchSize, 1, s_sample, s_sample, s_sample)\n",
    "noise = torch.FloatTensor(batchSize, nz,1 , 1, 1,  device=device).normal_(0,1)\n",
    "fixed_noise = torch.FloatTensor(batchSize, nz, 1, 1, 1 ).normal_(0, 1)\n",
    "#fixed_noise = torch.FloatTensor(1, nz, 1, 1,1).normal_(0, 1)\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "#one = torch.tensor(1, dtype=torch.float)\n",
    "mone = one * -1\n",
    "\n",
    "#torch.cuda.empty_cache()\n",
    "if cuda==True:\n",
    "    netD.cuda()\n",
    "    netG.cuda()\n",
    "    input = input.cuda()\n",
    "    one, mone = one.cuda(), mone.cuda()\n",
    "    noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "\n",
    "\n",
    "gen_iterations = 0\n",
    "for epoch in range(niter):\n",
    "    \n",
    "    data_iter = iter(dataloader)\n",
    "    i = 0\n",
    "    while i < len(dataloader):\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        for p in netD.parameters(): # reset requires_grad\n",
    "            p.requires_grad = True # they are set to False below in netG update\n",
    "           \n",
    "        if gen_iterations < 25 or gen_iterations % 500 == 0:\n",
    "            Diters = 20\n",
    "        else:\n",
    "            Diters = Diters_init\n",
    "            \n",
    "        j=0\n",
    "        while j < Diters and i < len(dataloader):\n",
    "            j += 1\n",
    "            \n",
    "            #########clamp parameters to a cube\n",
    "            if lambda_ is None:\n",
    "                for p in netD.parameters():\n",
    "                    p.data.clamp_(clamp_lower, clamp_upper)\n",
    "                \n",
    "            data=data_iter.next()\n",
    "            i+=1\n",
    "            \n",
    "            real_cpu= data\n",
    "            netD.zero_grad()\n",
    "          \n",
    "            batch_size=real_cpu.size(0)\n",
    "            if cuda==True:\n",
    "                real_cpu=real_cpu.cuda()\n",
    "        \n",
    "            input.resize_as_(real_cpu).copy_(real_cpu)\n",
    "            inputv = Variable(input)\n",
    "            #print(inputv.size())   \n",
    "            errD_real=netD(inputv)\n",
    "            \n",
    "            errD_real.backward(one)\n",
    "                \n",
    "            #Train with fake\n",
    "            noise.resize_(batchSize, nz, 1, 1, 1).normal_(0, 1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                noisev = Variable(noise) # totally freeze netG\n",
    "                \n",
    "            fake = Variable(netG(noisev).data)\n",
    "\n",
    "            inputv=fake\n",
    "               \n",
    "            errD_fake = netD(inputv)\n",
    "            errD_fake.backward(mone)\n",
    "            \n",
    "            ## train with gradient penalty\n",
    "            if lambda_ is not None:\n",
    "                gradient_penalty = _gradient_penalty(real_cpu.data, fake.data, lambda_)\n",
    "                gradient_penalty.backward()\n",
    "                errD = errD_real - errD_fake + gradient_penalty\n",
    "                wass_D =  errD_real - errD_fake\n",
    "                \n",
    "                optimizerD.step()\n",
    "                wass_loss.append(float(wass_D.data[0]))\n",
    "            else:\n",
    "                errD =  errD_real - errD_fake\n",
    "                optimizerD.step()\n",
    "                wass_loss.append(float(errD.data[0]))\n",
    "                \n",
    "            errD_real_l.append(float(errD_real.data[0]))\n",
    "            errD_fake_l.append(float(errD_fake.data[0]))\n",
    "           \n",
    "           \n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = False # to avoid computation\n",
    "        netG.zero_grad()\n",
    "        # in case our last batch was the tail batch of the dataloader,\n",
    "        # make sure we feed a full batch of noise\n",
    "        noise.resize_(batchSize, nz, 1, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev)\n",
    "        #print('G:',fake.shape)\n",
    "        errG = netD(fake)\n",
    "        \n",
    "        #print('errG: ', errG)\n",
    "        #label = torch.full(size = (batch_size,), fill_value = real_label, device = device)\n",
    "        errG.backward(one)\n",
    "        optimizerG.step()\n",
    "        \n",
    "        gen_iterations += 1\n",
    "        \n",
    "        if lambda_ is not None:\n",
    "            errD=wass_D\n",
    "        \n",
    "        print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f Loss_D_real: %f Loss_D_fake %f'\n",
    "            % (epoch, niter, i, len(dataloader), gen_iterations,\n",
    "            errD.data[0], errG.data[0], errD_real.data[0], errD_fake.data[0]))\n",
    "        \n",
    "        #wass_loss.append(float(errD.data[0]))\n",
    "        errG_l.append(float(errG.data[0]))\n",
    "\n",
    "        #if gen_iterations % 10==0  or (gen_iterations>5 and -float(errD.data[0])< min([-z for z in wass_loss])):\n",
    "        if gen_iterations % 200 == 0:\n",
    "            with torch.no_grad():\n",
    "                fake = netG(Variable(fixed_noise))\n",
    "                fake = np.array(fake)\n",
    "                real_cpu = np.array(real_cpu)\n",
    "\n",
    "            plot_power_spec((np.exp(fake * max_l5) -1), (np.exp(real_cpu * max_l5) -1), mean_5, s_sample)\n",
    "            #visualize2d((np.array(real_cpu)), (np.array(fake)), save='')\n",
    "            visualize2d(np.array(real_cpu), np.array(fake), log= False, save='')\n",
    "            hist_plot(np.array(fake), np.array(real_cpu), log_plot=False, redshift_fig_folder='') \n",
    "            plot_loss(wass_loss,'Wasserstein loss', log_=False)\n",
    "            \n",
    "            #plot_loss(wass_loss,'Wasserstein loss', log_=False)\n",
    "           \n",
    "    # do checkpointing\n",
    "    torch.save(netG.state_dict(), '{0}/netG_epoch_{1}.pth'.format(experiment, epoch))\n",
    "    torch.save(netD.state_dict(), '{0}/netD_epoch_{1}.pth'.format(experiment, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_loss(wass_loss,'Wasserstein loss', log_=False)\n",
    "plot_loss(errG_l,'Generator loss', log_=False)\n",
    "plot_loss(errD_real_l,'D real loss', log_=False)\n",
    "plot_loss(errD_fake_l,'D fake loss', log_=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Additional Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DCGAN_G_(nn.Module):\n",
    "    def __init__(self, ngpu,kernel,  dilation, bias):\n",
    "        super(DCGAN_G, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.kernel=kernel\n",
    "        self.bias=bias\n",
    "        self.dilation=dilation\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z,\n",
    "            nn.ConvTranspose3d(in_channels=nz,\n",
    "                               out_channels=ngf * 16,\n",
    "                               kernel_size =self.kernel,\n",
    "                               stride = 1,\n",
    "                               padding = 0,\n",
    "                               bias=False,\n",
    "                              dilation=self.dilation),\n",
    "            nn.BatchNorm3d(ngf * 16),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose3d(ngf * 16, ngf * 8, self.kernel, 2, 1, bias=self.bias, dilation=self.dilation),\n",
    "            nn.BatchNorm3d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose3d(ngf * 8, ngf * 4, self.kernel, 2, 1, bias=self.bias,dilation=self.dilation),\n",
    "            nn.BatchNorm3d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose3d(ngf * 4, ngf * 2, self.kernel, 2, 1, bias=self.bias,dilation=self.dilation),\n",
    "            nn.BatchNorm3d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose3d(ngf * 2,     ngf, self.kernel, 2, 1, bias=self.bias,dilation=self.dilation),\n",
    "            nn.BatchNorm3d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose3d(ngf, nc, self.kernel, 2, 1, bias=self.bias,dilation=self.dilation),\n",
    "            nn.Tanh(),\n",
    "            #nn.ReLU(True)\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        #print('G: ' ,output.size())\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/rimchang/3DGAN-Pytorch/blob/master/3D_GAN/model.py\n",
    "'''Detect when wass loss is making a big jump and keep iterating the \n",
    "critic till the curve goes up again to roughly were it was before the jump.'''\n",
    "\n",
    "class DCGAN_D_(nn.Module):\n",
    "    def __init__(self, ngpu, kernel, dilation, bias):\n",
    "        super(DCGAN_D, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.kernel=kernel\n",
    "        self.bias=bias\n",
    "        self.dilation=dilation\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            # nc = number of channels\n",
    "            \n",
    "            nn.Conv3d(in_channels=nc, \n",
    "                      out_channels=ndf, \n",
    "                      kernel_size=self.kernel, # == 4( 4,4,4)\n",
    "                      stride = 2, # == 2\n",
    "                     padding=1,\n",
    "                      dilation=self.dilation,\n",
    "                      # == 1\n",
    "                     # padding=0,\n",
    "                      bias=self.bias),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv3d(ndf, ndf * 2, self.kernel, 2, 1, bias=self.bias, dilation=self.dilation),\n",
    "            nn.BatchNorm3d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv3d(ndf * 2, ndf * 4, self.kernel, 2, 1, bias=self.bias, dilation=self.dilation),\n",
    "            nn.BatchNorm3d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv3d(ndf * 4, ndf * 8, self.kernel, 2, 1, bias=self.bias, dilation=self.dilation),\n",
    "            nn.BatchNorm3d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv3d(ndf * 8, ndf * 16, self.kernel, 2, 1, bias=self.bias, dilation=self.dilation),\n",
    "            nn.BatchNorm3d(ndf * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "           # nn.Conv3d(ndf * 16, ndf*32, 4, 2, 1, bias=False),\n",
    "            #nn.BatchNorm3d(ndf * 32),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            #Final conv (D does not have ti output a probability)\n",
    "            nn.Conv3d(ndf*16, 1, self.kernel, 1, 0, bias=self.bias, dilation=self.dilation)\n",
    "        )\n",
    "        #self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main,  input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "            \n",
    "        #print('D0:',output.size())\n",
    "        output = output.mean(0)\n",
    "        #return output.view(-1, 1).squeeze(1)\n",
    "        #print('D1:',output.size())\n",
    "        return output.view(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP_G(nn.Module):\n",
    "    def __init__(self, isize, nz, nc, ngf, ngpu):\n",
    "        super(MLP_G, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        main = nn.Sequential(\n",
    "            # Z goes into a linear of size: ngf\n",
    "            nn.Linear(nz, ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(ngf, ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(ngf, ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(ngf, ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(ngf, ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(ngf, ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(ngf,  nc * isize * isize *isize),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.main = main\n",
    "        self.nc = nc\n",
    "        self.isize = isize\n",
    "        self.nz = nz\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(input.size(0), input.size(1))\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output.view(output.size(0), self.nc, self.isize, self.isize, self.isize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP_D(nn.Module):\n",
    "    def __init__(self, isize, nz, nc, ndf, ngpu):\n",
    "        super(MLP_D, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        main = nn.Sequential(\n",
    "            # Z goes into a linear of size: ndf\n",
    "            nn.Linear(nc * isize * isize *isize, ndf),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(ndf, ndf),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(ndf, ndf),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(ndf, ndf),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(ndf, ndf),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(ndf, ndf),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(ndf, 1),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            #nn.Linear(ndf, 1)\n",
    "        )\n",
    "        self.main = main\n",
    "        self.nc = nc\n",
    "        self.isize = isize\n",
    "        self.nz = nz\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(input.size(0),\n",
    "                           input.size(1) * input.size(2) * input.size(3) *input.size(4))\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        output = output.mean(0)\n",
    "        return output.view(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Input minibatches\n",
    "def plot_mass_hist(fake, real, s_sample, together):\n",
    "    num_bins=100\n",
    "    al=0.9\n",
    "    dens=False\n",
    "    xlim=.02\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "               \n",
    "    k=0\n",
    "    for n in  range(fake.shape[0]):\n",
    "        k+=1\n",
    "        label='Generated' if k==1 else None\n",
    "        #plt.hist(np.where(fake[n].reshape(s_sample, s_sample, s_sample).flatten() >0),\n",
    "        plt.hist(np.where(fake[n].reshape(s_sample, s_sample, s_sample).flatten() > 0),\n",
    "                alpha=al, bins=num_bins, normed=dens, color='r', label=label)\n",
    "        \n",
    "    \n",
    "    if together==False:\n",
    "        plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "        plt.legend(loc='best',fontsize=18)\n",
    "        plt.xlim(0,xlim)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(20,10))\n",
    "    \n",
    "    k=0   \n",
    "    for m in range(real.shape[0]):\n",
    "        k+=1\n",
    "        label='Real' if k==1 else None\n",
    "        plt.hist(np.where(real[m].reshape(s_sample, s_sample, s_sample).flatten() >0),alpha=al,bins=num_bins, \n",
    "                 normed=dens ,color='g', label=label)\n",
    "    \n",
    "    plt.legend(loc='best',fontsize=18)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "    plt.title('PDFs of Hydrogen mass', fontsize=18)\n",
    "        #plt.savefig('speed_hist.jpg',format='jpg', dpi=100)\n",
    "    #plt.xlim(0,xlim)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Atakans architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from conv_utils import calculate_conv_output_dim, calculate_pool_output_dim\n",
    "\n",
    "# input: batch_size * nc * 64 * 64\n",
    "# output: batch_size * k * 1 * 1\n",
    "class N_Disc(nn.Module):\n",
    "    def __init__(self, cube_dimension, fc1_hidden_dim, fc2_output_dim, \n",
    "                embedding_dim, leakyrelu_const, pool_return_indices):        \n",
    "        super(N_Disc, self).__init__()\n",
    "\n",
    "        self.pool_return_indices = pool_return_indices\n",
    "      \n",
    "        # First Convolutional Layer\n",
    "        self.conv1_in_channels = 1\n",
    "        self.conv1_out_channels = 2\n",
    "        self.conv1_kernel = 3\n",
    "        self.conv1_stride = 1\n",
    "        self.conv1_padding = 0\n",
    "        conv1_output_dim = calculate_conv_output_dim(D=cube_dimension,\n",
    "                                        K=self.conv1_kernel,\n",
    "                                        P=self.conv1_padding,\n",
    "                                        S=self.conv1_stride)\n",
    "        print(\"Conv1 Output Dimension = \" + str(conv1_output_dim))\n",
    "        self.conv1_encode = nn.Conv3d(in_channels=self.conv1_in_channels, \n",
    "                                    out_channels=self.conv1_out_channels, \n",
    "                                    kernel_size=self.conv1_kernel, \n",
    "                                    stride =self.conv1_stride, \n",
    "                                    padding=self.conv1_padding)     \n",
    "        nn.init.xavier_uniform_(self.conv1_encode.weight)\n",
    "        self.bn1_encode = nn.BatchNorm3d(num_features = self.conv1_out_channels)\n",
    "        self.leakyrelu1 = nn.LeakyReLU(leakyrelu_const, inplace=True)\n",
    "\n",
    "        # First Average Pooling\n",
    "        self.pool1_kernel = 2\n",
    "        self.pool1_stride = 2 \n",
    "        pool1_output_dim = calculate_pool_output_dim(D=conv1_output_dim,\n",
    "                                                    K=self.pool1_kernel,\n",
    "                                                    S=self.pool1_stride)\n",
    "        print(\"Pool1 Output Dimension = \" + str(pool1_output_dim)) \n",
    "        self.pool1_encode = nn.MaxPool3d(kernel_size=self.pool1_kernel, \n",
    "                                            stride=self.pool1_stride,\n",
    "                                            return_indices = self.pool_return_indices)\n",
    "\n",
    "        # Second Convolutional Layer\n",
    "        self.conv2_in_channels = self.conv1_out_channels\n",
    "        self.conv2_out_channels = 12\n",
    "        self.conv2_kernel = 4\n",
    "        self.conv2_stride = 1\n",
    "        self.conv2_padding = 0\n",
    "        conv2_output_dim = calculate_conv_output_dim(D=pool1_output_dim,\n",
    "                                        K=self.conv2_kernel,\n",
    "                                        P=self.conv2_padding,\n",
    "                                        S=self.conv2_stride)\n",
    "        print(\"Conv2 Output Dimension= \" + str(conv2_output_dim))\n",
    "        self.conv2_encode = nn.Conv3d(in_channels=self.conv2_in_channels, \n",
    "                                    out_channels=self.conv2_out_channels, \n",
    "                                    kernel_size=self.conv2_kernel, \n",
    "                                    stride =self.conv2_stride, \n",
    "                                    padding=self.conv2_padding)     \n",
    "        nn.init.xavier_uniform_(self.conv2_encode.weight)  \n",
    "        self.bn2_encode = nn.BatchNorm3d(num_features = self.conv2_out_channels)\n",
    "        self.leakyrelu2 = nn.LeakyReLU(leakyrelu_const, inplace=True)\n",
    "\n",
    "        # Second Average Pooling\n",
    "        self.pool2_kernel = 2\n",
    "        self.pool2_stride = 2 \n",
    "        pool2_output_dim = calculate_pool_output_dim(D=conv2_output_dim,\n",
    "                                                K=self.pool2_kernel,\n",
    "                                                S=self.pool2_stride)\n",
    "        print(\"Pool2 Output Dimension = \" + str(pool2_output_dim)) \n",
    "        self.pool2_encode = nn.MaxPool3d(kernel_size=self.pool2_kernel, \n",
    "                                            stride=self.pool2_stride,\n",
    "                                            return_indices = self.pool_return_indices)   \n",
    "\n",
    "        # Third Convolutional Layer\n",
    "        self.conv3_in_channels = self.conv2_out_channels\n",
    "        self.conv3_out_channels = 24\n",
    "        self.conv3_kernel = 3\n",
    "        self.conv3_stride = 1\n",
    "        self.conv3_padding = 0\n",
    "        conv3_output_dim = calculate_conv_output_dim(D=pool2_output_dim,\n",
    "                                        K=self.conv3_kernel,\n",
    "                                        P=self.conv3_padding,\n",
    "                                        S=self.conv3_stride)\n",
    "        print(\"Conv3 Output Dimension= \" + str(conv3_output_dim))\n",
    "        self.conv3_encode = nn.Conv3d(in_channels=self.conv3_in_channels, \n",
    "                                    out_channels=self.conv3_out_channels, \n",
    "                                    kernel_size=self.conv3_kernel, \n",
    "                                    stride =self.conv3_stride, \n",
    "                                    padding=self.conv3_padding)     \n",
    "        nn.init.xavier_uniform_(self.conv3_encode.weight)\n",
    "        self.bn3_encode = nn.BatchNorm3d(num_features = self.conv3_out_channels)\n",
    "        self.leakyrelu3 = nn.LeakyReLU(leakyrelu_const, inplace=True)  \n",
    "\n",
    "        # Third Average Pooling\n",
    "        self.pool3_kernel = 2\n",
    "        self.pool3_stride = 2 \n",
    "        pool3_output_dim = calculate_pool_output_dim(D=conv3_output_dim,\n",
    "                                                K=self.pool3_kernel,\n",
    "                                                S=self.pool3_stride)\n",
    "        print(\"Pool3 Output Dimension = \" + str(pool3_output_dim)) \n",
    "        self.pool3_encode = nn.MaxPool3d(kernel_size=self.pool3_kernel, \n",
    "                                            stride=self.pool3_stride,\n",
    "                                            return_indices = self.pool_return_indices)  \n",
    "\n",
    "        # Fourth Convolutional Layer\n",
    "        self.conv4_in_channels = self.conv3_out_channels\n",
    "        self.conv4_out_channels = 32\n",
    "        self.conv4_kernel = 4\n",
    "        self.conv4_stride = 2\n",
    "        self.conv4_padding = 0\n",
    "        conv4_output_dim = calculate_conv_output_dim(D=pool3_output_dim,\n",
    "                                        K=self.conv4_kernel,\n",
    "                                        P=self.conv4_padding,\n",
    "                                        S=self.conv4_stride)\n",
    "        print(\"Conv4 Output Dimension= \" + str(conv4_output_dim))\n",
    "        self.conv4_encode = nn.Conv3d(in_channels=self.conv4_in_channels, \n",
    "                                    out_channels=self.conv4_out_channels, \n",
    "                                    kernel_size=self.conv4_kernel, \n",
    "                                    stride =self.conv4_stride, \n",
    "                                    padding=self.conv4_padding)     \n",
    "        nn.init.xavier_uniform_(self.conv4_encode.weight) \n",
    "        self.bn4_encode = nn.BatchNorm3d(num_features = self.conv4_out_channels)\n",
    "        self.leakyrelu4 = nn.LeakyReLU(leakyrelu_const, inplace=True)       \n",
    "\n",
    "        # Fifth Convolutional Layer\n",
    "        self.conv5_in_channels = self.conv4_out_channels\n",
    "        self.conv5_out_channels = 64\n",
    "        self.conv5_kernel = 3\n",
    "        self.conv5_stride = 1\n",
    "        self.conv5_padding = 0\n",
    "        conv5_output_dim = calculate_conv_output_dim(D=conv4_output_dim,\n",
    "                                        K=self.conv5_kernel,\n",
    "                                        P=self.conv5_padding,\n",
    "                                        S=self.conv5_stride)\n",
    "        print(\"Conv5 Output Dimension= \" + str(conv5_output_dim))\n",
    "        self.conv5_encode = nn.Conv3d(in_channels=self.conv5_in_channels, \n",
    "                                    out_channels=self.conv5_out_channels, \n",
    "                                    kernel_size=self.conv5_kernel, \n",
    "                                    stride =self.conv5_stride, \n",
    "                                    padding=self.conv5_padding)     \n",
    "        nn.init.xavier_uniform_(self.conv5_encode.weight) \n",
    "        self.bn5_encode = nn.BatchNorm3d(num_features = self.conv5_out_channels)\n",
    "        self.leakyrelu5 = nn.LeakyReLU(leakyrelu_const, inplace=True)\n",
    "\n",
    "        # Sixth Convolutional Layer\n",
    "        self.conv6_in_channels = self.conv5_out_channels\n",
    "        self.conv6_out_channels = 128\n",
    "        self.conv6_kernel = 2\n",
    "        self.conv6_stride = 1\n",
    "        self.conv6_padding = 0\n",
    "        conv6_output_dim = calculate_conv_output_dim(D=conv5_output_dim,\n",
    "                                        K=self.conv6_kernel,\n",
    "                                        P=self.conv6_padding,\n",
    "                                        S=self.conv6_stride)\n",
    "        print(\"Conv6 Output Dimension= \" + str(conv6_output_dim))\n",
    "        self.conv6_encode = nn.Conv3d(in_channels=self.conv6_in_channels, \n",
    "                                    out_channels=self.conv6_out_channels, \n",
    "                                    kernel_size=self.conv6_kernel, \n",
    "                                    stride =self.conv6_stride, \n",
    "                                    padding=self.conv6_padding)     \n",
    "        nn.init.xavier_uniform_(self.conv6_encode.weight) \n",
    "        self.bn6_encode = nn.BatchNorm3d(num_features = self.conv6_out_channels)\n",
    "        self.leakyrelu6 = nn.LeakyReLU(leakyrelu_const, inplace=True)\n",
    "\n",
    "        # 7th Convolutional Layer\n",
    "        self.conv7_in_channels = self.conv6_out_channels\n",
    "        self.conv7_out_channels = 1\n",
    "        self.conv7_kernel = 2\n",
    "        self.conv7_stride = 2\n",
    "        self.conv7_padding = 0\n",
    "        conv7_output_dim = calculate_conv_output_dim(D=conv6_output_dim,\n",
    "                                        K=self.conv7_kernel,\n",
    "                                        P=self.conv7_padding,\n",
    "                                        S=self.conv7_stride)\n",
    "        print(\"Conv7 Output Dimension= \" + str(conv7_output_dim))\n",
    "        \n",
    "        self.conv7_encode = nn.Conv3d(in_channels=self.conv7_in_channels, \n",
    "                                    out_channels=self.conv7_out_channels, \n",
    "                                    kernel_size=self.conv7_kernel, \n",
    "                                    stride =self.conv7_stride, \n",
    "                                    padding=self.conv7_padding)     \n",
    "        \n",
    "        nn.init.xavier_uniform_(self.conv7_encode.weight) \n",
    "        #self.bn7_encode = nn.BatchNorm3d(num_features = self.conv7_out_channels)\n",
    "        \n",
    "        #self.leakyrelu7 = nn.LeakyReLU(leakyrelu_const, inplace=True)\n",
    "     \n",
    "    #         # 1st FC Layer\n",
    "#         self.fc1_in_features = self.conv7_out_channels * conv7_output_dim**3\n",
    "#         self.fc1_encode = nn.Linear(in_features=self.fc1_in_features,\n",
    "#                                     out_features=fc1_hidden_dim)\n",
    "#         self.leakyrelu8 = nn.LeakyReLU(leakyrelu_const, inplace=True)\n",
    "\n",
    "#         # 2nd FC Layer\n",
    "#         self.fc2_encode = nn.Linear(in_features=self.fc1_hidden_dim,\n",
    "#                                     out_features=embedding_dim)\n",
    "#         self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        # Convolution Layers\n",
    "#         print(\"Input = \" +str(input.shape))\n",
    "        out = self.conv1_encode(input)\n",
    "#         print(\"conv1_encode = \" + str(out.shape))\n",
    "        out = self.pool1_encode(out)\n",
    "#         print(\"pool1_encode = \" + str(out.shape))\n",
    "        out = self.bn1_encode(out) \n",
    "        out = self.leakyrelu1(out)\n",
    "\n",
    "        out = self.conv2_encode(out)\n",
    "#         print(\"conv2_encode = \" + str(out.shape))\n",
    "        out = self.pool2_encode(out)\n",
    "#         print(\"pool2_encode = \" + str(out.shape))\n",
    "        out = self.bn2_encode(out) \n",
    "        out = self.leakyrelu2(out)\n",
    "\n",
    "        out = self.conv3_encode(out)\n",
    "#         print(\"conv3_encode = \" + str(out.shape))\n",
    "        out = self.pool3_encode(out)\n",
    "#         print(\"pool3_encode = \" + str(out.shape))\n",
    "        out = self.bn3_encode(out) \n",
    "        out = self.leakyrelu3(out)\n",
    "\n",
    "        out = self.conv4_encode(out)\n",
    "#         print(\"conv4_encode = \" + str(out.shape))\n",
    "        out = self.bn4_encode(out) \n",
    "        out = self.leakyrelu4(out)\n",
    "\n",
    "        out = self.conv5_encode(out)\n",
    "#         print(\"conv5_encode = \" + str(out.shape))\n",
    "        out = self.bn5_encode(out) \n",
    "        out = self.leakyrelu5(out)\n",
    "\n",
    "        out = self.conv6_encode(out)\n",
    "#         print(\"conv6_encode = \" + str(out.shape))\n",
    "        out = self.bn6_encode(out) \n",
    "        out = self.leakyrelu6(out)\n",
    "        \n",
    "        out = self.conv7_encode(out)\n",
    "#         print(\"conv7_encode = \" + str(out.shape))\n",
    "        #out = self.bn7_encode(out) \n",
    "        #out = self.leakyrelu7(out)\n",
    "        \n",
    "#         print(\"out = \" + str(out.shape))\n",
    "\n",
    "#         # Transform\n",
    "        print('D last', out.shape)\n",
    "        \n",
    "\n",
    "        #out = out.view(batchSize, -1)\n",
    "\n",
    "#         # FC Layers\n",
    "#         out = self.fc1_encode(out)\n",
    "#         out = self.leakyrelu8(out)\n",
    "\n",
    "#         out = self.fc2_encode(out)\n",
    "#         out = self.relu1(out)        \n",
    "        out= out.mean(0)\n",
    "        return out.view(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_densities(fake, real, log_):\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    ax.plot(xlabel='big', ylabel='big', title='Density')\n",
    "    sns.set(rc={'figure.figsize':(20,10),\"lines.linewidth\": 3})\n",
    "    #plt.subplots(figsize=(20,10))\n",
    "    #ax.set_ylim(0,0.6)\n",
    "    #ax.set_xlim(0, )\n",
    "    bw=0.2\n",
    "    eps=0.0000000000000001\n",
    "    eps=0\n",
    "    grid=200\n",
    "    n=0\n",
    "    if log_==True:\n",
    "        fake=np.log(fake+eps)\n",
    "        real=np.log(real+eps)\n",
    "    for m in range(fake.shape[0]):\n",
    "        n+=1\n",
    "        if n==1:\n",
    "            sns.kdeplot(fake[m][0].flatten(), ax=ax, bw=bw, cut=0, label='Generated', color='red', gridsize=grid);\n",
    "            sns.kdeplot(real[m][0].flatten(), ax=ax, bw=bw, cut=0, label='Real' ,color='blue',gridsize=grid);\n",
    "        else:\n",
    "            sns.kdeplot(fake[m][0].flatten(), ax=ax, bw=bw, cut=0, color='red', gridsize=grid);\n",
    "            sns.kdeplot(real[m][0].flatten(), ax=ax, bw=bw, cut=0,color='blue', gridsize=grid);\n",
    "        \n",
    "        ax.set_title(\"Densities of log HI mass\",fontsize=25)\n",
    "        ax.tick_params(labelsize=20)\n",
    "        plt.setp(ax.get_legend().get_texts(), fontsize='22')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from conv_utils import calculate_deconv_output_dim\n",
    "\n",
    "# input: batch_size * k * 1 * 1\n",
    "# output: batch_size * nc * image_size * image_size\n",
    "class N_Gen(nn.Module):\n",
    "    def __init__(self, embedded_cube_dimension,fc1_hidden_dim, fc2_output_dim, \n",
    "                embedding_dim, leakyrelu_const):\n",
    "        super(N_Gen, self).__init__()\n",
    "\n",
    "#         # 1st FC Layer\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.fc1_in_features = self.embedding_dim \n",
    "#         self.fc1_hidden_dim = fc1_hidden_dim\n",
    "#         self.fc1_decode = nn.Linear(in_features=self.fc1_in_features,\n",
    "#                                     out_features=self.fc1_hidden_dim)\n",
    "#         self.leakyrelu1 = nn.LeakyReLU(leakyrelu_const, inplace=True)\n",
    "\n",
    "#         # 2nd FC Layer\n",
    "#         self.fc2_output_dim = fc2_output_dim\n",
    "#         self.fc2_decode = nn.Linear(in_features=self.fc1_hidden_dim,\n",
    "#                                     out_features=self.fc2_output_dim )\n",
    "#         self.leakyrelu2 = nn.LeakyReLU(leakyrelu_const, inplace=True)\n",
    "\n",
    "        # 1st Deconvolutional Layer\n",
    "        self.deconv1_in_channels = batch_size\n",
    "        self.deconv1_out_channels = 128\n",
    "        self.deconv1_kernel = 2\n",
    "        self.deconv1_stride = 1\n",
    "        self.deconv1_padding = 0\n",
    "        deconv1_output_dim = calculate_deconv_output_dim(D=embedded_cube_dimension,\n",
    "                                        K=self.deconv1_kernel,\n",
    "                                        P=self.deconv1_padding,\n",
    "                                        S=self.deconv1_stride)\n",
    "        print(\"Deconv1 Output Dimension = \" + str(deconv1_output_dim))\n",
    "        self.deconv1_decode = nn.ConvTranspose3d(in_channels=self.deconv1_in_channels, \n",
    "                                    out_channels=self.deconv1_out_channels, \n",
    "                                    kernel_size=self.deconv1_kernel, \n",
    "                                    stride =self.deconv1_stride, \n",
    "                                    padding=self.deconv1_padding)     \n",
    "        nn.init.xavier_uniform_(self.deconv1_decode.weight)\n",
    "        self.bn1_decode = nn.BatchNorm3d(num_features = self.deconv1_out_channels)\n",
    "        self.leakyrelu1 = nn.LeakyReLU(leakyrelu_const, inplace=True)\n",
    "\n",
    "        # 2nd Deconvolutional Layer\n",
    "        self.deconv2_in_channels = self.deconv1_out_channels\n",
    "        self.deconv2_out_channels = 64\n",
    "        self.deconv2_kernel = 2\n",
    "        self.deconv2_stride = 1\n",
    "        self.deconv2_padding = 0\n",
    "        deconv2_output_dim = calculate_deconv_output_dim(D=deconv1_output_dim,\n",
    "                                        K=self.deconv2_kernel,\n",
    "                                        P=self.deconv2_padding,\n",
    "                                        S=self.deconv2_stride)\n",
    "        print(\"Deconv2 Output Dimension = \" + str(deconv2_output_dim))\n",
    "        self.deconv2_decode = nn.ConvTranspose3d(in_channels=self.deconv2_in_channels, \n",
    "                                    out_channels=self.deconv2_out_channels, \n",
    "                                    kernel_size=self.deconv2_kernel, \n",
    "                                    stride =self.deconv2_stride, \n",
    "                                    padding=self.deconv2_padding)     \n",
    "        nn.init.xavier_uniform_(self.deconv2_decode.weight)\n",
    "        self.bn2_decode = nn.BatchNorm3d(num_features = self.deconv2_out_channels)\n",
    "        self.leakyrelu2 = nn.LeakyReLU(leakyrelu_const, inplace=True)\n",
    "        \n",
    "        # 3rd Deconvolutional Layer\n",
    "        self.deconv3_in_channels = self.deconv2_out_channels\n",
    "        self.deconv3_out_channels = 32\n",
    "        self.deconv3_kernel = 3\n",
    "        self.deconv3_stride = 1\n",
    "        self.deconv3_padding = 0\n",
    "        deconv3_output_dim = calculate_deconv_output_dim(D=deconv2_output_dim,\n",
    "                                        K=self.deconv3_kernel,\n",
    "                                        P=self.deconv3_padding,\n",
    "                                        S=self.deconv3_stride)\n",
    "        print(\"Deconv3 Output Dimension = \" + str(deconv3_output_dim))\n",
    "        self.deconv3_decode = nn.ConvTranspose3d(in_channels=self.deconv3_in_channels, \n",
    "                                    out_channels=self.deconv3_out_channels, \n",
    "                                    kernel_size=self.deconv3_kernel, \n",
    "                                    stride =self.deconv3_stride, \n",
    "                                    padding=self.deconv3_padding)     \n",
    "        nn.init.xavier_uniform_(self.deconv3_decode.weight)\n",
    "        self.bn3_decode = nn.BatchNorm3d(num_features = self.deconv3_out_channels)\n",
    "        self.leakyrelu3 = nn.LeakyReLU(leakyrelu_const, inplace=True)\n",
    "        \n",
    "        # 4th Deconvolutional Layer\n",
    "        self.deconv4_in_channels = self.deconv3_out_channels\n",
    "        self.deconv4_out_channels = 24\n",
    "        self.deconv4_kernel = 4\n",
    "        self.deconv4_stride = 2\n",
    "        self.deconv4_padding = 0\n",
    "        deconv4_output_dim = calculate_deconv_output_dim(D=deconv3_output_dim,\n",
    "                                        K=self.deconv4_kernel,\n",
    "                                        P=self.deconv4_padding,\n",
    "                                        S=self.deconv4_stride)\n",
    "        print(\"Deconv4 Output Dimension = \" + str(deconv4_output_dim))\n",
    "        self.deconv4_decode = nn.ConvTranspose3d(in_channels=self.deconv4_in_channels, \n",
    "                                    out_channels=self.deconv4_out_channels, \n",
    "                                    kernel_size=self.deconv4_kernel, \n",
    "                                    stride =self.deconv4_stride, \n",
    "                                    padding=self.deconv4_padding)     \n",
    "        nn.init.xavier_uniform_(self.deconv4_decode.weight)\n",
    "        self.bn4_decode = nn.BatchNorm3d(num_features = self.deconv4_out_channels)\n",
    "        self.leakyrelu4 = nn.LeakyReLU(leakyrelu_const, inplace=True)\n",
    "        \n",
    "        # Avg Unpooling 1\n",
    "        # Just make 1 voxel to 8 voxels of 2-len edges\n",
    "        # Implemented in forward pass\n",
    "        self.avgunpool1_scale = 2\n",
    "        self.avgunpool1 = nn.Upsample(scale_factor = self.avgunpool1_scale, mode='nearest')\n",
    "        \n",
    "        \n",
    "        # 5th Deconvolutional Layer\n",
    "        self.deconv5_in_channels = self.deconv4_out_channels\n",
    "        self.deconv5_out_channels = 12\n",
    "        self.deconv5_kernel = 3\n",
    "        self.deconv5_stride = 1\n",
    "        self.deconv5_padding = 0\n",
    "        deconv5_output_dim = calculate_deconv_output_dim(D=deconv4_output_dim * self.avgunpool1_scale,\n",
    "                                        K=self.deconv5_kernel,\n",
    "                                        P=self.deconv5_padding,\n",
    "                                        S=self.deconv5_stride)\n",
    "        print(\"Deconv5 Output Dimension = \" + str(deconv5_output_dim))\n",
    "        self.deconv5_decode = nn.ConvTranspose3d(in_channels=self.deconv5_in_channels, \n",
    "                                    out_channels=self.deconv5_out_channels, \n",
    "                                    kernel_size=self.deconv5_kernel, \n",
    "                                    stride =self.deconv5_stride, \n",
    "                                    padding=self.deconv5_padding)     \n",
    "        nn.init.xavier_uniform_(self.deconv5_decode.weight)\n",
    "        self.bn5_decode = nn.BatchNorm3d(num_features = self.deconv5_out_channels)\n",
    "        self.leakyrelu5 = nn.LeakyReLU(leakyrelu_const, inplace=True)\n",
    "        \n",
    "        # Avg Unpooling 2\n",
    "        # Just make 1 voxel to 8 voxels of 2-len edges\n",
    "        # Implemented in forward pass\n",
    "        self.avgunpool2_scale = 2\n",
    "        self.avgunpool2 = nn.Upsample(scale_factor = self.avgunpool2_scale, mode='nearest')\n",
    "        \n",
    "        # 6th Deconvolutional Layer\n",
    "        self.deconv6_in_channels = self.deconv5_out_channels\n",
    "        self.deconv6_out_channels = 2\n",
    "        self.deconv6_kernel = 4\n",
    "        self.deconv6_stride = 1\n",
    "        self.deconv6_padding = 0\n",
    "        deconv6_output_dim = calculate_deconv_output_dim(D=deconv5_output_dim * self.avgunpool2_scale,\n",
    "                                        K=self.deconv6_kernel,\n",
    "                                        P=self.deconv6_padding,\n",
    "                                        S=self.deconv6_stride)\n",
    "        print(\"Deconv6 Output Dimension = \" + str(deconv6_output_dim))\n",
    "        self.deconv6_decode = nn.ConvTranspose3d(in_channels=self.deconv6_in_channels, \n",
    "                                    out_channels=self.deconv6_out_channels, \n",
    "                                    kernel_size=self.deconv6_kernel, \n",
    "                                    stride =self.deconv6_stride, \n",
    "                                    padding=self.deconv6_padding)     \n",
    "        nn.init.xavier_uniform_(self.deconv6_decode.weight)\n",
    "        self.bn6_decode = nn.BatchNorm3d(num_features = self.deconv6_out_channels)\n",
    "        self.leakyrelu6 = nn.LeakyReLU(leakyrelu_const, inplace=True)\n",
    "        \n",
    "        # Avg Unpooling 3\n",
    "        # Just make 1 voxel to 8 voxels of 2-len edges\n",
    "        # Implemented in forward pass\n",
    "        self.avgunpool3_scale = 2\n",
    "        self.avgunpool3 = nn.Upsample(scale_factor = self.avgunpool3_scale, mode='nearest')\n",
    "        \n",
    "        # 7th Deconvolutional Layer\n",
    "        self.deconv7_in_channels = self.deconv6_out_channels\n",
    "        self.deconv7_out_channels = 1\n",
    "        self.deconv7_kernel = 3\n",
    "        self.deconv7_stride = 1\n",
    "        self.deconv7_padding = 0\n",
    "        deconv7_output_dim = calculate_deconv_output_dim(D=deconv6_output_dim * self.avgunpool3_scale,\n",
    "                                        K=self.deconv7_kernel,\n",
    "                                        P=self.deconv7_padding,\n",
    "                                        S=self.deconv7_stride)\n",
    "        print(\"Deconv7 Output Dimension = \" + str(deconv7_output_dim))\n",
    "        self.deconv7_decode = nn.ConvTranspose3d(in_channels=self.deconv7_in_channels, \n",
    "                                    out_channels=self.deconv7_out_channels, \n",
    "                                    kernel_size=self.deconv7_kernel, \n",
    "                                    stride =self.deconv7_stride, \n",
    "                                    padding=self.deconv7_padding)     \n",
    "        nn.init.xavier_uniform_(self.deconv7_decode.weight)\n",
    "        self.bn7_decode = nn.BatchNorm3d(num_features = self.deconv7_out_channels)\n",
    "#         self.leakyrelu7 = nn.LeakyReLU(leakyrelu_const, inplace=True)\n",
    "        \n",
    "        # For data in [0,1]\n",
    "#         self.relu7 = nn.ReLU(inplace=True)     \n",
    "        # For data in [-1,1]\n",
    "        self.tanh7 = nn.Tanh()     \n",
    "\n",
    "    def forward(self, input):\n",
    "#         print(\"\\nDecoder - Forward Pass\")\n",
    "        \n",
    "        # Deconvolution Layers\n",
    "#         print(\"Input = \" +str(input.shape))\n",
    "        out = self.deconv1_decode(input)\n",
    "#         print(\"deconv1_decode = \" + str(out.shape))\n",
    "        out = self.bn1_decode(out)\n",
    "        out = self.leakyrelu1(out)\n",
    "\n",
    "        out = self.deconv2_decode(out)\n",
    "#         print(\"deconv2_decode = \" + str(out.shape))\n",
    "        out = self.bn2_decode(out)\n",
    "        out = self.leakyrelu2(out)\n",
    "        \n",
    "        out = self.deconv3_decode(out)\n",
    "#         print(\"deconv3_decode = \" + str(out.shape))\n",
    "        out = self.bn3_decode(out)\n",
    "        out = self.leakyrelu3(out)\n",
    "        \n",
    "        out = self.deconv4_decode(out)\n",
    "#         print(\"deconv4_decode = \" + str(out.shape))\n",
    "        out = self.bn4_decode(out)\n",
    "        out = self.leakyrelu4(out)\n",
    "        out = self.avgunpool1(out)\n",
    "#         print(\"avgunpool1 = \" + str(out.shape))\n",
    "\n",
    "        out = self.deconv5_decode(out)\n",
    "#         print(\"deconv5_decode = \" + str(out.shape))\n",
    "        out = self.bn5_decode(out)\n",
    "        out = self.leakyrelu5(out)\n",
    "        out = self.avgunpool2(out)\n",
    "#         print(\"avgunpool2 = \" + str(out.shape))\n",
    "        out = self.deconv6_decode(out)\n",
    "#         print(\"deconv6_decode = \" + str(out.shape))\n",
    "        out = self.bn6_decode(out)\n",
    "        out = self.leakyrelu6(out)\n",
    "        out = self.avgunpool3(out) \n",
    "#         print(\"avgunpool3 = \" + str(out.shape))\n",
    "        \n",
    "        out = self.deconv7_decode(out)\n",
    "#         print(\"deconv7_decode = \" + str(out.shape))\n",
    "        out = self.bn7_decode(out)\n",
    "#         out = self.leakyrelu7(out)\n",
    "#         out = self.relu7(out) # for [0,1]\n",
    "        out = self.tanh7(out) # for [-1,1]\n",
    "        \n",
    "#         print(\"decoder out = \" + str(out.shape))\n",
    "#         # Transformation\n",
    "#         out = out.view(batch_size, nz, 1,1,1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DCGAN_D__(nn.Module):\n",
    "    def __init__(self, ngpu, cube_len, leak_value, bias):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.cube_len=cube_len\n",
    "        self.leak_value=leak_value\n",
    "        self.bias=bias\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            #1\n",
    "            nn.Conv3d(in_channels=1, \n",
    "                      out_channels=self.cube_len, \n",
    "                      kernel_size=4, # == 4( 4,4,4)\n",
    "                      stride = 2, # == 2\n",
    "                     padding=1,\n",
    "                     # padding=0,\n",
    "                      bias=self.bias),\n",
    "            nn.BatchNorm3d(self.cube_len),\n",
    "            nn.LeakyReLU(self.leak_value, inplace=True),\n",
    "        \n",
    "            #2\n",
    "            nn.Conv3d(self.cube_len, self.cube_len*2, kernel_size=4, stride=2, bias=self.bias, padding=(1,1,1)),\n",
    "            nn.BatchNorm3d(self.cube_len*2),\n",
    "            nn.LeakyReLU(self.leak_value, inplace=True),\n",
    "  \n",
    "            \n",
    "            #3\n",
    "            nn.Conv3d(self.cube_len*2, self.cube_len*4, kernel_size=4, stride=2, bias=self.bias, padding=(1,1,1)),\n",
    "            nn.BatchNorm3d(self.cube_len*4),\n",
    "            nn.LeakyReLU(self.leak_value, inplace=True),\n",
    "        \n",
    "            #4\n",
    "            nn.Conv3d(self.cube_len*4, self.cube_len*8, kernel_size=4, stride=2, bias=self.bias, padding=(1,1,1)),\n",
    "            nn.BatchNorm3d(self.cube_len*8),\n",
    "            nn.LeakyReLU(self.leak_value, inplace=True),\n",
    "            \n",
    "            #5\n",
    "            nn.Conv3d(self.cube_len*8, self.cube_len*16, kernel_size=4, stride=2, bias=self.bias, padding=(1,1,1)),\n",
    "            nn.BatchNorm3d(self.cube_len* 16),\n",
    "            nn.LeakyReLU(self.leak_value, inplace=True),\n",
    "            \n",
    "            #6\n",
    "            nn.Conv3d(self.cube_len*16, 1, kernel_size=4, stride=1, bias=self.bias, padding=0)\n",
    "            \n",
    "            )    \n",
    "        \n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main,  input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        #print('D0:',output.size)\n",
    "        output = output.mean(0)\n",
    "        #print('D1:',output.size)\n",
    "        #return output.view(-1, 1).squeeze(1)\n",
    "        return output.view(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DCGAN_G__(nn.Module):\n",
    "    def __init__(self, ngpu, cube_len, leak_value, bias):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.cube_len=cube_len\n",
    "        self.leak_value=leak_value\n",
    "        self.bias=bias\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            #1\n",
    "            nn.ConvTranspose3d(in_channels=nz, \n",
    "                      out_channels=self.cube_len*8, \n",
    "                      kernel_size=4, # == 4( 4,4,4)\n",
    "                      stride = 2, # == 2\n",
    "                     #padding=(1,1,1), # == 1\n",
    "                      padding=0,\n",
    "                      bias=self.bias),\n",
    "            nn.BatchNorm3d(self.cube_len*8),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            #2\n",
    "            nn.ConvTranspose3d(self.cube_len*8, self.cube_len*4, kernel_size=4, stride=2, bias=self.bias, padding=1),\n",
    "            nn.BatchNorm3d(self.cube_len*4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            #3\n",
    "            nn.ConvTranspose3d(self.cube_len*4, self.cube_len*2, kernel_size=4, stride=2, bias=self.bias, padding=1),\n",
    "            nn.BatchNorm3d(self.cube_len*2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            #4\n",
    "            nn.ConvTranspose3d(self.cube_len*2, self.cube_len, kernel_size=4, stride=2, bias=self.bias, padding=1),\n",
    "            nn.BatchNorm3d(self.cube_len),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            #5 \n",
    "            nn.ConvTranspose3d(self.cube_len, 1, kernel_size=4, stride=2, bias=self.bias, padding=1)\n",
    "            \n",
    "            )    \n",
    "        \n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
